<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine_learning | Byte Tank]]></title>
  <link href="http://lopespm.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://lopespm.github.io/"/>
  <updated>2022-12-30T00:00:51+00:00</updated>
  <id>http://lopespm.github.io/</id>
  <author>
    <name><![CDATA[Pedro Lopes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deep Reinforcement Learning: Playing a Racing Game]]></title>
    <link href="http://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html"/>
    <updated>2016-10-06T00:00:00+01:00</updated>
    <id>http://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game</id>
    <content type="html"><![CDATA[<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="spzYVhOgKBA"></div></div>
   <p class="media-caption media-caption-two">Agent playing Out Run, session 201609171218_175eps<br />No time limit, no traffic, 2X time lapse</p>
 </div>
  </center>

<p>Above is the built <a href="https://deepmind.com/research/dqn/">deep Q-network (DQN)</a> agent playing <a href="https://en.wikipedia.org/wiki/Out_Run">Out Run</a>, trained for a total of 1.8 million frames on a Amazon Web Services g2.2xlarge (GPU enabled) instance. The agent was built using python and tensorflow. The Out Run game emulator is a modified version of <a href="https://github.com/lopespm/cannonball">Cannonball</a>. All source code for this project is <a href="/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html#source-code">available on GitHub</a>.</p>

<!--more-->

<p>The agent learnt how to play by being rewarded for high speeds and penalized for crashing or going off road. It fetched the game’s screens, car speed, number of off-road wheels and collision state from the emulator and issued actions to it such as pressing the left, right, accelerate or brake virtual button. </p>

<p>Agent trainer implements the deep Q-learning algorithm used by <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Google’s DeepMind Team to play various Atari 2600 games</a>. It uses a reward function and hyperparameters that fit best for Out Run, but could potentially be used to <a href="#plugging-other-problems-and-games">play other games or solve other problems</a>.</p>

<p>There is a wealth of <a href="#further-references">good information</a> about this reinforcement learning algorithm, but I found some topics difficult to grasp or contextualize solely from the information available online. I will attempt to add my humble contribution by tackling these and also provide details about the project’s implementation, results and how it can be used/modified/deployed.</p>

<p>Let’s start by one of its main gears: Q-learning</p>

<p />

<h1 id="concepts">Concepts</h1>

<h2 id="q-learning">Q-learning</h2>

<p>At the heart of deep Q-learning lies Q-learning, a popular and effective <a href="https://www.youtube.com/watch?time_continue=258&amp;v=bFPoHrAoPoQ">model-free</a> algorithm for learning from delayed reinforcement.  </p>

<p>Jacob Schrum has made available a terse and accessible <a href="https://www.youtube.com/playlist?list=PL4uSLeZ-ET3xLlkPVEGw9Bn4Z8Mbp-SQc">explanation</a> which takes around 45 minutes to watch and serves as a great starting point for the paragraphs below. 
Let’s take the canonical reinforcement learning example presented by Jacob (grid world):</p>

<p><img class="center" src="/files/dqn_outrun/gridworld.png"></p>

<p>To implement this algorithm, we need to build the Q-function (<a href="https://www.youtube.com/watch?v=XLkW_WGJoyQ">one of the forms</a> of the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bell-Equation</a>) by using the Q-value iteration update:</p>

<p><img class="center" src="/files/dqn_outrun/q_value_iteration_update.svg"></p>

<p>In the above grid, there are 9 actionable states, 2 terminal states and 4 possible actions (left, right, up, down), resulting in 36 (9 actionable states x 4 possible actions) Q-values.</p>

<p>This project aims to train an agent to play Out Run via its game screens, so for the sake of argument, let´s consider that each game screen is transformed into a 80x80 greyscale image (each pixel value ranging from 0 to 255), and that each transformed image represents a state. 6400 pixels (80x80) and 256 possible values per pixel translate to 256<sup>6400</sup> possible states. This value alone is a good indicator of how inflated the number of possible Q-values will be.</p>

<p>Multiplying <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py">9 possible actions</a> by 256<sup>6400</sup> possible states results in 256<sup>6400</sup> x 9 possible Q-values. If we use multiple and/or colored images for state representation, then this value will be even higher. Quite unwieldy if we want to store these values in a table or similar structure.</p>

<h2 id="enter-deep-neural-networks">Enter deep neural networks</h2>

<p>Deep neural networks work quite well for inferring the mapping implied by data, giving them the ability to predict an approximated output from an input that they never saw before. No longer do we need to store all state/action pair’s Q-values, we can now model these mappings in a more general, less redundant way. These networks also automatically learn a set of internal features which are useful in complex non-linear mapping domains, such as image processing, releasing us from laborious feature handcrafting tasks.</p>

<p>This is perfect. We can now use a deep neural network to approximate the Q-function: the network would accept a state/action combination as input and would output the corresponding Q-value. Training-wise, we would need the network’s Q-value output for a given state/action combo (obtained through a forward pass) and the target Q-value, which is calculated through the expression: <script type="math/tex">r_{t+1} + \gamma \underset{a}\max Q(s_{t+1}, a)</script>. With these two values, we can perform a gradient step on the squared difference between the target Q-value and the network’s output.</p>

<p>This is perfect, but there is still room for improvement. Imagine we have 5 possible actions for any given state: when calculating the target Q-value, to get the optimal future value estimate (consequent state’s maximum Q-value) we need to ask (forward pass) our neural network for a Q-value 5 times per learning step.</p>

<p>Another approach (used in <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DeepMind’s</a> <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">network</a>) would be to feed in the game’s screens and have the network output the Q-value for each possible action. This way, a single forward pass would output all the Q-values for a given state, translating into one forward pass per optimal future value estimate.</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/q_network_formulations.png'><img src='/files/dqn_outrun/q_network_formulations.png'></a></center><figcaption class='media-caption'> Image courtesy of Tambet Matiisen’s <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning">Demystifying Deep Reinforcement Learning</a> - Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind papers. </figcaption></figure> </p>

<p>Q-learning and deep neural networks are the center pieces of a deep Q-network reinforcement learning agent and I think that by understanding them and how they fit together, it can be easier to picture how the algorithm works as a whole.</p>

<h1 id="implementation">Implementation</h1>

<center><a href="http://lopespm.github.io/files/dqn_outrun/overall_view_play.png"><img align="center" src="http://lopespm.github.io/files/dqn_outrun/overall_view_play.png" alt="" /></a></center>

<p />

<p>Above is an overall representation of how the different components relate during a play evaluation, centered around the <code>deep Q-network for playing</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, the main decision component.</p>

<p>Each game screen is resized to a desaturated 80x80 pixels image (opposed to 84x84 on DeepMind’s papers), and if you might be wondering why each state is a sequence of four game screens instead of one, that is because the agent’s history is used for better motion perception. Achieving this requires a sequence of preprocessed images to be stacked in channels (like you would stack RGB channels on a colored image) and fed to the network. Note that RGB channels and agent history could be used simultaneously for state representation. For example, with three channels per (RGB) image and an agent history length of four, the network would be fed twelve channels per input state.</p>

<p>The <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/q_network.py#L121">network’s architecture</a> is essentially the same used by DeepMind, except for the first convolutional neural network’s input (80x80x4 instead of 84x84x4, to account for the different input sizes) and the linear layer’s output (9 instead of 18, to account for the different number of actions available)</p>

<p>The algorithm used to train this network is well described <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">here (page 7)</a> and <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">here</a>, but I would like to present it graphically, to hopefully provide some further intuition.</p>

<p>Below is <a href="https://github.com/lopespm/agent-trainer">agent trainer</a>´s implementation of the aforementioned algorithm. It adds some new concepts which were not approached by this article:</p>

<ul>
  <li>Experience replay mechanism sported by replay memories: randomly samples previous transitions, thereby smoothing the training distribution over many past behaviors</li>
  <li>Separate training network, cloned at fixed intervals to the target playing network, making the algorithm more stable when compared with standard online Q-learning</li>
  <li>ε-greedy policy to balance exploitation/exploration</li>
</ul>

<center><a href="http://lopespm.github.io/files/dqn_outrun/overall_view_train_merged.png"><img align="center" src="http://lopespm.github.io/files/dqn_outrun/overall_view_train_merged.png" alt="" /></a></center>

<p />

<h2 id="reward-function">Reward function</h2>

<p>The reward function’s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. April Yu et al. have an interesting paper on <a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">simulated autonomous vehicle control</a> which details a DQN agent used to drive a game that strongly resembles Out Run (<a href="http://codeincomplete.com/games/racer/v4-final/">JavaScript Racer</a>). Based on their reward function experiments, I’ve built a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L60">function</a> which rewards logarithmically based on speed and penalizes when the car is off-road, crashed or stopped. </p>

<p><figure class='image'><center><a href='/files/dqn_outrun/reward_function_plot.png'><img src='/files/dqn_outrun/reward_function_plot.png'></a></center><figcaption class='media-caption'> Reward values for when the car is not crashed or off-road </figcaption></figure> </p>

<h1 id="deployment">Deployment</h1>

<p>Run the trainer and emulator on your local machine by following the guide available on <a href="https://github.com/lopespm/agent-trainer/blob/master/README.md">agent-trainer’s readme</a>.</p>

<p>It is also possible to deploy the agent to an AWS EC2 instance or generic Linux remote machine by using a set of bash scripts offered by <a href="https://github.com/lopespm/agent-trainer-deployer">agent-trainer-deployer</a>.</p>

<h3 id="aws-ec2">AWS EC2</h3>

<p>Amazon allows you to bid on spare EC2 computing capacity via <a href="https://aws.amazon.com/ec2/spot/">spot instances</a>. These can cost a fraction of on-demand ones, and for this reason were chosen as the prime method for training in this project, leading to the need for mid-training instance termination resilience.</p>

<p>To accommodate this scenario, the deployment scripts and agent-trainer are designed to support train session resumes. To persist results and decrease boot up time between sessions, a long-lived EBS volume is attached to the live instance. The volume contains the training results, agent-trainer´s source code, cannonball’s source code, dockerfiles and their respective docker images. </p>

<p><figure class='image'><center><a href='/files/dqn_outrun/deployed-diagram.png'><img src='/files/dqn_outrun/deployed-diagram.png'></a></center><figcaption class='media-caption'> Relationship between components when deploying agent-trainer to an AWS EC2 instance </figcaption></figure> </p>

<h1 id="results">Results</h1>

<p>The hyperparameters used on all sessions mimic the ones used on DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a>, except for the number of frames skipped between actions, which are spaced apart by 450ms (equivalent to 13 frames) on agent-trainer.</p>

<p>The Out Run game, as you would play it in an arcade, clutters the road with various cars in order to make the game more challenging. In-game traffic was disabled for both training and evaluation plays, rendering a more achievable starting point for these experiments. Training with random traffic could be an interesting posterior experiment.</p>

<p>Some experiments were made by increasing the discount factor up its final value during training, as proposed on <a href="https://arxiv.org/abs/1512.02011">“How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies”</a>, but did not achieve better stability or rewards when compared to a fixed 0.99 discount factor. The aforementioned paper also proposes decaying the learning rate during training, which increased stability and performance significantly. Decaying the learning rate without minimum value clipping yielded the best results.</p>

<p>Another improvement was to train the game without a time limit, meaning that the training episode would only finish when the car reached the last stage’s end. This allowed for a broader replay memory training set, since the agent traversed a wide range of different tracks and settings.</p>

<p>Play evaluation was the same between all experiments, this is, the agent was evaluated by playing on the default 80 second, easy mode.</p>

<p>Here is a summary of the most relevant training sessions (you can find their models, metrics and visualizations on <a href="https://github.com/lopespm/agent-trainer-results">agent-trainer-results</a>):</p>

<table>
  <thead>
    <tr>
      <th>Session</th>
      <th>M</th>
      <th>Training<br />game mode</th>
      <th>Learning rate decay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>201609040550_5010eps</td>
      <td>a)</td>
      <td>timed; easy</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_2700eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_7300eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609160922_54eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609171218_175eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>unclipped learning rate decay</td>
    </tr>
  </tbody>
</table>

<p class="media-caption"><a href="https://github.com/lopespm/agent-trainer-results">Training sessions</a> summary: session names are formed by &#60;session ID&#62;_&#60;number of episodes trained&#62;<br />(M)achine used:  a) AMD Athlon(tm) II X2 250 Processor @ 3GHz; 2GB RAM DDR3-1333 SDRAM; SSD 500 GB: Samsung 850 EVO (CPU only training); b) AWS EC2 g2.2xlarge (GPU enabled instance), 200 GB General Purpose SSD (GP2)</p>

<p />

<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
   <div class="youtube-player video-frame-two" data-id="1Gpl9Xc-E8M"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609111241_2700eps</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="6F3eCoCw57E"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609171218_175eps<br class="video-br-end" /></p>
</div>
</center>

<p>Notice on the videos above how the timed mode trained session <code>201609111241_2700eps</code> reaches the first checkpoint about five seconds earlier than the unlimited time mode trained session <code>201609171218_175eps</code>, but proceeds to drive off-road two turns after. Its stability gets increasingly compromised as more episodes are trained, which can be observed by the rampant loss increase before 7300 episodes are reached (<code>201609111241_7300eps</code>):</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/201609111241<em>7300eps/metrics_in_train.png'><img src='/files/dqn_outrun/201609111241<em>7300eps/metrics_in_train.png'></a></center><figcaption class='media-caption'> Training metrics for session 201609111241</em>7300eps </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609111241<em>7300eps/metrics_trained_play.png'><img src='/files/dqn_outrun/201609111241<em>7300eps/metrics_trained_play.png'></a></center><figcaption class='media-caption'> Play evaluation metrics for session 201609111241</em>7300eps: using ε=0.0; evaluation made at the end of every 20 training episodes </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/metrics_in_train.png'><img src='/files/dqn_outrun/201609171218<em>175eps/metrics_in_train.png'></a></center><figcaption class='media-caption'> Training metrics for session 201609171218</em>175eps </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/metrics_trained_play.png'><img src='/files/dqn_outrun/201609171218<em>175eps/metrics_trained_play.png'></a></center><figcaption class='media-caption'> Play evaluation metrics 201609171218</em>175eps: using ε=0.0; evaluation made at the end of every training episode </figcaption></figure></p>

<p>Both <code>201609111241_2700eps</code> and <code>201609111241_7300eps</code> timed trained sessions mostly drive off-road and stall after the first stage, whereas the unlimited time mode trained session <code>201609171218_175eps</code> can race through all the stages crashing <em>only</em> three times (as shown on the article’s first video) and is able to match the performance of a timed trained session when evaluated on the default easy timed mode. </p>

<p>Below is the loss plot for <code>201609160922_54eps</code> and <code>201609171218_175eps</code>, both trained using the game’s unlimited time mode, difference being that <code>201609160922_54eps</code> keeps a fixed learning rate and <code>201609171218_175eps</code> decays it every 50100 steps:</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/loss<em>201609171218</em>201609160922.png'><img src='/files/dqn_outrun/loss<em>201609171218</em>201609160922.png'></a></center><figcaption class='media-caption'> Loss comparison between sessions <font color="#9c27b0">■</font> 201609160922<em>54eps and <font color="#009688">■</font> 201609171218</em>175eps, as viewed on <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a> </figcaption></figure></p>

<p>Other representative visualizations:</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/t-SNE_timed_easy_mode.png'><img src='/files/dqn_outrun/201609171218<em>175eps/t-SNE_timed_easy_mode.png'></a></center><figcaption class='media-caption'> t-SNE visualization, generated by letting the agent play one game on timed easy mode. Agent is using the network trained on session 201609171218</em>175eps </figcaption></figure>
<figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/t-SNE_no_time_mode.png'><img src='/files/dqn_outrun/201609171218<em>175eps/t-SNE_no_time_mode.png'></a></center><figcaption class='media-caption'> t-SNE visualization, generated by letting the agent play one game on unlimited time mode. Agent is using the network trained on session 201609171218</em>175eps </figcaption></figure>
 <figure class='image'><center><a href='/files/dqn_outrun/conv_net_filters.png'><img src='/files/dqn_outrun/conv_net_filters.png'></a></center><figcaption class='media-caption'> Visualization of the first convolutional network layer’s filters. These can be viewed via <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a></figcaption></figure> </p>

<p><br /></p>

<h1 id="final-notes">Final Notes</h1>

<h2 id="plugging-other-problems-and-games">Plugging other problems and games</h2>

<p>Agent-trainer was not built from the get-go to train games or problems other than Out Run, but I think it would be interesting to perform a thought exercise on what would be necessary to do so.</p>

<p>There are three main areas in which <a href="https://github.com/lopespm/agent-trainer">agent-trainer</a> has domain knowledge about Out Run:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer/tree/master/agent/game"><code>game</code></a> package, which contains
    <ul>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py"><code>Action</code></a> enumeration: describes all the possible actions in the game.</li>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/cannonball_wrapper.py"><code>cannonball_wrapper</code></a> module: only this module has access to the cannonball emulator. It translates the aforementioned actions into game actions and is accessed by methods such as <code>start_game()</code>, <code>reset()</code> and <code>speed()</code>.</li>
    </ul>
  </li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> class: contains the reward function. Instead of using a generic reward function like DeepMind, it was chosen to have a tailor-made reward function for Out Run, which takes into account the car’s speed and its off-road and crash status.</li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/visualization/metrics.py"><code>metrics</code></a> module: aware of the <code>speed</code> metric, which is Out Run specific, and <code>score</code>, which is game specific domain knowledge.</li>
</ul>

<p>Training another game would require the creation of a new wrapper with the same interface as <code>cannonball_wrapper</code>, a new <code>Action</code> enumerator specific to the game, a new <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> with a different reward function and the removal/replacement of the <code>speed</code> metric.</p>

<p>Apart from the previously mentioned steps, solving generic problems would require the preprocessor to be changed/replaced if images were not to be used for state representation. An option would be to create a new preprocessor class with a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/image_preprocessor.py#L13"><code>process(input)</code></a> method, tweak the hyperparameters as required (so that the network knows which dimensions to expect on its input), and finally inject the newly created class in <a href="https://github.com/lopespm/agent-preprocessor/blob/master/agent/trainer/episode.py#L300"><code>EpisodeRunner</code></a>, replacing the old preprocessor class.</p>

<h2 id="further-references">Further references</h2>

<p>I am not a machine learning expert, but from my learner’s point of view, if you are interested in getting your feet wet, Andrew Ng’s Machine Learning Course is as a great starting point. It is freely available on the <a href="https://www.coursera.org/learn/machine-learning">Coursera online learning platform</a>. This was my first solid contact with the subject and served as a major stepping stone for related topics such as Reinforcement Learning.</p>

<p><a href="https://www.udacity.com/course/deep-learning--ud730">Udacity Google Deep Learning</a>: this free course tackles some of the popular deep learning techniques, all the while using tensorflow. I did this right after Andrew Ng’s course and found it to leave the student with less support during lessons - less hand-holding if you will - and as result I spent a good amount of time dabbling to reach a solution for the assignments. </p>

<p>As a side note, I started building this project by the end of the Deep Learning course, mostly because I wanted to apply and consolidate the concepts I learnt into something more practical and to share this knowledge further, so it could hopefully help more people who are interested in this.</p>

<p>Other useful resources:</p>

<ul>
  <li>DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a> and its respective <a href="https://sites.google.com/a/deepmind.com/dqn/">source code</a></li>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></li>
  <li><a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">Deep Reinforcement Learning for Simulated Autonomous Vehicle Control</a></li>
  <li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
  <li><a href="https://www.udacity.com/course/reinforcement-learning--ud600">Udacity Reinforcement Learning by Georgia Tech</a></li>
  <li><a href="https://www.youtube.com/watch?v=dV80NAlEins">Deep learning lecture by Nando de Freitas</a></li>
  <li><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/">Learning reinforcement learning (with code, exercises and solutions)</a></li>
  <li><a href="https://gym.openai.com/">OpenAI Gym</a>: quoting the project’s page: <em>”a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Go”</em></li>
  <li><a href="https://github.com/devsisters/DQN-tensorflow">Tensorflow implementation</a> of Human-Level Control through Deep Reinforcement Learning, by Devsisters corp.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>All source code is available on GitHub:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer">Agent Trainer</a>: the core python+tensorflow application</li>
  <li><a href="https://github.com/lopespm/cannonball">Cannonball</a>: custom Cannonball (Out Run game emulator) fork which contains the changes needed to access the emulator externally</li>
  <li><a href="https://github.com/lopespm/agent-trainer-deployer">Agent Trainer Deployer</a>: bash scripts to deploy agent-trainer to a generic remote machine or AWS EC2 instance</li>
  <li><a href="https://github.com/lopespm/agent-trainer-docker">Agent Trainer Docker</a>: Dockerfiles used when deploying agent-trainer to a remote machine</li>
  <li><a href="https://github.com/lopespm/agent-trainer-results">Agent Trainer Results</a>: Collection of training sessions, each containing their resulting network, metrics and visualizations</li>
</ul>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>“Deep Q-network for playing” in this project is equivalent to DeepMind’s “target network $\hat Q$” and “Deep Q-network for training” is equivalent to DeepMind’s “network Q”<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
