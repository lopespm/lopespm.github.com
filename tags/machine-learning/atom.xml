<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: machine learning | Byte Tank]]></title>
  <link href="https://lopespm.com/tags/machine-learning/atom.xml" rel="self"/>
  <link href="https://lopespm.com/"/>
  <updated>2024-09-12T06:22:49+00:00</updated>
  <id>https://lopespm.com/</id>
  <author>
    <name><![CDATA[Pedro Lopes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Personal Local (Llama3 8B) LLM extended with WhatsApp + Obsidian Data]]></title>
    <link href="https://lopespm.com/machine_learning/2024/06/24/personal-llm.html"/>
    <updated>2024-06-24T00:00:00+00:00</updated>
    <id>https://lopespm.com/machine_learning/2024/06/24/personal-llm</id>
    <content type="html"><![CDATA[<p>This blog article will describe the implementation of a fully offline Llama3 8B LLM agent with access to WhatsApp Messages and Obsidian notes (or any other data sources), making it able to hold conversations about any of the topics present there. All data stays local, including the data from WhatsApp and Obsidian - it is in fact a <em>personal</em> assistant.</p>

<p>Source code for this project is <a href="/machine_learning/2024/06/24/personal-llm.html#source-code">available on GitHub</a>.</p>

<p><center>
          <picture aria-label="Header image of Personal LLM">
             <source type="image/webp" srcset="/files/personal_llm/showcase_interaction3.webp" aria-label="Header image of Personal LLM">
             <source type="image/png" srcset="/files/personal_llm/showcase_interaction3.png" aria-label="Header image of Personal LLM">
             <img src="/files/personal_llm/showcase_interaction3.png" aria-label="Header image of Personal LLM">
           </picture>
         </center></p>

<!--more-->

<p><br /></p>

<h1 id="motivation--objective">Motivation &amp; Objective</h1>

<h2 id="why-was-this-project-assembled-in-the-first-place">Why was this project assembled in the first place?</h2>

<ul>
  <li>I’ve built up a considerable personal database:
    <ul>
      <li>For more than 15 years I’ve been journaling and writing several notes, keeping them aggregated in the same place as I changed their hosting platform. From simple text files, passing by Evernote, and more recently into <a href="https://obsidian.md/">Obsidian</a>, into which I migrated all of my written notes <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></li>
      <li>Most of my relevant correspondence for almost 10 years has been made via WhatsApp, which stores all messages locally</li>
    </ul>
  </li>
  <li>I’ve realized how powerful it is to reflect on past events and approach them with the lenses of the new experiences and lessons I’ve been accumulating throughout life, but due to the considerable amount of content present in the above data stores, sometimes it is hard to reach and associate the relevant information I am looking for, even though I try to keep the above relatively organized.</li>
  <li>And of course, how great would it be to have a personal assistant with access to all of these, in a self-contained offline fashion?
    <ul>
      <li>Currently, state of the art Large Language Models (LLMs) such as <a href="https://ai.meta.com/blog/meta-llama-3/">Llama3</a> are feasible to be run locally on a laptop, which is something that still blows my mind.</li>
      <li>Pre-trained LLMs can have their capabilities augmented (as we will see below) to serve different custom purposes</li>
    </ul>
  </li>
</ul>

<p>All of this combined meant that it would be feasible to host a state of the art LLM in my laptop, with potential access to my personal files, without requiring this sensitive information to be hosted or processed elsewhere, which was something that had quite an appeal for me.</p>

<h2 id="objective">Objective</h2>

<p>Build an offline assistant with access to WhatsApp messages and Obsidian notes (or any other data sources), using components that don’t fully abstract the process (similar to the approach I took when <a href="/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html">creating a deep reinforcement learning agent to play a racing game</a>), to learn more about how LLMs and their infrastructures work.</p>

<p><br /></p>

<h1 id="building-blocks">Building Blocks</h1>

<h2 id="running-an-offline-llm">Running an offline LLM</h2>

<p>One of the crucial pieces is actually being able to run an LLM locally. Below we will explore the hardware requirements, and software we can use.</p>

<h3 id="the-more-gpu-cores-and-vram-the-better">The more GPU cores and VRAM, the better</h3>

<p>When running LLMs on a personal computer, there are two main factors that influence its inference performance and overall capability to even being able to run them: number of GPU cores, and available VRAM (video RAM, this is, memory available to the GPU).</p>

<ul>
  <li>The more GPU cores available, the more operations can be processed, and machine learning algorithms are all about just finding patterns in numbers</li>
  <li>The more (VRAM) memory available, the bigger the model can be stored in memory and be efficiently processed. For example, if we consider Llama3 with 8 billion parameters (more information in <a href="https://www.youtube.com/watch?v=cpYqED1q6ro">this video</a>) <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>:
    <ul>
      <li>One 32 bit float value = 32 bits of memory, this is, 4 bytes (1 byte = 8 bits)</li>
      <li>8 billion parameters = 8.000.000.000 * 4 bytes = 32.000.000.000 bytes ≈ 32GB VRAM, for a full precision model (32 bit float)</li>
      <li>If we would use a quantized model, this is, using lower precision floats to represent the parameters:</li>
      <li>Using half precision 16 bit float parameters: 16 GB VRAM</li>
      <li>Using quarter precision, 8 bit float parameters: 8 GB VRAM</li>
      <li>And if we would use Llama 70 billion parameters, it would amount to 280 GB VRAM (float32), 140 GB VRAM (float16), 70 GB VRAM (float8), 35 GB VRAM(float4)</li>
    </ul>
  </li>
</ul>

<p>I personally own a MacBook Pro M3 Max with 40 GPU cores and 128GB of unified RAM (meaning that this memory is available to the CPU and GPU), and it just so happens that Apple Silicon computers (M1, M2, M3 chips) are very capable machines to run inference on LLMs, such as Llama3 8B (full precision), or even 70B models (I’ve personally tested the 70B 4bit model <sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> on this machine, and can attest that it runs well and in bounds of memory available, as seen on the calculations above)</p>

<h3 id="running-the-inference">Running the inference</h3>

<p>There are several methods or libraries that can be used for running a LLM in a personal computer. One of the most approachable ones is <a href="https://www.ollama.com/">ollama</a>, which effortlessly allows you (after a quick installation step), to run Llama 3, Phi 3, Mistral, Gemma offline on your computer.</p>

<p>Another possible way is to load the model’s definition from Hugging Face (think of it roughly as a GitHub for machine learning models), and run them via the <a href="https://pypi.org/project/transformers/">transformers</a> library, which vividly encourage you to explore. The amount of freely available pre-trained models to perform tasks on different modalities such as text, vision, and audio is inspiring, to say the least.</p>

<p>For this project, a third option was picked, which was using <a href="https://pypi.org/project/mlx-lm/">mlx_lm</a> library, that essentially still leverages the Hugging Face Hub to provide the models, but optimizes execution by leveraging <a href="https://unfoldai.com/mlx-ml-on-apple-silicon/">MLX</a>, an array framework crafted specifically for harnessing the power of Apple silicon, which was developed by Apple’s machine learning research team.</p>

<h2 id="augmenting-the-llms-knowledge">Augmenting the LLM’s Knowledge</h2>

<p>Now that our LLM is running locally, the question that follows is: how to make it aware of all the contents present in Obsidian notes and WhatsApp messages? We have a few options:</p>

<h3 id="retrain--fine-tune-a-pre-trained-llm">1. Retrain / fine tune a pre-trained LLM</h3>

<p>The idea here is to retrain / fine tune the entire model, or parts of it, to make it knowledgeable about our contents.</p>

<p>A pre-trained LLM comes with the advantage of common probabilistic knowledge about a language or set of languages that can be leveraged upon, which was a result of a long and costly training process. As such, a full retraining of the model that would not retain information about its trained parameters would be out of the question. Instead, there are several ways we can leverage it:</p>

<ul>
  <li>Freeze the existing LLM layers (all or a selection of them), add also add new trainable layer layers on top of the existing LLM layers, and only train the trainable and unfreezed ones.</li>
  <li><a href="https://llama.meta.com/docs/how-to-guides/fine-tuning/">Fine tune</a> the existing pre-trained LLM, which essentially modifies all parameters of the model with a low training rate, in order to retain as much existing knowledge and capabilities as much as possible, and avoid <a href="https://arxiv.org/html/2402.18865v1">catastrophic forgetting</a>. Because these models are so massive, they are not particularly sensitive to any given parameters being off by a lot, or a little. Their overall size and complexity make up for it, so we can mostly get away with rough updates. To fine tune a model, there are essentially two ways to go about it:
    <ul>
      <li>Load all of the model’s parameters into memory, and modify them directly during the learning process</li>
      <li>Take advantage of the fact that you can multiply two small matrices to get a larger matrix (which is the main bread and butter of deep learning models), so we can try to find out the two smaller matrices instead, meaning that we only need to maintain those in memory. All the parameters of the model are still modified during training, but we don’t need to hold them all in memory at once. That’s what Low-Rank Adaptation (<a href="https://www.entrypointai.com/blog/lora-fine-tuning/">LoRA</a>) fine-tuning method does, and it is one of the most popular ways to fine tune LLMs, along with its QLoRA counterpart (which uses a data type that reduces by 4x the already lower LoRA memory footprint).</li>
    </ul>
  </li>
</ul>

<p>Some important aspects of these techniques:</p>

<ul>
  <li>We are compressing our documents into the LLMs internal knowledge, and we have less control of how precisely these models retain the most accurate information. These could lead to hallucinations.</li>
  <li>If the data we want to augment our LLM with is dynamic in nature (which for our case, I would prefer it to be, so that the assistant can have access to the most relevant information when used), this would mean that the model would need to be fine tuned every time an update should be made.</li>
  <li>The training data needs to be structured in a way that makes sense for the model to be used. For example, WhatsApp messages and Obsidian notes would need to be transformed into a sensible training set that would fit the tone I would aim to have for the assistant.</li>
</ul>

<p>Given that I would want to experiment with different types of assistants and different ways to leverage and explore the data in a conversational way, and have access to the most up to date information available, which could be available in several different formats (for example, let’s say I would like to augment the LLMs knowledge with historical CO2 readings of my apartment), retraining / fine tuning the model was not a very inviting approach.</p>

<h3 id="insert-all-documents-into-the-prompt-as-context">2. Insert all documents into the prompt, as context</h3>

<p>The idea here is to provide all the context that LLM needs, in the <a href="https://llama.meta.com/docs/how-to-guides/prompting/">prompt’s context</a>. For example:</p>

<div><div class="CodeRay">
  <div class="code"><pre>
&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

You are a AI assistant which answers the user's question based on your prior knowledge
and a collection of the user's documents.

Each of the documents has the format &lt;&lt;document:||document_name||&gt;&gt;||content||.
||document_name|| represents the document name. ||content|| represents the
document's content. Don't show the &lt;&lt;document:||document_name||&gt;&gt; portion to the user.

For example, for: &lt;&lt;document:&lt;obsidian&gt;:hello world&gt;&gt;This is an interesting
description of hello.
- ||document_name|| is &lt;obsidian&gt;:hello world
- ||content|| This is an interesting description of hello.

These are the user's documents:
----------
&lt;&lt;document:&lt;obsidian&gt;:lamas running down the street&gt;&gt;The content for lamas
going down the street
&lt;&lt;document:&lt;obsidian&gt;:alpacas running down the street&gt;&gt;That time when I saw a
alpaca running down the street.
----------

&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Tell me about that time I've seen animals
running down the street&lt;|eot_id|&gt;

</pre></div>
</div>
</div>

<p><br />
This would be the most convenient approach, but the context window available to a LLM, the number of tokens it can use at one time to generate a response, is limited.</p>

<p>Llama3 8B for example, allows for a 8K token context window, and there have been attempts to extend it further to <a href="https://arxiv.org/pdf/2404.19553">80K</a> or <a href="https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k">262K</a>, and Google’s Gemini 1.5 Pro can handle a <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#architecture">1 million token context window</a>, for example.</p>

<p>Given that my Obsidian notes total around 8M characters in content, and my WhatsApp messages around 16M, even if we assume that an average of 1 <a href="https://x.com/karpathy/status/1789590397749957117">token</a> per 4 characters, just the Obsidian notes would clock around 2M tokens, which makes this method infeasible</p>

<h3 id="rag-retrieval-augmented-generation">3. RAG (Retrieval Augmented Generation)</h3>

<p>To navigate around the context window limitation, instead of inserting all the documents into the prompt’s context in one go, we can instead be selective on the information we want to provide on the prompt, depending on the user’s request(s).</p>

<p>For example, if the user asks the AI assistant “what do you think about that gaming project I did 4 years ago?”, behind the scenes we can query a database, and ask it the most relevant results about gaming projects that happened about 4 years ago, and then use those documents content into the prompt’s context.</p>

<p>That’s the overall idea of RAG.</p>

<p>Now, many questions come up:</p>

<ol>
  <li><em>Transforming a conversation into a query:</em> How do we transform the user’s prompt + the context of the conversation, into a query that asks our database exactly what the main LLM needs to provide a contextual reply?</li>
  <li><em>Fetching contents related to a query:</em> How can the content database know which are the most relevant content related to a given query?</li>
</ol>

<p>Let’s explore these in the next section.</p>

<h2 id="what-makes-rag-tick">What makes RAG tick?</h2>

<h3 id="transforming-a-conversation-into-a-query">1. Transforming a conversation into a query</h3>

<p>In order to produce a query that transforms an entire conversation into a concise sentence that can be used to get the documents in the database that are most similar to it, we can feed in the conversation into a LLM specialized in creating summarized queries. Here is an example of prompt we can use for it:</p>

<div><div class="CodeRay">
  <div class="code"><pre>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;   
You are the user's document keeper that has access to all the documents for the user.
Based on the entire conversation only, you will only reply with the question you
will ask to query the appropriate user documents, in order to get all the relevant
user's documents related to the conversation.
&lt;|eot_id|&gt;

&lt;&lt; entire conversation &gt;&gt;

&lt;|start_header_id|&gt;document keeper&lt;|end_header_id|&gt;
</pre></div>
</div>
</div>

<p><br />
This is how it would it would look like with actual information about the conversation:</p>

<div><div class="CodeRay">
  <div class="code"><pre>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;   
You are the user's document keeper that has access to all the documents for the user.
Based on the entire conversation only, you will only reply with the question you
will ask to query the appropriate user documents, in order to get all the relevant
user's documents related to the conversation.
&lt;|eot_id|&gt;

&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
Hi there! I\'m a helpful AI assistant with access to your documents. What can I do for you today?
&lt;|eot_id|&gt;
&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
What is the name of the first steam game I developed?
&lt;|eot_id|&gt;
&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
Survival Ball, which you published in 2018
&lt;|eot_id|&gt;
&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
which interesting things could you tell me about it?
&lt;|eot_id|&gt;

&lt;|start_header_id|&gt;document keeper&lt;|end_header_id|&gt;
</pre></div>
</div>
</div>

<p><br />
This LLM’s reply would be <code>Which are interesting facts about Survival Ball, a game developed in 2008?</code>.</p>

<h3 id="fetching-contents-related-to-a-query">2. Fetching contents related to a query</h3>

<p>Now that we have a query, such as “Which are interesting facts about Survival Ball, a game developed in 2008?”, how can we get the documents (Obsidian notes and/or WhatsApp messages) that are most relevant with that query?</p>

<p>One possible way would be to perform a custom decomposition of the query phrase, and attempt to look for documents that most referenced similar words to the query. This custom search would be brittle, require careful heuristics to match these, and would not completely take into account what that sentence means, and therefore the matching between query and documents would be likely unreliable.</p>

<p>Instead, we have a much more powerful concept at our disposal, which will help us perform this matching: text embeddings.</p>

<h4 id="text-embeddings">Text embeddings</h4>

<p>The idea behind text embeddings is that if we transform a given piece of text into a vector that is in close proximity to other vectors that have a similar meaning, then from a given query text we can get the most conceptually similar pieces of text to it. <sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>For example, if the vectors representing our text were trained to be closer together when they rhyme, then close to the word “dog”, we might have the word “fog” closer to it, but “cat” further away; and conversely, if we trained these to represent their conceptual meaning, then for the word “dog”, we might see the word “cat” nearer to it, but “floor” further away <sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></p>

<p>A mind-boggling implication is that we can perform algebraic expressions on these vectors. For example, if our vectors were trained to represent the meaning of the text we could have the following:</p>

<ul>
  <li>The classical example is that if grab the vectors for king, woman and man, and perform this operation: <code>(king + woman) - man</code>, the closest vector could be <code>queen</code></li>
  <li><code>(London + Japan) - England -&gt; Tokyo</code></li>
  <li><code>(boat - fly) - airplane -&gt; sail</code></li>
  <li><code>(cow - oink) - pig -&gt; moos</code></li>
</ul>

<p>Likewise, if we probe which are the closest pieces of text related to <code>dog</code>, we could get <code>dogs, puppy, pit_bull, pooch</code></p>

<p>These examples were takes from an actual working <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> model that was trained on a google news dataset, which I invite you to explore in this <a href="https://colab.research.google.com/drive/1qxnNUojAzIniOXwFZi4y7NlWk4cTzIMZ?usp=sharing">Google Colab notebook</a>.</p>

<center>
  <div class="image-mosaic">
  <div class="image-mosaic-card image-mosaic-card-tall image-mosaic-card-wide" style="background-image: url('/files/personal_llm/google_colab_word2vec.png')"></div>
  </div>
  <figcaption class="media-caption" style="margin-top: -7px; margin-bottom: 24px;"> Exploring the above examples using a <a href="https://colab.research.google.com/drive/1qxnNUojAzIniOXwFZi4y7NlWk4cTzIMZ?usp=sharing">word2vec model in Google Colab</a></figcaption>
</center>

<p>For our project’s objective, the most relevant property is the latter one. Given query text, we would transform it into a vector, and get the closest vectors in our document (vector) database, which in our project will be supported by a PostgreSQL database using the <a href="https://github.com/pgvector/pgvector">pgvector</a> extension. More about this below.</p>

<h1 id="putting-it-all-together">Putting it all together</h1>

<p>With the prior knowledge for the previous section, we are now in a position to build an system architecture that would best fit our objective:</p>

<p><figure class='image'><center><a href='/files/personal_llm/architecture.png'><img src='/files/personal_llm/architecture.png'></a></center><figcaption class='media-caption'> Personal LLM Architecture </figcaption></figure></p>

<p>The overall idea is that we have a Llama3 8B LLM powering the main AI assistant, which has its knowledge augmented via RAG (Retrieval Augmented Generation) by using another Llama3 8B LLM to generate queries used to retrieve documents from a vector database powered by PostgreSQL. These retrieved documents are then added to the main LLM’s context, and the LLM generates a response based on the retrieved documents + user’s prompt + ongoing conversation. Below are further details for these.</p>

<ol>
  <li>First <em>prepare documents and store them</em> in a vector database.</li>
  <li>and then, when using the chat interface, <em>evaluate the user’s input and conversation</em>, query the vector database for relevant documents, add them as context into the main LLMs prompt, which will output the response that the user will see.</li>
</ol>

<h2 id="document-preparation-and-storage">Document preparation and storage</h2>

<p>Before anything else, all Obsidian notes and WhatsApp messages are exported, parsed, transformed into text embeddings, and inserted into the vector database.</p>

<h3 id="exporting-the-data-sources">1. Exporting the data sources</h3>

<p>No export procedure is required for Obsidian, since it stores its text contents inside markdown (<code>.md</code>) files, we just need to point directly to the folder we want to target, which can be a vault’s root folder, for example.</p>

<p>For WhatsApp messages, a <code>result.json</code> holding a structured JSON dump of all messages and their respective metadata, is generated via an open source exporting utility which extracts all messages from the local WhatsApp database present in the mobile device; and a <code>contacts.csv</code> is exported from all Google contacts, in order to enrich the context of the aforementioned messages. A step-by-step process to generate these is described <a href="https://github.com/lopespm/personal_llm?tab=readme-ov-file#setup-of-data-sources">in the repo’s Readme</a></p>

<h3 id="parse-vectorize-and-persist">2. Parse, vectorize and persist</h3>

<p>With our data sources ready, we can now start extracting relevant information, transforming the relevant content into a vector, and then persist into a vector database. The entire process is made by running:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ python create_and_persist_embeddings.py
</pre></div>
</div>
</div>
<p><br /></p>

<p>Each of the separate Obsidian note paragraphs and WhatsApp messages are accumulated into a single entry, so that locally close contextual content can be readily available and provided to the main LLM AI assistant, when retrieved, so that it has better immediately relevant contextual information about a given topic. It was chosen to accumulate a set of note paragraphs / messages that in total would not surpass the maximum content length of the model used to vectorize the content, which in this case has a maximum content length of 514 (see how to get the maximum content length <a href="https://gist.github.com/lopespm/342bd297f0cd6a2819f69a1ea10e7777">here</a>).</p>

<p>All of these are <a href="https://github.com/lopespm/personal_llm/blob/main/create_and_persist_embeddings.py#L7">batch processed</a>, so that vector embedding creation can be done much more efficiently. Given that in total there were 25M characters worth of content across these two data sources, these optimizations make a substantial change in the processing run time. Further optimizations could be made in optimizing <a href="https://github.com/lopespm/personal_llm/blob/main/create_and_persist_embeddings.py#L33-L36">DB insertion commits</a> and streaming each of the data sources contents, instead of storing them entirely into memory.</p>

<h4 id="obsidian">Obsidian</h4>

<p>The python script calls into a <a href="https://github.com/lopespm/personal_llm/blob/main/obsidian_to_embeddings.py">custom module</a> that traverses through all the markdown (<code>.md</code>) files inside Obsidian’s root folder and its respective descendent folders, and accumulate a group of lines in a given file into a distinct entry.</p>

<p>For example, a Obsidian note called <code>Survival Ball TV.md</code> with this content</p>

<div><div class="CodeRay">
  <div class="code"><pre>Only had to take OUYA project, change some resources to match Android TV controllers, detect Nexus Remote.
Test on the Nexus Player
Delete all OUYA related resources
Most of the time was spent (...)
</pre></div>
</div>
</div>
<p><br /></p>

<p>Would be broken down into to two separate database entries <sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>:</p>

<ul>
  <li>Entry 1.
    <ul>
      <li>DB ID of the entry</li>
      <li>Content: <code>Only had to take OUYA project, change some resource (...)</code> - these would be first lines of the note</li>
      <li>Source: <code>&lt;obsidian&gt;:Survival Ball TV</code></li>
      <li>Embedding: the vector representation the content text (content attribute above)</li>
    </ul>
  </li>
  <li>Entry 2.
    <ul>
      <li>DB ID of the entry</li>
      <li>Content: <code>Unity is pretty great, and has tons of really great stuff (...)</code> - the lines coming after the ones in Entry 1. above</li>
      <li>Source: <code>&lt;obsidian&gt;:Survival Ball TV</code></li>
      <li>Embedding: the vector representation of the content text (content attribute above)</li>
    </ul>
  </li>
</ul>

<p><figure class='image'><center><a href='/files/personal_llm/obsidian_db_entry2.png'><img src='/files/personal_llm/obsidian_db_entry2.png'></a></center><figcaption class='media-caption'> Sample Obsidian entries in the vector database <sup>6</sup> </figcaption></figure></p>

<h4 id="whatsapp">WhatsApp</h4>

<p>Likewise, the script also calls into a <a href="https://github.com/lopespm/personal_llm/blob/main/whatsapp_to_embeddings.py">custom module</a> that traverses all the messages, and similarly to the approach taken above for Obsidian notes, a set of messages are aggregated into a single entry.</p>

<p>For example, a WhatsApp conversation on the “Deeper Conversations” chat group would be broken down into several different entries, such as [^6]:</p>

<ul>
  <li>Entry 1.
    <ul>
      <li>DB ID of the entry</li>
      <li>Content: <code>message from "Deeper Conversations 💭" to me, on 2021-06-26 10:15:44: And that’s absolutely normal (...)</code> - several lines of the conversation would be aggregated here</li>
      <li>Source: <code>&lt;whatsapp&gt;:xxxxxxxx-yyyyyyyy@t.as</code> (the numbers were redacted)</li>
      <li>Embedding: the vector representation of the content text (content attribute above)</li>
    </ul>
  </li>
  <li>Entry 2.
    <ul>
      <li>DB ID of the entry</li>
      <li>Content: <code>message from "Deeper Conversations 💭" to me, on 2021-07-04 18:37:54: Challenge yourself (...)</code> - another group of several lines of the conversation would be aggregated here</li>
      <li>Source: <code>&lt;whatsapp&gt;:xxxxxxxx-yyyyyyyy@t.as</code> (the numbers were redacted)</li>
      <li>Embedding: the vector representation of the content text (content attribute above)</li>
    </ul>
  </li>
  <li>Entry 3., 4., etc would have a similar structure to the ones above</li>
</ul>

<p><figure class='image'><center><a href='/files/personal_llm/whatsapp_db_entry.png'><img src='/files/personal_llm/whatsapp_db_entry.png'></a></center><figcaption class='media-caption'> Sample WhatsApp entries in the vector database <sup>6</sup> </figcaption></figure></p>

<h3 id="storage-vector-database">3. Storage (Vector Database)</h3>

<p>The storage of the above entries is handled by a PostgreSQL database using the <a href="https://github.com/pgvector/pgvector">pgvector</a> extension, which allows PostgreSQL to store and query vector attributes, and notably for our project, it provides tha capability to query for the closest vectors / entries related to a given query vector via <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> <sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>.</p>

<p>The database is hosted via a docker container, which makes it easily encapsulated and quick to set up and run.</p>

<h2 id="embedding-model">Embedding model</h2>

<p>As we <a href="/machine_learning/2024/06/24/personal-llm.html#text-embeddings">discussed above</a>, embedding models can be trained have their embedding representation to be optimized and better suit the problem at hand. In general, there are several types of tasks a embedding can be better or worse at, so testing each single model against our problem would certainly inefficient.</p>

<p>Fortunately, a Massive Text Embedding Benchmark was been made and its results are freely available <a href="https://huggingface.co/spaces/mteb/leaderboard">in this dashboard</a>, which compares several models against capabilities such as classification, clustering and reranking.</p>

<p><figure class='image'><center><a href='/files/personal_llm/text_embedding_model_leaderboard.png'><img src='/files/personal_llm/text_embedding_model_leaderboard.png'></a></center><figcaption class='media-caption'> Massive Text Embedding Benchmark (MTEB) Leaderboard </figcaption></figure></p>

<p>The model used for this project is <a href="https://huggingface.co/intfloat/multilingual-e5-large"><code>intfloat/multilingual-e5-large</code></a><sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>, which was chosen for its:</p>

<ul>
  <li>Reasonable embedding dimensions (1024), which essentially means of many dimensions will the embedding vector have, allowing for a reduced database footprint and faster retrieval when compared to higher dimension models (such as GritLM-8x7B with 4096 dimensions)</li>
  <li>Good average score across the all tasks (ranking 20th overall)</li>
  <li>And most notably support 100 different languages, <a href="https://arxiv.org/pdf/1911.02116">one of them being Portuguese</a>, which is quite relevant since my notes and messages are largely a mix of English and Portuguese, with some sprinkles of other languages (which this model also supports)</li>
</ul>

<p><em>Related (source code <a href="https://github.com/lopespm/personal_llm/blob/main/embedding_generator_multilingual.py">here</a>)</em></p>

<h2 id="evaluation-phase">Evaluation phase</h2>

<p>Having all the documents vectorized and available in a database, we can now jump into the evaluation phase, in which we leverage upon these on chat conversation with an AI assistant.</p>

<p>To start the chat session, we only need to activate the respective conda environment <sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup> and execute:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ python main.py
</pre></div>
</div>
</div>
<p><br /></p>

<h3 id="the-main-ai-assistant-llm">1. The main AI Assistant LLM</h3>

<p>Upon starting the above script, a Llama3 8B model (<a href="https://huggingface.co/mlx-community/Meta-Llama-3-8B-Instruct-4bit">mlx-community/Meta-Llama-3-8B-Instruct-4bit</a>) is loaded into memory and is instructed to behave as AI assistant / philosopher / psychotherapist (or any other role one so wishes, as <a href="/machine_learning/2024/06/24/personal-llm.html#results">seen below</a>, and the user is greeted with an initial message, and is able to start inserting a prompt.</p>

<p><figure class='image'><center><a href='/files/personal_llm/greeting.png'><img src='/files/personal_llm/greeting.png'></a></center><figcaption class='media-caption'> Greeting message from the AI assistant </figcaption></figure></p>

<p>This will be LLM having the conversation with the user.</p>

<h3 id="transform-conversation-into-db-query">2. Transform conversation into DB query</h3>

<p>Once the user inserts their prompt, the entire conversation is provided to an auxiliary “document keeper” LLM, which has only one job: transform the entire conversation the user is having with the main AI assistant into concise sentence that aims to represent the information we are interested to provide as context on the upcoming reply.</p>

<p>The document keeper LLM produces a query sentence <a href="/machine_learning/2024/06/24/personal-llm.html#transforming-a-conversation-into-a-query">using the previously described process</a> (related <a href="https://github.com/lopespm/personal_llm/blob/main/formulate_query_for_retrieving_content.py">source code</a>), and then transforms it into a vector using the <a href="/machine_learning/2024/06/24/personal-llm.html#embedding-model">embedding model</a>.</p>

<p>Now we have everything we need. Using this vector representation of the query, we can ask the database which are the top 10 entries which have their respective vectors closest to the query vector.</p>

<p>So for example, if the text query created by the “document keeper” LLM is <code>Which are interesting facts about Survival Ball, a game developed in 2008?</code> (resulting in a vector representation of [1.3, 4.2, 3.3, etc]), then a <a href="https://github.com/lopespm/personal_llm/blob/main/retrieve_related_content_from_db.py">SQL query such as this one would be made against the DB</a>:</p>

<div><div class="CodeRay">
  <div class="code"><pre><span class="class">SELECT</span> id, content, source, <span class="integer">1</span> - (embedding &lt;=&gt; [<span class="float">1.3</span>, <span class="float">4.2</span>, <span class="float">3.3</span>, etc]) <span class="keyword">AS</span> cosine_similarity
<span class="keyword">FROM</span> items
<span class="keyword">ORDER</span> <span class="keyword">BY</span> cosine_similarity <span class="directive">DESC</span> LIMIT <span class="integer">10</span>,
</pre></div>
</div>
</div>
<p><br /></p>

<h3 id="retrieved-entries-are-added-as-context-on-the-main-ai-llm-prompt">3. Retrieved entries are added as context on the main AI LLM prompt</h3>

<p>Now that we have the retrieved a set of relevant entries we want the main AI assistant LLM to consider towards its reply, we now can add them as context for the prompt for the main AI assistant LLM, very similarly to what was <a href="/machine_learning/2024/06/24/personal-llm.html#insert-all-documents-into-the-prompt-as-context">described above</a>:</p>

<div><div class="CodeRay">
  <div class="code"><pre>
&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;

You are a AI assistant which answers the user's question based on your prior knowledge
and a collection of the user's documents.

Each of the documents has the format &lt;&lt;document:||document_name||&gt;&gt;||content||.
||document_name|| represents the document name. ||content|| represents the
document's content. Don't show the &lt;&lt;document:||document_name||&gt;&gt; portion to the user.

For example, for: &lt;&lt;document:&lt;obsidian&gt;:hello world&gt;&gt;This is an interesting
description of hello.
- ||document_name|| is &lt;obsidian&gt;:hello world
- ||content|| This is an interesting description of hello.

These are the user's documents:
----------
&lt;&lt;document:&lt;obsidian&gt;:lamas running down the street&gt;&gt;The content for lamas
going down the street
&lt;&lt;document:&lt;obsidian&gt;:alpacas running down the street&gt;&gt;That time when they saw a
alpaca running down the street.
&lt;&lt;document:&lt;whatsapp&gt;:aaaaaaa-bbbbbbbb@t.we&gt;&gt;message from &quot;Thy Table Turner&quot; to me, on 2024-03-26 13:15:44: I've just seen a alpaca shuffling down the street
&lt;&lt;document:&lt;whatsapp&gt;:aaaaaaa-bbbbbbbb@t.we&gt;&gt;message from me to &quot;Thy Table Turner&quot;, on 2024-03-26 13:16:32: Pretty sure it was lama though
----------

&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Tell me about that time I've seen animals
running down the street&lt;|eot_id|&gt;

</pre></div>
</div>
</div>

<h3 id="output-from-the-ai-assistant-llm-presented-to-the-user">4. Output from the AI assistant LLM presented to the user</h3>

<p>We now have everything we need, and the prompt is ready to be <a href="https://github.com/lopespm/personal_llm/blob/main/main.py">evaluated by the main LLM</a>.</p>

<p>Once the prompt is evaluated by this LLM, we present the output to the user, and then we cycle through the entire evaluation phase again once the user submits their request.</p>

<h1 id="results">Results</h1>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
  <div class="youtube-player video-frame-two" data-id="QOpAox8iXAc"></div></div>
   <p class="media-caption media-caption-two">Example interaction with the AI LLM, having its role as "AI assistant"</p>
 </div>
  </center>

<p>Overall, I’ve found interactions with this AI to be quite interesting, even though sometimes it can output blobs of information that could be cleaner or have better answers. The former can be addressed by tweaking the prompts (i.e. prompt engineering) having increased efficacy towards the behavior I aim the Ai to have, and the latter by either cleaning Obsidian notes directly, or improving the data processing and extraction. Regardless, I find these results to be quite pleasing, useful and even amusing.</p>

<p>For example, the code allows you to choose the AI’s persona by changing <a href="https://github.com/lopespm/personal_llm/blob/main/main.py#L8-L10">this constant</a></p>

<p><figure class='image'><center><a href='/files/personal_llm/ai_role.png'><img src='/files/personal_llm/ai_role.png'></a></center><figcaption class='media-caption'> The AI’s persona is customizable </figcaption></figure></p>

<p>When choosing the AI’s persona to be of a AI “psychotherapist”, this happens <em>(disclaimer: please consult a professional if you require therapy, this agent is meant for pedagogical / entertainment / utilitarian uses only)</em>:</p>

<p><figure class='image'><center><a href='/files/personal_llm/ai_psycotherapist.png'><img src='/files/personal_llm/ai_psycotherapist.png'></a></center><figcaption class='media-caption'> Example interaction with the AI LLM, having its role as “AI assistant”  </figcaption></figure></p>

<h1 id="final-notes">Final Notes</h1>

<h2 id="adding-other-data-sources">Adding other data sources</h2>

<p>This project focuses on using Obsidian notes and WhatsApp messages to augment the main LLM, but virtually any piece of content could be used to augment it. CO2 historical readings, tax documents, financial statements, diplomas, official documents, calendar events, etc. Sky is the limit.</p>

<p>Doing so in practice would be as straightforward as adding an <a href="https://github.com/lopespm/personal_llm/blob/main/create_and_persist_embeddings.py">additional module to this script</a>, and have it return the pieces relevant pieces of content, and then re-run:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ python create_and_persist_embeddings.py
</pre></div>
</div>
</div>
<p><br /></p>

<h2 id="inspiring-wealth-of-possibilities">Inspiring wealth of possibilities</h2>

<p>This type of assistant has been something I’ve been daydreaming for a long time, and having the peace of mind that I have full control and customization of it is something that profoundly satisfies me. Not only that, this entire project has an immense learning opportunity for me, and as I researched through it and pieced the puzzle pieces together, I’ve felt humbled and inspired by the explosive amount of possibilities that these models provide.</p>

<p>For example, just browsing through the <a href="https://huggingface.co/models">wealth of freely available models</a>, ready to be used, combined and assembled into different scenarios, leaves me at awe:</p>

<p><figure class='image'><center><a href='/files/personal_llm/hugging_face_models.png'><img src='/files/personal_llm/hugging_face_models.png'></a></center><figcaption class='media-caption'> A sample of different classes of models available in Hugging Face. Each of those having dozens of models, sometimes thousands. All freely available. </figcaption></figure></p>

<h2 id="further-references">Further references</h2>

<p>I find it fulfilling to learn about a subject by fundamentally understanding its core components, hence the usage of several lower level components, <a href="/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html">similar to how I previously built a deep reinforcement learning agent</a>), but there are several readily available tools and libraries that handle a lot of the heavy lifting:</p>

<ul>
  <li><a href="https://www.langchain.com/">LangChain</a>: framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain’s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis. This could have been used to abstract most, if not all, of the RAG heavy lifting, which is quite useful.</li>
  <li>Other Vector Databases: although PostgreSQL worked great for this project, there are other possible, such as <a href="https://www.pinecone.io/learn/retrieval-augmented-generation/">Pinecone</a> (this is a externally hosted DB, hence why it was not considered), <a href="https://qdrant.tech/">Qdrant</a>, <a href="https://www.trychroma.com/">chroma</a>, <a href="https://github.com/facebookresearch/faiss">faiss</a> or <a href="https://www.mongodb.com/resources/basics/retrieval-augmented-generation">MongoDB</a></li>
  <li><a href="https://replicate.com/">Replicate</a>: Similarly to Hugging Face, Replicate is a platform that collects various open-source large language models (LLMs), offering AI models for a variety of purposes, including AI text generation, AI image generation, and AI video generation. Developers can use these models through Replicate’s API.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>All source code is available on GitHub at <a href="https://github.com/lopespm/personal_llm">https://github.com/lopespm/personal_llm</a></p>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Obsidian is an amazing platform / app which I vividly recommend. I’ve recently migrated all of my notes from Evernote into Obsidian, and couldn’t be happier. Obsidian stores all of the notes in plain <code>.md</code> files that are yours to keep, and allows you to completely customize it to your heart’s desire. For example, I created a custom script that packages all of the notes, encrypts it, and stores that vault into my Google Drive. I would be happy to provide more details about this in a posterior note.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Note that these calculations are based on loading + inference for a batch size of 1. Training a model required far more VRAM<a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Specifically, I’ve used the model available on hugging face at <a href="https://huggingface.co/mlx-community/Meta-Llama-3-70B-Instruct-4bit">mlx-community/Meta-Llama-3-70B-Instruct-4bit</a><a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>The process of how these embedding models are trained is a fascinating topic by itself. The idea behind it is that you place a given piece of text on the model’s input and on its corresponding contextual word on the output, and then train the model to attempt to correctly predict the output text. For example, if using a word2vec skip-gram architecture, the input is a single word and the output is trained to be the words around it. The model’s number of weights is lower than the number of pieces of text in the dictionary, so that the model is forced to compress these associations into its weights. Once the model is trained, we use its learned weights to generate an embedding for a given piece of text. When a model is trained on large swathes of text, this simple method yields incredible results.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>In practice, representing these vectors as being closer together when they rhyme, vs when they are conceptually closer, can be achieved by training the embedding model on poetry books and giving higher emphasis on the last words of each line for the former, and training the embedding model on more generic text datasets, such as wikipedia, and using samples around the seed word uniformly as target words (using a <a href="https://en.wikipedia.org/wiki/Word2vec">skip-gram</a> architecture for example) for the latter.<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>These screenshots were taken from <a href="https://dbeaver.io/">DBeaver</a>, and open source universal database tool which is incredibly powerful and well designed.<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>The pgvector extension supports L2 distance, inner product, cosine distance, L1 distance, Hamming distance, and Jaccard distance.<a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>Related backing “Multilingual E5 Text Embeddings: A Technical Report” paper for this model is available in <a href="https://arxiv.org/pdf/2402.05672">arxiv</a><a href="#fnref:8" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>Conda provides for a self-contained and easy way to manage packages and environments. Whereas pip is a package manager and virtualenv is an environment manager, conda is both. Here is an <a href="https://docs.conda.io/projects/conda/en/latest/commands/index.html#conda-vs-pip-vs-virtualenv-commands">illustrative comparison table</a><a href="#fnref:9" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Reinforcement Learning: Playing a Racing Game]]></title>
    <link href="https://lopespm.com/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html"/>
    <updated>2016-10-06T00:00:00+00:00</updated>
    <id>https://lopespm.com/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game</id>
    <content type="html"><![CDATA[<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="spzYVhOgKBA"></div></div>
   <p class="media-caption media-caption-two">Agent playing Out Run, session 201609171218_175eps<br />No time limit, no traffic, 2X time lapse</p>
 </div>
  </center>

<p>Above is the built <a href="https://deepmind.com/research/dqn/">deep Q-network (DQN)</a> agent playing <a href="https://en.wikipedia.org/wiki/Out_Run">Out Run</a>, trained for a total of 1.8 million frames on a Amazon Web Services g2.2xlarge (GPU enabled) instance. The agent was built using python and tensorflow. The Out Run game emulator is a modified version of <a href="https://github.com/lopespm/cannonball">Cannonball</a>. All source code for this project is <a href="/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html#source-code">available on GitHub</a>.</p>

<!--more-->

<p>The agent learnt how to play by being rewarded for high speeds and penalized for crashing or going off road. It fetched the game’s screens, car speed, number of off-road wheels and collision state from the emulator and issued actions to it such as pressing the left, right, accelerate or brake virtual button. </p>

<p>Agent trainer implements the deep Q-learning algorithm used by <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Google’s DeepMind Team to play various Atari 2600 games</a>. It uses a reward function and hyperparameters that fit best for Out Run, but could potentially be used to <a href="#plugging-other-problems-and-games">play other games or solve other problems</a>.</p>

<p>There is a wealth of <a href="#further-references">good information</a> about this reinforcement learning algorithm, but I found some topics difficult to grasp or contextualize solely from the information available online. I will attempt to add my humble contribution by tackling these and also provide details about the project’s implementation, results and how it can be used/modified/deployed.</p>

<p>Let’s start by one of its main gears: Q-learning</p>

<p />

<h1 id="concepts">Concepts</h1>

<h2 id="q-learning">Q-learning</h2>

<p>At the heart of deep Q-learning lies Q-learning, a popular and effective <a href="https://www.youtube.com/watch?time_continue=258&amp;v=bFPoHrAoPoQ">model-free</a> algorithm for learning from delayed reinforcement.  </p>

<p>Jacob Schrum has made available a terse and accessible <a href="https://www.youtube.com/playlist?list=PL4uSLeZ-ET3xLlkPVEGw9Bn4Z8Mbp-SQc">explanation</a> which takes around 45 minutes to watch and serves as a great starting point for the paragraphs below. 
Let’s take the canonical reinforcement learning example presented by Jacob (grid world):</p>

<p><img class="center" src="/files/dqn_outrun/gridworld.png"></p>

<p>To implement this algorithm, we need to build the Q-function (<a href="https://www.youtube.com/watch?v=XLkW_WGJoyQ">one of the forms</a> of the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bell-Equation</a>) by using the Q-value iteration update:</p>

<p><img class="center" src="/files/dqn_outrun/q_value_iteration_update.svg"></p>

<p>In the above grid, there are 9 actionable states, 2 terminal states and 4 possible actions (left, right, up, down), resulting in 36 (9 actionable states x 4 possible actions) Q-values.</p>

<p>This project aims to train an agent to play Out Run via its game screens, so for the sake of argument, let´s consider that each game screen is transformed into a 80x80 greyscale image (each pixel value ranging from 0 to 255), and that each transformed image represents a state. 6400 pixels (80x80) and 256 possible values per pixel translate to 256<sup>6400</sup> possible states. This value alone is a good indicator of how inflated the number of possible Q-values will be.</p>

<p>Multiplying <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py">9 possible actions</a> by 256<sup>6400</sup> possible states results in 256<sup>6400</sup> x 9 possible Q-values. If we use multiple and/or colored images for state representation, then this value will be even higher. Quite unwieldy if we want to store these values in a table or similar structure.</p>

<h2 id="enter-deep-neural-networks">Enter deep neural networks</h2>

<p>Deep neural networks work quite well for inferring the mapping implied by data, giving them the ability to predict an approximated output from an input that they never saw before. No longer do we need to store all state/action pair’s Q-values, we can now model these mappings in a more general, less redundant way. These networks also automatically learn a set of internal features which are useful in complex non-linear mapping domains, such as image processing, releasing us from laborious feature handcrafting tasks.</p>

<p>This is perfect. We can now use a deep neural network to approximate the Q-function: the network would accept a state/action combination as input and would output the corresponding Q-value. Training-wise, we would need the network’s Q-value output for a given state/action combo (obtained through a forward pass) and the target Q-value, which is calculated through the expression: <script type="math/tex">r_{t+1} + \gamma \underset{a}\max Q(s_{t+1}, a)</script>. With these two values, we can perform a gradient step on the squared difference between the target Q-value and the network’s output.</p>

<p>This is perfect, but there is still room for improvement. Imagine we have 5 possible actions for any given state: when calculating the target Q-value, to get the optimal future value estimate (consequent state’s maximum Q-value) we need to ask (forward pass) our neural network for a Q-value 5 times per learning step.</p>

<p>Another approach (used in <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DeepMind’s</a> <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">network</a>) would be to feed in the game’s screens and have the network output the Q-value for each possible action. This way, a single forward pass would output all the Q-values for a given state, translating into one forward pass per optimal future value estimate.</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/q_network_formulations.png'><img src='/files/dqn_outrun/q_network_formulations.png'></a></center><figcaption class='media-caption'> Image courtesy of Tambet Matiisen’s <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning">Demystifying Deep Reinforcement Learning</a> - Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind papers. </figcaption></figure> </p>

<p>Q-learning and deep neural networks are the center pieces of a deep Q-network reinforcement learning agent and I think that by understanding them and how they fit together, it can be easier to picture how the algorithm works as a whole.</p>

<h1 id="implementation">Implementation</h1>

<center><a href="https://lopespm.com/files/dqn_outrun/overall_view_play.png"><img align="center" src="https://lopespm.com/files/dqn_outrun/overall_view_play.png" alt="" /></a></center>

<p />

<p>Above is an overall representation of how the different components relate during a play evaluation, centered around the <code>deep Q-network for playing</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, the main decision component.</p>

<p>Each game screen is resized to a desaturated 80x80 pixels image (opposed to 84x84 on DeepMind’s papers), and if you might be wondering why each state is a sequence of four game screens instead of one, that is because the agent’s history is used for better motion perception. Achieving this requires a sequence of preprocessed images to be stacked in channels (like you would stack RGB channels on a colored image) and fed to the network. Note that RGB channels and agent history could be used simultaneously for state representation. For example, with three channels per (RGB) image and an agent history length of four, the network would be fed twelve channels per input state.</p>

<p>The <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/q_network.py#L121">network’s architecture</a> is essentially the same used by DeepMind, except for the first convolutional neural network’s input (80x80x4 instead of 84x84x4, to account for the different input sizes) and the linear layer’s output (9 instead of 18, to account for the different number of actions available)</p>

<p>The algorithm used to train this network is well described <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">here (page 7)</a> and <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">here</a>, but I would like to present it graphically, to hopefully provide some further intuition.</p>

<p>Below is <a href="https://github.com/lopespm/agent-trainer">agent trainer</a>´s implementation of the aforementioned algorithm. It adds some new concepts which were not approached by this article:</p>

<ul>
  <li>Experience replay mechanism sported by replay memories: randomly samples previous transitions, thereby smoothing the training distribution over many past behaviors</li>
  <li>Separate training network, cloned at fixed intervals to the target playing network, making the algorithm more stable when compared with standard online Q-learning</li>
  <li>ε-greedy policy to balance exploitation/exploration</li>
</ul>

<center><a href="https://lopespm.com/files/dqn_outrun/overall_view_train_merged.png"><img align="center" src="https://lopespm.com/files/dqn_outrun/overall_view_train_merged.png" alt="" /></a></center>

<p />

<h2 id="reward-function">Reward function</h2>

<p>The reward function’s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. April Yu et al. have an interesting paper on <a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">simulated autonomous vehicle control</a> which details a DQN agent used to drive a game that strongly resembles Out Run (<a href="http://codeincomplete.com/games/racer/v4-final/">JavaScript Racer</a>). Based on their reward function experiments, I’ve built a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L60">function</a> which rewards logarithmically based on speed and penalizes when the car is off-road, crashed or stopped. </p>

<p><figure class='image'><center><a href='/files/dqn_outrun/reward_function_plot.png'><img src='/files/dqn_outrun/reward_function_plot.png'></a></center><figcaption class='media-caption'> Reward values for when the car is not crashed or off-road </figcaption></figure> </p>

<h1 id="deployment">Deployment</h1>

<p>Run the trainer and emulator on your local machine by following the guide available on <a href="https://github.com/lopespm/agent-trainer/blob/master/README.md">agent-trainer’s readme</a>.</p>

<p>It is also possible to deploy the agent to an AWS EC2 instance or generic Linux remote machine by using a set of bash scripts offered by <a href="https://github.com/lopespm/agent-trainer-deployer">agent-trainer-deployer</a>.</p>

<h3 id="aws-ec2">AWS EC2</h3>

<p>Amazon allows you to bid on spare EC2 computing capacity via <a href="https://aws.amazon.com/ec2/spot/">spot instances</a>. These can cost a fraction of on-demand ones, and for this reason were chosen as the prime method for training in this project, leading to the need for mid-training instance termination resilience.</p>

<p>To accommodate this scenario, the deployment scripts and agent-trainer are designed to support train session resumes. To persist results and decrease boot up time between sessions, a long-lived EBS volume is attached to the live instance. The volume contains the training results, agent-trainer´s source code, cannonball’s source code, dockerfiles and their respective docker images. </p>

<p><figure class='image'><center><a href='/files/dqn_outrun/deployed-diagram.png'><img src='/files/dqn_outrun/deployed-diagram.png'></a></center><figcaption class='media-caption'> Relationship between components when deploying agent-trainer to an AWS EC2 instance </figcaption></figure> </p>

<h1 id="results">Results</h1>

<p>The hyperparameters used on all sessions mimic the ones used on DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a>, except for the number of frames skipped between actions, which are spaced apart by 450ms (equivalent to 13 frames) on agent-trainer.</p>

<p>The Out Run game, as you would play it in an arcade, clutters the road with various cars in order to make the game more challenging. In-game traffic was disabled for both training and evaluation plays, rendering a more achievable starting point for these experiments. Training with random traffic could be an interesting posterior experiment.</p>

<p>Some experiments were made by increasing the discount factor up its final value during training, as proposed on <a href="https://arxiv.org/abs/1512.02011">“How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies”</a>, but did not achieve better stability or rewards when compared to a fixed 0.99 discount factor. The aforementioned paper also proposes decaying the learning rate during training, which increased stability and performance significantly. Decaying the learning rate without minimum value clipping yielded the best results.</p>

<p>Another improvement was to train the game without a time limit, meaning that the training episode would only finish when the car reached the last stage’s end. This allowed for a broader replay memory training set, since the agent traversed a wide range of different tracks and settings.</p>

<p>Play evaluation was the same between all experiments, this is, the agent was evaluated by playing on the default 80 second, easy mode.</p>

<p>Here is a summary of the most relevant training sessions (you can find their models, metrics and visualizations on <a href="https://github.com/lopespm/agent-trainer-results">agent-trainer-results</a>):</p>

<table>
  <thead>
    <tr>
      <th>Session</th>
      <th>M</th>
      <th>Training<br />game mode</th>
      <th>Learning rate decay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>201609040550_5010eps</td>
      <td>a)</td>
      <td>timed; easy</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_2700eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_7300eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609160922_54eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609171218_175eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>unclipped learning rate decay</td>
    </tr>
  </tbody>
</table>

<p class="media-caption"><a href="https://github.com/lopespm/agent-trainer-results">Training sessions</a> summary: session names are formed by &#60;session ID&#62;_&#60;number of episodes trained&#62;<br />(M)achine used:  a) AMD Athlon(tm) II X2 250 Processor @ 3GHz; 2GB RAM DDR3-1333 SDRAM; SSD 500 GB: Samsung 850 EVO (CPU only training); b) AWS EC2 g2.2xlarge (GPU enabled instance), 200 GB General Purpose SSD (GP2)</p>

<p />

<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
   <div class="youtube-player video-frame-two" data-id="1Gpl9Xc-E8M"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609111241_2700eps</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="6F3eCoCw57E"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609171218_175eps<br class="video-br-end" /></p>
</div>
</center>

<p>Notice on the videos above how the timed mode trained session <code>201609111241_2700eps</code> reaches the first checkpoint about five seconds earlier than the unlimited time mode trained session <code>201609171218_175eps</code>, but proceeds to drive off-road two turns after. Its stability gets increasingly compromised as more episodes are trained, which can be observed by the rampant loss increase before 7300 episodes are reached (<code>201609111241_7300eps</code>):</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/201609111241<em>7300eps/metrics_in_train.png'><img src='/files/dqn_outrun/201609111241<em>7300eps/metrics_in_train.png'></a></center><figcaption class='media-caption'> Training metrics for session 201609111241</em>7300eps </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609111241<em>7300eps/metrics_trained_play.png'><img src='/files/dqn_outrun/201609111241<em>7300eps/metrics_trained_play.png'></a></center><figcaption class='media-caption'> Play evaluation metrics for session 201609111241</em>7300eps: using ε=0.0; evaluation made at the end of every 20 training episodes </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/metrics_in_train.png'><img src='/files/dqn_outrun/201609171218<em>175eps/metrics_in_train.png'></a></center><figcaption class='media-caption'> Training metrics for session 201609171218</em>175eps </figcaption></figure><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/metrics_trained_play.png'><img src='/files/dqn_outrun/201609171218<em>175eps/metrics_trained_play.png'></a></center><figcaption class='media-caption'> Play evaluation metrics 201609171218</em>175eps: using ε=0.0; evaluation made at the end of every training episode </figcaption></figure></p>

<p>Both <code>201609111241_2700eps</code> and <code>201609111241_7300eps</code> timed trained sessions mostly drive off-road and stall after the first stage, whereas the unlimited time mode trained session <code>201609171218_175eps</code> can race through all the stages crashing <em>only</em> three times (as shown on the article’s first video) and is able to match the performance of a timed trained session when evaluated on the default easy timed mode. </p>

<p>Below is the loss plot for <code>201609160922_54eps</code> and <code>201609171218_175eps</code>, both trained using the game’s unlimited time mode, difference being that <code>201609160922_54eps</code> keeps a fixed learning rate and <code>201609171218_175eps</code> decays it every 50100 steps:</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/loss<em>201609171218</em>201609160922.png'><img src='/files/dqn_outrun/loss<em>201609171218</em>201609160922.png'></a></center><figcaption class='media-caption'> Loss comparison between sessions <font color="#9c27b0">■</font> 201609160922<em>54eps and <font color="#009688">■</font> 201609171218</em>175eps, as viewed on <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a> </figcaption></figure></p>

<p>Other representative visualizations:</p>

<p><figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/t-SNE_timed_easy_mode.png'><img src='/files/dqn_outrun/201609171218<em>175eps/t-SNE_timed_easy_mode.png'></a></center><figcaption class='media-caption'> t-SNE visualization, generated by letting the agent play one game on timed easy mode. Agent is using the network trained on session 201609171218</em>175eps </figcaption></figure>
<figure class='image'><center><a href='/files/dqn_outrun/201609171218<em>175eps/t-SNE_no_time_mode.png'><img src='/files/dqn_outrun/201609171218<em>175eps/t-SNE_no_time_mode.png'></a></center><figcaption class='media-caption'> t-SNE visualization, generated by letting the agent play one game on unlimited time mode. Agent is using the network trained on session 201609171218</em>175eps </figcaption></figure>
 <figure class='image'><center><a href='/files/dqn_outrun/conv_net_filters.png'><img src='/files/dqn_outrun/conv_net_filters.png'></a></center><figcaption class='media-caption'> Visualization of the first convolutional network layer’s filters. These can be viewed via <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a></figcaption></figure> </p>

<p><br /></p>

<h1 id="final-notes">Final Notes</h1>

<h2 id="plugging-other-problems-and-games">Plugging other problems and games</h2>

<p>Agent-trainer was not built from the get-go to train games or problems other than Out Run, but I think it would be interesting to perform a thought exercise on what would be necessary to do so.</p>

<p>There are three main areas in which <a href="https://github.com/lopespm/agent-trainer">agent-trainer</a> has domain knowledge about Out Run:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer/tree/master/agent/game"><code>game</code></a> package, which contains
    <ul>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py"><code>Action</code></a> enumeration: describes all the possible actions in the game.</li>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/cannonball_wrapper.py"><code>cannonball_wrapper</code></a> module: only this module has access to the cannonball emulator. It translates the aforementioned actions into game actions and is accessed by methods such as <code>start_game()</code>, <code>reset()</code> and <code>speed()</code>.</li>
    </ul>
  </li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> class: contains the reward function. Instead of using a generic reward function like DeepMind, it was chosen to have a tailor-made reward function for Out Run, which takes into account the car’s speed and its off-road and crash status.</li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/visualization/metrics.py"><code>metrics</code></a> module: aware of the <code>speed</code> metric, which is Out Run specific, and <code>score</code>, which is game specific domain knowledge.</li>
</ul>

<p>Training another game would require the creation of a new wrapper with the same interface as <code>cannonball_wrapper</code>, a new <code>Action</code> enumerator specific to the game, a new <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> with a different reward function and the removal/replacement of the <code>speed</code> metric.</p>

<p>Apart from the previously mentioned steps, solving generic problems would require the preprocessor to be changed/replaced if images were not to be used for state representation. An option would be to create a new preprocessor class with a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/image_preprocessor.py#L13"><code>process(input)</code></a> method, tweak the hyperparameters as required (so that the network knows which dimensions to expect on its input), and finally inject the newly created class in <a href="https://github.com/lopespm/agent-preprocessor/blob/master/agent/trainer/episode.py#L300"><code>EpisodeRunner</code></a>, replacing the old preprocessor class.</p>

<h2 id="further-references">Further references</h2>

<p>I am not a machine learning expert, but from my learner’s point of view, if you are interested in getting your feet wet, Andrew Ng’s Machine Learning Course is as a great starting point. It is freely available on the <a href="https://www.coursera.org/learn/machine-learning">Coursera online learning platform</a>. This was my first solid contact with the subject and served as a major stepping stone for related topics such as Reinforcement Learning.</p>

<p><a href="https://www.udacity.com/course/deep-learning--ud730">Udacity Google Deep Learning</a>: this free course tackles some of the popular deep learning techniques, all the while using tensorflow. I did this right after Andrew Ng’s course and found it to leave the student with less support during lessons - less hand-holding if you will - and as result I spent a good amount of time dabbling to reach a solution for the assignments. </p>

<p>As a side note, I started building this project by the end of the Deep Learning course, mostly because I wanted to apply and consolidate the concepts I learnt into something more practical and to share this knowledge further, so it could hopefully help more people who are interested in this.</p>

<p>Other useful resources:</p>

<ul>
  <li>DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a> and its respective <a href="https://sites.google.com/a/deepmind.com/dqn/">source code</a></li>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></li>
  <li><a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">Deep Reinforcement Learning for Simulated Autonomous Vehicle Control</a></li>
  <li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
  <li><a href="https://www.udacity.com/course/reinforcement-learning--ud600">Udacity Reinforcement Learning by Georgia Tech</a></li>
  <li><a href="https://www.youtube.com/watch?v=dV80NAlEins">Deep learning lecture by Nando de Freitas</a></li>
  <li><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/">Learning reinforcement learning (with code, exercises and solutions)</a></li>
  <li><a href="https://gym.openai.com/">OpenAI Gym</a>: quoting the project’s page: <em>”a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Go”</em></li>
  <li><a href="https://github.com/devsisters/DQN-tensorflow">Tensorflow implementation</a> of Human-Level Control through Deep Reinforcement Learning, by Devsisters corp.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>All source code is available on GitHub:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer">Agent Trainer</a>: the core python+tensorflow application</li>
  <li><a href="https://github.com/lopespm/cannonball">Cannonball</a>: custom Cannonball (Out Run game emulator) fork which contains the changes needed to access the emulator externally</li>
  <li><a href="https://github.com/lopespm/agent-trainer-deployer">Agent Trainer Deployer</a>: bash scripts to deploy agent-trainer to a generic remote machine or AWS EC2 instance</li>
  <li><a href="https://github.com/lopespm/agent-trainer-docker">Agent Trainer Docker</a>: Dockerfiles used when deploying agent-trainer to a remote machine</li>
  <li><a href="https://github.com/lopespm/agent-trainer-results">Agent Trainer Results</a>: Collection of training sessions, each containing their resulting network, metrics and visualizations</li>
</ul>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>“Deep Q-network for playing” in this project is equivalent to DeepMind’s “target network $\hat Q$” and “Deep Q-network for training” is equivalent to DeepMind’s “network Q”<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
