
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Implementation: Autocomplete System Design for Large Scale - Byte Tank</title>
  <meta name="author" content="Pedro Lopes">

  
  <meta name="description" content="Implementation of Autocomplete / Typeahead System Design for Google Large Scale - using Docker Compose">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://lopespm.com/2020/08/03/implementation-autocomplete-system-design.html">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/stylesheets/data-table.css" media="screen, projection" rel="stylesheet" type="text/css" />
  <link href="/atom.xml" rel="alternate" title="Byte Tank" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js" async></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic&display=swap" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic&display=swap" rel="stylesheet" type="text/css">
<link href='https://fonts.googleapis.com/css?family=Open+Sans&display=swap' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Fjalla+One&display=swap' rel='stylesheet' type='text/css'>
<link href="/stylesheets/rocket-loader/loader.min.css" rel="stylesheet" type="text/css">
  <meta property="og:title" content="Implementation: Autocomplete System Design for Large Scale - Byte Tank" />
<meta property="og:description" content="Implementation of Autocomplete / Typeahead System Design for Google Large Scale - using Docker Compose" />
<meta property="og:url" content="https://lopespm.com/2020/08/03/implementation-autocomplete-system-design.html" />
<meta property="og:image" content="https://lopespm.com/files/autocomplete/pintro_image_1200.png" />
<meta property="og:site_name" content="Byte Tank" />
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="article" />

<meta name="twitter:title" content="Implementation: Autocomplete System Design for Large Scale - Byte Tank">
<meta name="twitter:description" content="Implementation of Autocomplete / Typeahead System Design for Google Large Scale - using Docker Compose">

<meta name="twitter:creator" content="@lopes_pm">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lopespm.com/files/autocomplete/pintro_image_1200.png">


  
  <script async>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-39962763-1', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body   class="collapse-sidebar sidebar-footer" >
  <script src="https://www.youtube.com/iframe_api" defer></script>
<script defer>

  /* Based on: http://labnol.org/?p=27941 */

  // function onYouTubeIframeAPIReady() {
  //   initialize();
  // }

  document.addEventListener("DOMContentLoaded", initialize );

  function initialize() {
    var div, n,
      v = document.getElementsByClassName("youtube-player");
    for (n = 0; n < v.length; n++) {
      div = document.createElement("div");
      div.setAttribute("data-id", v[n].dataset.id);
      div.innerHTML = labnolThumb(v[n].dataset.id, v[n].dataset.thumbsize);
      div.onclick = labnolIframe;

      v[n].classList.add("ytp-thumbnail-overlay");
      v[n].classList.add("ytp-cued-thumbnail-overlay");

      v[n].appendChild(div);
    }
  }

  function labnolThumb(id, thumbSize) {
    var thumb = getThumb(thumbSize);
    var thumbWithId = thumb.replace("ID", id);

    var youtubePlayButton = '<div class="play-youtube-button"><button class="ytp-large-play-button ytp-button" aria-label="Play Video"><svg height="100%" version="1.1" viewBox="0 0 68 48" width="100%"><path class="ytp-large-play-button-bg" d="m .66,37.62 c 0,0 .66,4.70 2.70,6.77 2.58,2.71 5.98,2.63 7.49,2.91 5.43,.52 23.10,.68 23.12,.68 .00,-1.3e-5 14.29,-0.02 23.81,-0.71 1.32,-0.15 4.22,-0.17 6.81,-2.89 2.03,-2.07 2.70,-6.77 2.70,-6.77 0,0 .67,-5.52 .67,-11.04 l 0,-5.17 c 0,-5.52 -0.67,-11.04 -0.67,-11.04 0,0 -0.66,-4.70 -2.70,-6.77 C 62.03,.86 59.13,.84 57.80,.69 48.28,0 34.00,0 34.00,0 33.97,0 19.69,0 10.18,.69 8.85,.84 5.95,.86 3.36,3.58 1.32,5.65 .66,10.35 .66,10.35 c 0,0 -0.55,4.50 -0.66,9.45 l 0,8.36 c .10,4.94 .66,9.45 .66,9.45 z" fill="#1f1f1e" fill-opacity="0.81"></path><path d="m 26.96,13.67 18.37,9.62 -18.37,9.55 -0.00,-19.17 z" fill="#fff"></path><path d="M 45.02,23.46 45.32,23.28 26.96,13.67 43.32,24.34 45.02,23.46 z" fill="#ccc"></path></svg></button></div>';



    var bg = '<div class="bg" style=\'background-image: url(\"' + thumbWithId + '\')\"></div>';
    var fadeBg = '<div class="fadeBg"></div>';
    return bg + fadeBg + youtubePlayButton;
  }

  function getThumb(thumbSize) {
    if (thumbSize === "0")
      return 'https://i.ytimg.com/vi/ID/hqdefault.jpg';
    if (thumbSize === "1")
      return 'https://i.ytimg.com/vi/ID/sddefault.jpg';
    else if (thumbSize === "2")
      return 'https://i.ytimg.com/vi/ID/maxresdefault.jpg';
    return 'https://i.ytimg.com/vi/ID/sddefault.jpg';
  }

  function labnolIframe() {
    var container = document.createElement("div");

    var loaderContainerOuter = document.createElement("div");
    loaderContainerOuter.setAttribute("style", "display: table; position: absolute; top: 0; left: 0; height: 100%; width: 100%; background-color: black; z-index: 200;");
    var loaderContainerMiddle = document.createElement("div");
    loaderContainerMiddle.setAttribute("style", "  display: table-cell; vertical-align: middle;");
    var loaderContainerInner = document.createElement("div");
    loaderContainerInner.setAttribute("style", "margin-left: auto; margin-right: auto; width: 400px;");

    loaderContainerOuter.appendChild(loaderContainerMiddle);
    loaderContainerMiddle.appendChild(loaderContainerInner);

    const loader = Rocket.loader({
      target: loaderContainerInner,
      size: 'large',
      type: 'spinner',
      colour: 'grey-blue-dark'
      //body: "Loading Video"
    });

    var containerPlayer = document.createElement("div");
    container.appendChild(containerPlayer);
    container.appendChild(loaderContainerOuter);

    this.parentNode.replaceChild(container, this);

    var didLoadAndPlay = 0;
    player = new YT.Player(containerPlayer, {
      videoId: this.dataset.id,
      playerVars: { 'autoplay': 1 },
      events: {
        'onReady': function(event) {
          event.target.playVideo();

          // Fallback if onStateChange is not called
          setInterval(function () {
            if (didLoadAndPlay === 0) {
              container.removeChild(loaderContainerOuter);
              didLoadAndPlay = 1;
            }
          }, 5000);
        },
        'onStateChange': function (event) {
          if (didLoadAndPlay === 0) {
            container.removeChild(loaderContainerOuter);
            didLoadAndPlay = 1;
          }
        }
      }
    });

  }

</script>


<script src="/javascripts/rocket-loader/tools.min.js" defer></script>
<script src="/javascripts/rocket-loader/loader.min.js" defer></script>
  <!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config" async>
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" type="text/javascript" async></script>
  <header role="banner"><hgroup>
  <h1><a href="/">Byte Tank</a></h1>
  
    <h2>Pedro Lopes Blog</h2>
  
</hgroup>


</header>
  <nav role="navigation">

<ul class="subscription">
  <li><a href="https://www.youtube.com/@lopespm"><img src="/images/cn_youtube_19.png" height="19px" width="19px" alt="Go to YouTube Channel"/></a></li>
  <li><a href="https://twitter.com/lopes_pm"><img src="/images/cn_twitter_19.png" height="19px" width="19px" alt="Go to Twitter"/></a></li>
  <li><a href="https://github.com/lopespm"><img src="/images/cn_github_19.png" height="19px" width="19px" alt="Go to GitHub"/></a></li>
  <li><a href="/atom.xml"><img src="/images/cn_rss_19.png" height="19px" width="19px" alt="RSS Feed"/></a></li>  
</ul>

  

<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/notes">Notes</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="article-full">
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Implementation: Autocomplete System Design for Large Scale</h1>
    
    
      <p class="meta">
        








  


<time datetime="2020-08-03T00:00:00+00:00" pubdate data-updated="true">Aug 3<span>rd</span>, 2020</time>
        <!--
        
        -->
        
      </p>
    
  </header>


<div class="entry-content"><p>This article will go through my implementation/design of a large scale autocomplete/typeahead suggestions system design, like the suggestions one gets when typing a Google search.</p>

<p>This design was implemented using Docker Compose<sup id="fnref:10"><a href="#fn:10" rel="footnote">1</a></sup>, and you can find the source code here: <a href="https://github.com/lopespm/autocomplete">https://github.com/lopespm/autocomplete</a></p>

<div style="margin-bottom: 20px">
    <div class="youtube-player" data-id="UFr5Mb7YXRg"></div>
</div>
<p class="media-caption media-caption-one">YouTube video overview of this implementation</p>

<!--more-->

<h1 id="requirements">Requirements</h1>

<p>The design has to accommodate a Google-like scale of about 5 billion daily searches, which translates to about 58 thousand queries per second. We can expect 20% of these searches to be unique, this is, 1 billion queries per day.</p>

<p>If we choose to index 1 billion queries, with 15 characters on average per query<sup id="fnref:12"><a href="#fn:12" rel="footnote">2</a></sup> and 2 bytes per character (we will only support the english locale), then we will need about 30GB of storage to host these queries.</p>

<h2 id="functional-requirements">Functional Requirements</h2>

<ul>
  <li>Get a list of top phrase suggestions based on the user input (a prefix)</li>
  <li>Suggestions ordered by weighting the frequency and recency of a given phrase/query<sup id="fnref:11"><a href="#fn:11" rel="footnote">3</a></sup></li>
</ul>

<p>The main two APIs will be:</p>

<ul>
  <li><em>top-phrases(prefix)</em>: returns the list of top phrases for a given prefix</li>
  <li><em>collect-phrase(phrase)</em>: submits the searched phrase to the system. This phrase will later be used by the assembler to build a data structure which maps a prefix to a list of top phrases</li>
</ul>

<h2 id="non-functional-requirements">Non-Functional Requirements</h2>

<ul>
  <li><em>Highly Available</em></li>
  <li><em>Performant</em> - response time for the top phrases should be quicker than the user’s type speed (&lt; 200ms)</li>
  <li><em>Scalable</em> - the system should accommodate a large number of requests, while still maintaining its performance</li>
  <li><em>Durable</em> - previously searched phrases (for a given timespan) should be available, even if there is a hardware fault or crash</li>
</ul>

<h1 id="design--implementation">Design &amp; Implementation</h1>

<h2 id="high-level-design">High-level Design</h2>

<div style="margin-bottom: 7px">
<center>
          <picture aria-label="High level design diagram">
             <source type="image/webp" srcset="/files/autocomplete/phigh_level_design.webp" aria-label="High level design diagram" />
             <source type="image/png" srcset="/files/autocomplete/phigh_level_design.png" aria-label="High level design diagram" />
             <img src="/files/autocomplete/phigh_level_design.png" aria-label="High level design diagram" />
           </picture>
         </center>
</div>

<p>The two main sub-systems are: </p>

<ul>
  <li>the distributor, which handles the user’s requests for the top phrases of a given prefix</li>
  <li>the assembler, which collects user searches and assembles them into a data structure that will later be used by the distributor</li>
</ul>

<h2 id="detailed-design">Detailed Design</h2>

<div style="margin-top: 25px">
<center>
          <picture aria-label="Detailed design diagram">
             <source type="image/webp" srcset="/files/autocomplete/psystem_design_diagram.webp" aria-label="Detailed design diagram" />
             <source type="image/png" srcset="/files/autocomplete/psystem_design_diagram.png" aria-label="Detailed design diagram" />
             <img src="/files/autocomplete/psystem_design_diagram.png" aria-label="Detailed design diagram" />
           </picture>
         </center>
</div>

<p>This implementation uses off-the-shelf components like kafka (message broker), hadoop (map reduce and distributed file system), redis (distributed cache) and nginx (load balancing, gateway, reverse proxy), but also has custom services built in python, namely the trie distribution and building services. The trie data structure is custom made as well.</p>

<p>The backend services in this implementation are built to be self sustainable and don’t require much orchestration. For example, if an active backend host stops responding, it’s corresponding ephemeral znode registry eventually disappears, and another standby backend node takes its place by attempting to claim the position via an <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Ephemeral+Nodes">ephemeral</a> <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Sequence+Nodes+--+Unique+Naming">sequential</a> znode on zookeeper.</p>

<h3 id="trie-the-bedrock-data-structure">Trie: the bedrock data structure</h3>

<p>The data structure used by, and provided to the distributor is a <a href="https://en.wikipedia.org/wiki/Trie">trie</a>, with each of its prefix nodes having a list of top phrases. The top phrases are referenced using the <a href="https://en.wikipedia.org/wiki/Flyweight_pattern">flyweight pattern</a>, meaning that the string literal of a phrase is stored only once. Each prefix node has a list of top phrases, which are a list of references to string literals.</p>

<p>As we’ve seen before, we will need about 30GB to index 1 billion queries, which is about the memory we would need for the above mentioned trie to store 1 billion queries. Since we want to keep the trie in memory to enable fast lookup times for a given query, we are going to partition the trie into multiple tries, each one on a different machine. This relieves the memory load on any given machine.</p>

<p>For increased availability, the services hosting these tries will also have multiple replicas. For increased durability, the serialized version of the tries will be available in a distributed file system (HDFS), and these can be rebuilt via map reduce tasks in a predictable, deterministic way.</p>

<h2 id="information-flow">Information Flow</h2>

<h3 id="assembler-collect-data-and-assemble-the-tries">Assembler: collect data and assemble the tries</h3>

<ol>
  <li>Client submits the searched phrase to the gateway via <code>http://localhost:80/search?phrase="a user query"</code></li>
  <li>Since the search service is outside the scope of this implementation, the gateway directly sends the searched phrase to the collector’s load balancer via <code>http://assembler.collector-load-balancer:6000/collect-phrase?phrase="a user query"</code></li>
  <li>The collector’s load balancer forwards the request to one of the collector backends via <code>http://assembler.collector:7000/collect-phrase?phrase="a user query"</code></li>
  <li>The collector backend sends a message to the <em>phrases</em> topic to the message broker (kafka). The key and value are the phrase itself <sup id="fnref:1"><a href="#fn:1" rel="footnote">4</a></sup></li>
  <li>The Kafka Connect HDFS Connector <code>assembler.kafka-connect</code> dumps the messages from the <em>phrases</em> topic into the <code>/phrases/1_sink/phrases/{30 minute window timestamp}</code><sup id="fnref:2"><a href="#fn:2" rel="footnote">5</a></sup> folder <sup id="fnref:3"><a href="#fn:3" rel="footnote">6</a></sup></li>
  <li>Map Reduce jobs are triggered <sup id="fnref:4"><a href="#fn:4" rel="footnote">7</a></sup>: they will reduce the searched phrases into a single HDFS file, by weighting the recency and frequency of each phrase <sup id="fnref:5"><a href="#fn:5" rel="footnote">8</a></sup>
    <ol>
      <li>A <code>TARGET_ID</code> is generated, according to the current time, for example <code>TARGET_ID=20200807_1517</code></li>
      <li>The first map reduce job is executed for the <em>K</em> <sup id="fnref:6"><a href="#fn:6" rel="footnote">9</a></sup> most recent <code>/phrases/1_sink/phrases/{30 minute window timestamp}</code> folders, and attributes a base weight for each of these (the more recent, the higher the base weight). This job will also sum up the weights for the same phrase in a given folder. The resulting files will be stored in the <code>/phrases/2_with_weight/2_with_weight/{TARGET_ID}</code> HDFS folder</li>
      <li>The second map reduce job will sum up all the weights of a given phrase from <code>/phrases/2_with_weight/2_with_weight/{TARGET_ID}</code> into <code>/phrases/3_with_weight_merged/{TARGET_ID}</code></li>
      <li>The third map reduce job will order the entries by descending weight, and pass them through a single reducer, in order to produce a single file. This file is placed in <code>/phrases/4_with_weight_ordered/{TARGET_ID}</code></li>
      <li>The zookeper znode <code>/phrases/assembler/last_built_target</code> is set to the <code>TARGET_ID</code></li>
    </ol>
  </li>
  <li>The Trie Builder service, which was listening to changes in the <code>/phrases/assembler/last_built_target</code> znode, builds a trie for each partition<sup id="fnref:7"><a href="#fn:7" rel="footnote">10</a></sup>, based on the <code>/phrases/4_with_weight_ordered/{TARGET_ID}</code> file. For example, one trie may cover the prefixes until <em>mod</em>, another from <em>mod</em> to <em>racke</em>, and another from <em>racke</em> onwards.
    <ol>
      <li>Each trie is serialized and written into the <code>/phrases/5_tries/{TARGET_ID}/{PARTITION}</code> HDFS file (e.g. <code>/phrases/5_tries/20200807_1517/mod\|racke</code>), and the zookeeper znode <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/trie_data_hdfs_path</code> is set to the previously mentioned HDFS file path.</li>
      <li>The service sets the zookeper znode <code>/phrases/distributor/next_target</code> to the <code>TARGET_ID</code></li>
    </ol>
  </li>
</ol>

<h3 id="transfering-the-tries-to-the-distributor-sub-system">Transfering the tries to the Distributor sub-system</h3>

<ol>
  <li>The distributor backend nodes can either be in active mode (serving requests) or standby mode. The nodes in standby mode will fetch the most recent tries, load them into memory, and mark themselves as ready to take over the active position. In detail:
    <ol>
      <li>The standby nodes, while listening to changes to the znode <code>/phrases/distributor/next_target</code>, detect its modification and create an <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Ephemeral+Nodes">ephemeral</a> <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Sequence+Nodes+--+Unique+Naming">sequential</a> znode, for each partition, one at a time, inside the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/</code> znode. If the created znode is one of the first R znodes (R being the number of replica nodes per partition <sup id="fnref:8"><a href="#fn:8" rel="footnote">11</a></sup>), proceed to the next step. Otherwise, remove the znode from this partition and try to join the next partition.</li>
      <li>The standby backend node fetches the serialized trie file from <code>/phrases/5_tries/{TARGET_ID}/{PARTITION}</code>, and starts loading the trie into memory.</li>
      <li>When the trie is loaded into memory, the standby backend node marks itself as ready by setting the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/{CREATED_ZNODE}</code> znode to the backend’s hostname.</li>
    </ol>
  </li>
  <li>The Trie Backend Applier service polls the <code>/phrases/distributor/{TARGET_ID}/</code> sub-znodes (the <code>TARGET_ID</code> is the one defined in <code>/phrases/distributor/next_target</code>), and checks if all the nodes in all partitions are marked as ready
    <ol>
      <li>If all of them are ready for this next <code>TARGET_ID</code>, the service, in a single transaction, changes the value of the <code>/phrases/distributor/next_target</code> znode to empty, and sets the <code>/phrases/distributor/current_target</code> znode to the new <code>TARGET_ID</code>. With this single step, all of the standby backend nodes which were marked as ready will now be active, and will be used for the following Distributor requests.</li>
    </ol>
  </li>
</ol>

<h3 id="distributor-handling-the-requests-for-top-phrases">Distributor: handling the requests for Top Phrases</h3>

<p>With the distributor’s backend nodes active and loaded with their respective tries, we can start serving top phrases requests for a given prefix:</p>

<ol>
  <li>Client requests the gateway for the top phrases for a given prefix via <code>http://localhost:80/top-phrases?prefix="some prefix"</code></li>
  <li>The gateway sends this request to the distributor’s load balancer via <code>http://distributor.load-balancer:5000/top-phrases?prefix="some prefix"</code></li>
  <li>The load balancer forwards the request to one of the frontends via <code>http://distributor.frontend:8000/top-phrases?prefix="some prefix"</code></li>
  <li>The frontend service handles the request:
    <ol>
      <li>The frontend service checks if the distributed cache (redis) has an entry for this prefix <sup id="fnref:9"><a href="#fn:9" rel="footnote">12</a></sup>. If it does, return these cached top phrases. Otherwise, continue to next step</li>
      <li>The frontend service gets the partitions for the current <code>TARGET_ID</code> from zookeeper (<code>/phrases/distributor/{TARGET_ID}/partitions/</code> znode), and picks the one that matches the provided prefix</li>
      <li>The frontend service chooses a random znode from the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/</code> znode, and gets its hostname</li>
      <li>The frontend service requests the top phrases, for the given prefix, from the selected backend via <code>http://{BACKEND_HOSTNAME}:8001/top-phrases="some prefix"</code>
        <ol>
          <li>The backend returns the list of top phrases for the given prefix, using its corresponding loaded trie</li>
        </ol>
      </li>
      <li>The frontend service inserts the list of top phrases into the distributed cache (cache aside pattern), and returns the top phrases</li>
    </ol>
  </li>
  <li>The top phrases response bubbles up to the client</li>
</ol>

<h2 id="zookeper-znodes-structure">Zookeper Znodes structure</h2>

<p>Note: Execute the shell command <code>docker exec -it zookeeper ./bin/zkCli.sh</code> while the system is running to explore the current zookeeper’s znodes.</p>

<ul>
  <li>phrases
    <ul>
      <li>distributor</li>
      <li>assembler
        <ul>
          <li>last_built_target <em>- set to a <code>TARGET_ID</code></em></li>
        </ul>
      </li>
      <li>distributor
        <ul>
          <li>current_target <em>- set to a <code>TARGET_ID</code></em></li>
          <li>next_target <em>- set to a <code>TARGET_ID</code></em></li>
          <li><code>{TARGET_ID}</code> <em>- e.g. 20200728_2241</em>
            <ul>
              <li>partitions
                <ul>
                  <li>|<em>{partition 1 end}</em>
                    <ul>
                      <li>trie_data_hdfs_path <em>- HDFS path where the serialized trie is saved</em></li>
                      <li>nodes
                        <ul>
                          <li>0000000000</li>
                          <li>0000000001</li>
                          <li>0000000002</li>
                          <li>…</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li><em>{partition 2 start}</em>|<em>{partition 2 end}</em>
                    <ul>
                      <li>…</li>
                    </ul>
                  </li>
                  <li><em>{partition 3 start}</em>|
                    <ul>
                      <li>…</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="hdfs-folder-structure">HDFS folder structure</h2>

<p>Note: Access <code>http://localhost:9870/explorer.html</code> in your browse while the system is running to browse the current HDFS files and folders.</p>

<ul>
  <li>phrases
    <ul>
      <li>1_sink *- the searched phrases are dumped here, partitioned into 30min time blocks, *
        <ul>
          <li><em>{e.g 20200728_2230}</em></li>
          <li><em>{e.g 20200728_2300}</em></li>
        </ul>
      </li>
      <li>2_with_weight <em>- phrases with their initial weight applied, divided by time block</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>3_with_weight_merged <em>- consolidation of all the time blocks: phrases with their final weight</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>4_with_weight_ordered <em>- single file of phrases ordered by descending weight</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>5_tries <em>- storage of serialized tries</em>
        <ul>
          <li><code>{TARGET_ID}</code>
            <ul>
              <li>|<em>{partition 1 end}</em></li>
              <li><em>{partition 2 start}</em>|<em>{partition 2 end}</em></li>
              <li><em>{partition 3 start}</em>|</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="client-interaction">Client interaction</h2>

<p>You can interact with the system by accessing <code>http://localhost</code> in your browser. The search suggestions will be provided by the system as you write a query, and you can feed more queries/phrases into the system by submitting more searches.</p>

<center>
          <picture aria-label="Client interaction screenshot">
             <source type="image/webp" srcset="/files/autocomplete/pclient_web_705.webp" aria-label="Client interaction screenshot" />
             <source type="image/png" srcset="/files/autocomplete/pclient_web_705.png" aria-label="Client interaction screenshot" />
             <img src="/files/autocomplete/pclient_web_705.png" aria-label="Client interaction screenshot" />
           </picture>
         </center>

<h1 id="source-code">Source Code</h1>

<p>You can get the full source code at <a href="https://github.com/lopespm/autocomplete">https://github.com/lopespm/autocomplete</a>. I would be happy to know your thoughts about this implementation and design.</p>

<p><br /></p>
<hr />

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:10">
      <p>Docker compose was used instead of a container orchestrator tool like Kubernetes or Docker Swarm, since the main objective of this implementation was to build and share a system in simple manner.<a href="#fnref:10" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p>The average length of a search query was <a href="https://en.wikipedia.org/wiki/Web_search_query">2.4 terms</a>, and the average word length in English language is <a href="https://medium.com/@wolfgarbe/the-average-word-length-in-english-language-is-4-7-35750344870f">4.7 characters</a><a href="#fnref:12" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p><em>Phrase</em> and <em>Query</em> are used interchangeably in this article. Inside the system though, only the term <em>Phrase</em> is used.<a href="#fnref:11" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>In this implementation, only one instance of the broker is used, for clarity. However, for a large number of incoming requests it would be best to partition this topic along multiple instances (the messages would be partitioned according to the <em>phrase</em> key), in order to distribute the load.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><em>/phrases/1_sink/phrases/{30 minute window timestamp} folder</em>: For example, provided the messages A[time: 19h02m], B[time: 19h25m], C[time: 19h40m], the messages A and B would be placed into folder <em>/phrases/1_sink/phrases/20200807_1900</em>, and message C into folder <em>/phrases/1_sink/phrases/20200807_1930</em><a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>We could additionally pre-aggregate these messages into another topic (using Kafka Streams), before handing them to Hadoop<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>For clarity, the map reduce tasks are triggered manually in this implementation via <em>make do_mapreduce_tasks</em>, but in a production setting they could be triggered via cron job every 30 minutes for example.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>An additional map reduce could be added to aggregate the <em>/phrases/1_sink/phrases/</em> folders into larger timespan aggregations (e.g. 1-day, 5-week, 10-day, etc)<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Configurable in <em>assembler/hadoop/mapreduce-tasks/do_tasks.sh</em>, by the variable MAX_NUMBER_OF_INPUT_FOLDERS<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Partitions are defined in <em>assembler/trie-builder/triebuilder.py</em><a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>The number of replica nodes per partition is configured via the environment variable NUMBER_NODES_PER_PARTITION in docker-compose.yml<a href="#fnref:8" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>The distributed cache is disabled by default in this implementation, so that it is clearer for someone using this codebase for the first time to understand what is happening on each update/step. The distributed cache can be enabled via the environment variable DISTRIBUTED_CACHE_ENABLED in docker-compose.yml<a href="#fnref:9" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Pedro Lopes</span></span>

      








  


<time datetime="2020-08-03T00:00:00+00:00" pubdate data-updated="true">Aug 3<span>rd</span>, 2020</time>
      
      

<span class="categories">
  
    <a class='category' href='/tags/system-design/'>system design</a>
  
</span>


    </p>
    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2019/02/06/survival-ball-making-the-game.html" title="Previous Post: Survival Ball: Making the Game">&laquo; Survival Ball: Making the Game</a>
      
      
        <a class="basic-alignment right" href="/2020/12/25/hackernews-daily.html" title="Next Post: Hacker News Daily">Hacker News Daily &raquo;</a>
      
    </p>
  </footer>
</article>

  <div id="giscus-comments">
    <script async src="https://giscus.app/client.js"
        data-repo="lopespm/lopespm.github.com"
        data-repo-id="MDEwOlJlcG9zaXRvcnk3MDExOTAxNg=="
        data-category="Comments"
        data-category-id="DIC_kwDOBC3uaM4CYFCk"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="0"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
    </script>
  </section>

<!--

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

-->
</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/notes/2024/09/15/connection_with_reality.html">Connection with reality</a>
      </li>
    
      <li class="post">
        <a href="/notes/2024/09/12/interview_learnings_cia.html">Interview Learnings from former CIA Intelligence Officer</a>
      </li>
    
      <li class="post">
        <a href="/notes/2024/09/11/obsidian-backup.html">Obsidian Encrypted Backup - Google Drive, Dropbox, OneDrive</a>
      </li>
    
      <li class="post">
        <a href="/notes/2024/09/01/investment-tracker.html">Investment Tracker - Free Spreadsheet Template</a>
      </li>
    
      <li class="post">
        <a href="/notes/2024/08/26/uk-investment-guide.html">UK Investment Guide</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/lopespm">@lopespm</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'lopespm',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2024 - <a href="/about">Pedro Lopes</a> -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>. Theme based on <a href="https://github.com/lucaslew/whitespace"> Whitespace</a>. Subscribe via <a href="/atom.xml">RSS Feed</a>
</p>

</footer>
  





<script src="/javascripts/anchor.min.js"></script>

<script>
    anchors.options = {
      placement: 'left'
    };
    anchors.add();
    anchors.remove('.entry-title');
    anchors.remove('#byte-tank');
</script>



</body>
</html>
