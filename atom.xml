<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Byte Tank - Pedro Lopes Blog]]></title>
  <link href="http://lopespm.github.com/atom.xml" rel="self"/>
  <link href="http://lopespm.github.com/"/>
  <updated>2018-12-27T12:57:23+00:00</updated>
  <id>http://lopespm.github.com/</id>
  <author>
    <name><![CDATA[Pedro Lopes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Unity: Dynamic Multi Target Camera]]></title>
    <link href="http://lopespm.github.com/libraries/games/2018/12/27/camera-multi-target.html"/>
    <updated>2018-12-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/libraries/games/2018/12/27/camera-multi-target</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=In3eVapQ5mk"><img class="center" src="http://lopespm.github.com/files/camera_multi_target/demo.gif" /></a></p>

<p>Mostly invisible, yet essential, camera work is key to any game with dynamic cameras. This article dissects a concise Unity <a href="https://github.com/lopespm/unity-camera-multi-target">open source</a> library which dynamically keeps a set of objects (e.g. players and important objects) in view, a common problem for a wide range of games.</p>

<p>The library was developed for, and used by my first Steam game, <a href="https://survivalball.com/">Survival Ball</a>. The game has an heavy shared screen local co-op component, which requires the camera to dynamically keep many key elements in view.</p>

<!--more-->

<p>There are good camera resources for Unity, but I found them to either do too much or too little for this specific problem, so I thought this could be a good opportunity to learn a bit more about dynamic camera movement and to share that knowledge and <a href="#where-to-get-it">code</a> with the community.</p>

<h1 id="overview">Overview</h1>

<p>The library is fed with the desired camera rotation (pitch, yaw and roll), the target objects that will be tracked and the camera that will be transformed. </p>

<p>The library’s sole responsibility is to calculate a camera position in which all targets lie inside its view. To achieve this, all target objects are projected onto a slice (plane) of the <a href="https://docs.unity3d.com/Manual/UnderstandingFrustum.html">camera’s view frustrum</a>. The projections located inside the view frustrum will be visible and the others will not. The idea is to trace back a new camera position from the outermost target projections, since this way we are guaranteed to include all projections inside the view.</p>

<p><img class="center" src="http://lopespm.github.com/files/camera_multi_target/side_view_screen_basis_with_captions.png" /></p>

<p>In order to make the bulk of the operations easier to compute, the process starts by multiplying the camera’s <a href="https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Using_quaternion_as_rotations">inverse rotation</a> with each of the targets positions, which will place them as they would if the camera’s axis would be aligned with the world’s axis (identity rotation). Once the camera position is calculated in this transformed space, the camera rotation is multiplied by this position, resulting in the final desired camera position. The actual camera position is then progressively interpolated towards this desired position, to smooth out the camera movement.</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_1.png"><img src="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_1.png" /></a></center><figcaption class="media-caption"> Original targets positions </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_2_with_captions.png"><img src="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_2_with_captions.png" /></a></center><figcaption class="media-caption"> Multiply the camera&#8217;s inverse rotation with each of the targets positions </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_3.png"><img src="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_3.png" /></a></center><figcaption class="media-caption"> Calculate the camera position in this transformed space </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_4_with_captions.png"><img src="http://lopespm.github.com/files/camera_multi_target/transformed_space/transformed_space_4_with_captions.png" /></a></center><figcaption class="media-caption"> Multiply the camera&#8217;s rotation with the calculated position in the previous transformed space, which will reveal the final camera position in the original space</figcaption></figure>

<h1 id="implementation">Implementation</h1>

<p>Most of the operations are performed in the transformed space where the camera’s axis would be aligned with the world’s axis (identity rotation). After the targets are rotated into the camera’s identity rotation space by multiplying the camera’s inverse rotation with each of the targets positions, the first task is to calculate their projections.</p>

<p>Please note that in all the figures below (with the exception of the <a href="#horizontal-projections"><em>horizontal field of view angle</em></a> section), the camera is present for reference only, as its final desired position will only be uncovered in the final step.</p>

<h2 id="targets-projections">Targets projections</h2>

<p>For each target, four projections are cast to a plane parallel to the view plane, sliced from the camera’s view frustrum. The line described from the target object to its respective projection is parallel to the camera’s view frustrum edges. Relative to the camera, two of these projections run horizontally, and the other two vertically.</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_1.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_1.png" /></a></center><figcaption class="media-caption"> Perspective view of the targets projections </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_2.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_2.png" /></a></center><figcaption class="media-caption"> Side view of the targets projections </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_3.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_intro/projections_3.png" /></a></center><figcaption class="media-caption"> Top view of the targets projections </figcaption></figure>

<p>If any of the target’s projections are outside the camera’s view frustrum (or its sliced plane), then the target object will not be visible. If they are inside, the target object will be visible. This means that the four outermost projections from all targets will define the limit of where the view frustrum must be in order to have all objects in view or partially in view. Adding some padding to these outermost projections (i.e. moving these projections away from the center of the view frustrum plane slice), will result in additional padding between the target object and the camera’s view borders.</p>

<h3 id="vertical-projections">Vertical projections</h3>

<p>For all vertical projections positions, we are interested in finding their Y component. In the figure below, notice the right triangle with one vertex on the target object and another one on the projection. If we discover the length of the side running parallel the projection plane, that value can be added to the Y component of the target’s position, resulting in the Y component for the upper projection.</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_calc/projections_calc_with_captions_vertical.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_calc/projections_calc_with_captions_vertical.png" /></a></center><figcaption class="media-caption"> Side view of the elements needed to calculate the vertical projections </figcaption></figure>

<p><script type="math/tex">\theta</script> is equal to half the camera’s vertical field of view angle <script type="math/tex">\theta'</script> (<script type="math/tex">\theta = \frac{\theta'}{2}</script>). The vertical field of view angle <script type="math/tex">\theta'</script> is provided by the camera’s <a href="https://docs.unity3d.com/ScriptReference/Camera-fieldOfView.html"><code>fieldOfView</code></a> in degrees, which needs to be converted to radians for our purposes (<script type="math/tex">\theta' = \theta'_{degrees} \times \frac{\pi}{180º}</script>).</p>

<p>The triangle’s adjacent edge length (relative to <script type="math/tex">\theta</script>) is known, thus we can find the length of the opposite side of the triangle by using <a href="https://www.khanacademy.org/math/geometry/hs-geo-trig/hs-geo-trig-ratios-intro/a/finding-trig-ratios-in-right-triangles">trigonometric ratios</a>.</p>

<script type="math/tex; mode=display">opposite = tan(\theta) \times adjacent</script>

<p>With this, the upper projection’s Y/Z components can be fully calculated. The bottom projection has the same Z component as the upper one, and its Y component is equal to the target’s Y component minus the calculated opposite triangle edge length.</p>

<h3 id="horizontal-projections">Horizontal projections</h3>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_calc/projections_calc_with_captions_horizontal.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_calc/projections_calc_with_captions_horizontal.png" /></a></center><figcaption class="media-caption"> Top view of the elements needed to calculate the horizontal projections </figcaption></figure>

<p>The horizontal projections follow a set of similar set of calculations, difference being that we are now interested in finding the X component (instead of Y), and the horizontal field of view angle is used instead of the vertical one. The horizontal field of view angle <script type="math/tex">\gamma'</script> and its half value <script type="math/tex">\gamma</script> (<script type="math/tex">\gamma = \gamma' \times 2</script>) need some further steps to be computed, which will be detailed in the following section.</p>

<h4 id="horizontal-field-of-view-angle">Horizontal field of view angle</h4>

<p>Consider the following figure, in which <script type="math/tex">\gamma</script> represents half the horizontal field of view angle, <script type="math/tex">\theta</script> represents half the vertical field of view angle, <script type="math/tex">w</script> the viewport width and <script type="math/tex">h</script> the viewport height:</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/horizontal_fov.png"><img src="http://lopespm.github.com/files/camera_multi_target/horizontal_fov.png" /></a></center><figcaption class="media-caption"> Elements for the calculation of the horizontal field of view angle </figcaption></figure>

<p>Using trigonometric ratios, these two equations can be devised:</p>

<script type="math/tex; mode=display">
\begin{cases}
tan(\gamma) = \frac{w / 2}{adj} \\ 
adj = \frac{h / 2}{tan(\theta)}
\end{cases} 
</script>

<p>Replacing <script type="math/tex">adj</script> in the first equation with the definition of the second one:</p>

<script type="math/tex; mode=display">
tan(\gamma) = \frac{w / 2}{\frac{h / 2}{tan(\theta)}} 
\Leftrightarrow \\
tan(\gamma) = \frac{w / 2}{h / 2} \times tan(\theta)
\Leftrightarrow \\
tan(\gamma) = \frac{w}{h} \times tan(\theta)
\Leftrightarrow \\
\gamma = arctan(\frac{w}{h} \times tan(\theta))
</script>

<p>Unity’s camera has an <a href="https://docs.unity3d.com/ScriptReference/Camera-aspect.html"><code>aspect</code></a> attribute (view canvas width divided by height, i.e. <script type="math/tex">aspect\ ratio = \frac{w}{h}</script>), with which we can finalize our equation and discover the horizontal field of view angle half value.</p>

<script type="math/tex; mode=display">
\gamma = arctan(aspect\ ratio \times tan(\theta))
</script>

<h3 id="outermost-projections">Outermost projections</h3>

<p>Having all target projections calculated, the four outermost ones are picked:</p>

<ul>
  <li><script type="math/tex">p_{hMax}</script> is the projection with the highest X component</li>
  <li><script type="math/tex">p_{hMin}</script> is the projection with the lowest X component</li>
  <li><script type="math/tex">p_{vMax}</script> is the projection with the highest Y component</li>
  <li><script type="math/tex">p_{vMin}</script> is the projection with the lowest Y component</li>
</ul>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_outermost/projections_outermost.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_outermost/projections_outermost.png" /></a></center><figcaption class="media-caption"> All outermost projections </figcaption></figure>

<h2 id="calculating-the-camera-position">Calculating the camera position</h2>

<h3 id="in-the-transformed-space">In the transformed space</h3>

<p>The X and Y components of the desired camera position in the transformed space are the midpoints of their respective outermost projections, this is, the midpoint between <script type="math/tex">p_{hMax}</script> and <script type="math/tex">p_{hMin}</script> is the camera’s X position, and the midpoint between <script type="math/tex">p_{vMax}</script> and <script type="math/tex">p_{vMin}</script> is the camera’s Y position.</p>

<p>The Z component of the camera position in the transformed space is calculated by backtracking a view frustrum from the the outermost projections to the camera Z component candidate. The furthest Z component from the projection plane will be the chosen, in order for the final camera position to contain all targets within its view.</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_distance/projections_distance_with_captions_vertical.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_distance/projections_distance_with_captions_vertical.png" /></a></center><figcaption class="media-caption"> Elements for adjacent<sub>v</sub> calculation </figcaption></figure>

<figure class="image"><center><a href="http://lopespm.github.com/files/camera_multi_target/projections_distance/projections_distance_with_captions_horizontal.png"><img src="http://lopespm.github.com/files/camera_multi_target/projections_distance/projections_distance_with_captions_horizontal.png" /></a></center><figcaption class="media-caption"> Elements for adjacent<sub>h</sub> calculation </figcaption></figure>

<p>Once again, trigonometric ratios will be used to calculate these Z position candidates.</p>

<script type="math/tex; mode=display">
adjacent_{v} = \frac{opposite_{v}}{tan(\theta)}
\\
adjacent_{h} = \frac{opposite_{h}}{tan(\gamma)}
</script>

<p>The highest value between <script type="math/tex">adjacent_{v}</script> and <script type="math/tex">adjacent_{h}</script> will be picked for the camera’s Z position component in the transformed space.</p>

<h3 id="final-camera-position-in-the-original-space">Final camera position in the original space</h3>

<p>With the camera position calculated in the transformed space, we can now multiply the desired camera rotation with this position, which will provide us with the final desired camera position. The actual camera position is then progressively interpolated towards this desired position, to smooth out the camera movement.</p>

<h1 id="where-to-get-it">Where to get it</h1>

<p>The library is available on GitHub and will be available in the Unity Asset Store once it is approved<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>. An example scene of the library’s usage is included. Feedback is most welcome and I hope this can be useful!</p>

<ul>
  <li><a href="https://github.com/lopespm/unity-camera-multi-target">GitHub Repository</a></li>
</ul>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><em>I have been informed that the Unity Asset Store team is experiencing longer than normal review times. From the information I gathered online, it might take a couple of weeks until it is available. I will post an update on my twitter <a href="https://twitter.com/lopes_pm">@lopes_pm</a> once it is published.</em><a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ArXiv Papers: React Native Android App]]></title>
    <link href="http://lopespm.github.com/apps/2018/03/12/arxiv-papers.html"/>
    <updated>2018-03-12T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/apps/2018/03/12/arxiv-papers</id>
    <content type="html"><![CDATA[<p><img src="http://lopespm.github.com/files/arxiv-papers/arxiv_layout_implementation.png" /> </p>

<p>ArXiv Papers is an <a href="https://github.com/lopespm/arxiv-papers-mobile">open source</a> mobile application to search, download and save arXiv papers. It was developed using a react native / redux framework and is currently available for smartphone and tablet devices. You can get it on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.arxiv">Google Play</a>.</p>

<!--more-->

<h2 id="motivation">Motivation</h2>

<p>Often I would find myself downloading the same arXiv paper on my phone because it would get buried in the “downloads” folder under the default arXiv id file name, so instead of sifting through a myriad of already downloaded PDFs I would just revisit the paper’s web page and redownload it.</p>

<p>Organizing each paper neatly into its proper folder with a more discernable filename is quite laborious on a mobile device and the apps I tried out either did too much or too little. I just wanted to quickly search an arXiv article and quickly refer to the ones I downloaded.</p>

<p>This seemed not only a good opportunity to leverage on <a href="https://arxiv.org/help/api/index">arXiv’s public API</a> and develop this idea into an application, but also to expand from Android native development and dive into cross platform app development, specifically Android/iOS.</p>

<h1 id="bootstraping">Bootstraping</h1>

<p>Cross platform mobile app development frameworks come far and wide, so the plan was to go through them, choose one framework, learn its foundations, and attempt to solidify my understanding of its various concepts, advantages, pains and quirks by using it in this new application.</p>

<p>I went shopping for a suitable framework and eventually settled with react native/redux. It seemed like a good fit for the project and a valuable add to my programming tool belt. React and redux had been around for a while and were quite mature and capable at that point. The react native library did not reach a major version yet, but appeared to be quite solid and had a rich ecosystem, vibrant community and various <a href="https://code.facebook.com/posts/1189117404435352/react-native-for-android-how-we-built-the-first-cross-platform-react-native-app/">successful</a> <a href="https://developers.soundcloud.com/blog/react-native-at-soundcloud">use</a> <a href="https://www.youtube.com/watch?v=8qCociUB6aQ">cases</a> in demanding production apps.</p>

<p>I first built my foundations through courses (<a href="https://egghead.io/instructors/dan-abramov">Dan Abramov’s</a> react and redux courses are wonderful), examples, lectures, tutorials, documentation and articles. I was pleasantly impressed with the <a href="https://github.com/react-community/create-react-native-app">CRNA</a> project’s workflow and how it enables you to develop an entire application without touching a single native implementation piece. However, Android and iOS are two distinct platforms, with different design and UX paradigms, meaning that even unejected applications should be specifically tailored for each platform. With this in mind, and knowing that I would need native integrations to download papers for example, I decided to narrow the scope to Android only, but a potential future iOS version should not be a huge pain.</p>

<h1 id="design">Design</h1>

<figure class="image"><center><a href="http://lopespm.github.com/files/arxiv-papers/arxiv_layout_mockup.png"><img src="http://lopespm.github.com/files/arxiv-papers/arxiv_layout_mockup.png" /></a></center><figcaption class="media-caption"> Final Android smartphone app design, made in Sketch </figcaption></figure>

<p>After a few iterations, the above design seemed like a good compromise between functionality, breadth and ease of use. Most importantly, it catered to the app’s motif: search, download and save arXiv articles. </p>

<p>Many, if not all of the apps I explored, allowed the user to browse through arXiv’s categories. For this application however, it did not seem to be an essential feature. Browsing categories serves discoverability, but I am assuming that whoever uses this app would at least have a rough idea of what she is looking for. In that case, keyword search can fill that use case. Looking for DeepMind’s <a href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning</a>? Write “deep atari” and the paper you are looking for appears on the first page.</p>

<p>Bookmark-wise, the assumption is that if you download a paper, then most probably it is something your are interested in, thus the higher the probability for its recall. How about merging the two concepts? You are on the article’s details screen, you download it, read it if you wish, and when you arrive back at the home screen, it will be available for immediate recall.</p>

<h1 id="implementation">Implementation</h1>

<p>React by itself is a library which could be seen as the view in model-view-controller, whereas react native builds upon it by providing native component rendering, and redux is used for state management. Moreover, I chose to use <a href="https://redux-observable.js.org/">Redux Observable</a> to handle redux action side effects, <a href="https://nativebase.io/">Native Base</a> for most of the UI components, Flow for type safety, <a href="https://github.com/wix/detox">Detox</a> for end to end tests and Jest for unit and integration tests.</p>

<h2 id="action-side-effects">Action Side Effects</h2>

<p>Redux Observable does not seem to be a popular choice for action side effects, when compared with redux sagas for example, but I think it offers one of the best solutions in this domain. At its core it uses rxJS, a library for reactive programming, which is a natural fit for the asynchronous nature of redux action side effects. This approach is inherently declarative, provides powerful composition (complex async tasks godsend) and makes it easy to cancel async tasks, a common edge case which is often overlooked and can make the difference between an OK application and a very solid application. </p>

<h2 id="testing">Testing</h2>

<p>Application logic was contained inside the action’s handlers, selectors and reducers, so I chose not to test the react components because the unit and integration tests could extract an higher value when applied to the before mentioned triad. Adding to this, ESLint was used for code quality control and catching basic errors, and much of the application was typed using Flow. Redux Observable epics were not type checked, since at the moment there are <a href="https://github.com/redux-observable/redux-observable/issues/258">no meaningful type definitions available</a>.</p>

<p>Integration tests were done for the different application modules (articles, papers, etc), using actions as inputs and selectors as outputs; action side effects were tested separately in their respective unit tests. More unit tests could be made to each of the redux units and further satisfy the traditional test pyramid, but the before mentioned static analysis served as a good foundation and most of the module’s inner code paths were covered by the aforementioned tests, so I opted for a more a pragmatic approach, one which could yield a higher value with minimum developer effort.</p>

<p>To encompass the whole application, end to end tests were built with the aid of Detox. Detox’s android support is still nearing completion, but appears to be a very promising solution. One of its most distinctive features, when compared with popular black box testing frameworks like Appium, is its direct integration with the native layers via EarlGrey for iOS and Espresso for Android, translating to more stable, faster and less flaky tests. Using it was straightforward, the API is well designed and it did not stand in the way while writing the tests. Overall, a good experience for this small application.</p>

<h2 id="folder-structure">Folder Structure</h2>

<p>Regarding folder structure, I opted for a feature based tree which branched into framework concepts (reducer, action, selector, react component, redux observable epic). As it typically happens in other domains, using the framework’s concepts as the root of your folder structure works great for small applications, but as complexity grows, root division by features makes the structure much easier to extend and reason about.</p>

<div><div class="CodeRay">
  <div class="code"><pre>├── __e2e__
├── modules
│   ├── articles
│   ├── connection
│   ├── donations
│   ├── navigation
│   ├── papers
│   │   ├── __tests__
│   │   ├── epic
│   │   ├── paper.js
│   │   ├── actions.js
│   │   ├── reducer.js
│   │   └── selector.js
│   ├── permission
│   ├── shared
│   └── snackbar
├── configureStore
├── theme
├── App.js
├── epic.js
├── reducer.js
└── selector.js
</pre></div>
</div>
</div>

<p><br /></p>

<h1 id="get-it-hack-it">Get it, Hack it</h1>

<p>Feel free to hack away <a href="https://github.com/lopespm/arxiv-papers-mobile">the code</a> or download the application on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.arxiv">Google Play</a>, let me know your thoughts about it!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[React Native Offscreen Toolbar]]></title>
    <link href="http://lopespm.github.com/libraries/2018/01/25/react-native-offscreen-toolbar.html"/>
    <updated>2018-01-25T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/libraries/2018/01/25/react-native-offscreen-toolbar</id>
    <content type="html"><![CDATA[<p>In material design, there is a <a href="https://material.io/guidelines/patterns/scrolling-techniques.html#scrolling-techniques-behavior">common scrolling technique</a> in which the toolbar is smoothly tucked away while scrolling down and is made visible again when scrolling up. This behaviour is fairly straightforward to implement when developing a native android app, but for a react native app, the best solution I found was <a href="https://medium.com/appandflow/react-native-collapsible-navbar-e51a049b560a">Janic Duplessis’</a>.</p>

<p>The <a href="https://www.npmjs.com/package/react-native-offscreen-toolbar">library</a> I am releasing today is an encapsulation of this behaviour and its implementation is heavily based on Janic Duplessis’ approach.</p>

<center>
  <div style="overflow: auto; margin-bottom: 16px">
    <div style="float: left;height: 10px; width:8%;"></div>
    <img style="float: left;height: auto; width:40%;" src="http://lopespm.github.com/files/rn-offscreen-toolbar/simplelist_demo.gif" />
    <div style="float: left;height: 10px; width:4%;"></div>
    <img style="float: left;height: auto; width:40%;" src="http://lopespm.github.com/files/rn-offscreen-toolbar/search_demo.gif" />
  </div>
  <figcaption class="media-caption">Library usage in both the <a href="https://github.com/lopespm/react-native-offscreen-toolbar/tree/master/example">example</a> bundled with the library and in the search screen of a to be released application</figcaption>
</center>

<!--more-->

<h2 id="how-to-use-it">How to Use it</h2>

<p>Installation:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ $ npm install react-native-offscreen-toolbar --save
</pre></div>
</div>
</div>

<p><br /></p>

<p>Usage (full example <a href="https://github.com/lopespm/react-native-offscreen-toolbar/tree/master/example">here</a>):</p>

<div><div class="CodeRay">
  <div class="code"><pre>import OffscreenToolbar from 'react-native-offscreen-toolbar';
 
export default class YourComponent extends React.Component {
    render() {
        const toolbar = () =&gt; (&lt;ToolbarAndroid title={'Title'} /&gt;);
        const listItem = ({item}) =&gt; &lt;Text&gt;{item.key}&lt;/Text&gt;;
        const scrollable = () =&gt; (&lt;FlatList data={DUMMY_DATA} renderItem={listItem}/&gt;);
        return (
            &lt;View style={styles.container}&gt;
                &lt;OffscreenToolbar
                    toolbar={toolbar}
                    scrollable={scrollable} /&gt;
            &lt;/View&gt;
        );
    }
}
</pre></div>
</div>
</div>

<p><br /></p>

<p>You can pass any component you desire as a <code>toolbar</code>, but typically this will be a toolbar/navbar backed by react native or a UI library of your choice, like <a href="https://nativebase.io/">Native Base</a>.</p>

<p>The scrollable however, will need to be scrollable component such as a <code>ScrollView</code>, <code>ListView</code>, <code>FlatList</code>, etc. The <code>OffscreenToolbar</code> will then create hooks around this component in order to gauge the user’s scrolling behaviour and change toolbar’s animation accordingly.</p>

<p>You can also provide the following properties to the component:</p>

<ul>
  <li><code>scrollableOverlay</code>: for a search screen as the one presented above, this component can be used to present an overlay between the scrollable and the toolbar</li>
  <li><code>toolbarHeight</code>: adjust this property according to your toolbar height. This value is used for the toolbar’s animation</li>
  <li><code>scrollablePaddingTop</code>: since the scrollable is laying behind the toolbar, this property comes in handy to place the scrollable content below the toolbar</li>
</ul>

<h2 id="where-to-get-the-library">Where to get the library</h2>

<ul>
  <li><a href="https://github.com/lopespm/react-native-offscreen-toolbar">Source code on GitHub</a></li>
  <li><a href="https://www.npmjs.com/package/react-native-offscreen-toolbar">NPM distribution</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Reinforcement Learning: Playing a Racing Game]]></title>
    <link href="http://lopespm.github.com/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html"/>
    <updated>2016-10-06T00:00:00+01:00</updated>
    <id>http://lopespm.github.com/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game</id>
    <content type="html"><![CDATA[<center>
  <div style=" display: inline-block; ">
    <iframe width="350" height="197" src="https://www.youtube.com/embed/spzYVhOgKBA" frameborder="0" allowfullscreen=""></iframe>
    <p class="media-caption">Agent playing Out Run, session 201609171218_175eps<br />No time limit, no traffic, 2X time lapse</p>
  </div>
</center>
<p>Above is the built <a href="https://deepmind.com/research/dqn/">deep Q-network (DQN)</a> agent playing <a href="https://en.wikipedia.org/wiki/Out_Run">Out Run</a>, trained for a total of 1.8 million frames on a Amazon Web Services g2.2xlarge (GPU enabled) instance. The agent was built using python and tensorflow. The Out Run game emulator is a modified version of <a href="https://github.com/lopespm/cannonball">Cannonball</a>. All source code for this project is <a href="http://lopespm.github.com/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html#source-code">available on GitHub</a>.</p>

<!--more-->

<p>The agent learnt how to play by being rewarded for high speeds and penalized for crashing or going off road. It fetched the game’s screens, car speed, number of off-road wheels and collision state from the emulator and issued actions to it such as pressing the left, right, accelerate or brake virtual button. </p>

<p>Agent trainer implements the deep Q-learning algorithm used by <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Google’s DeepMind Team to play various Atari 2600 games</a>. It uses a reward function and hyperparameters that fit best for Out Run, but could potentially be used to <a href="#plugging-other-problems-and-games">play other games or solve other problems</a>.</p>

<p>There is a wealth of <a href="#further-references">good information</a> about this reinforcement learning algorithm, but I found some topics difficult to grasp or contextualize solely from the information available online. I will attempt to add my humble contribution by tackling these and also provide details about the project’s implementation, results and how it can be used/modified/deployed.</p>

<p>Let’s start by one of its main gears: Q-learning</p>

<p />

<h1 id="concepts">Concepts</h1>

<h2 id="q-learning">Q-learning</h2>

<p>At the heart of deep Q-learning lies Q-learning, a popular and effective <a href="https://www.youtube.com/watch?time_continue=258&amp;v=bFPoHrAoPoQ">model-free</a> algorithm for learning from delayed reinforcement.  </p>

<p>Jacob Schrum has made available a terse and accessible <a href="https://www.youtube.com/playlist?list=PL4uSLeZ-ET3xLlkPVEGw9Bn4Z8Mbp-SQc">explanation</a> which takes around 45 minutes to watch and serves as a great starting point for the paragraphs below. 
Let’s take the canonical reinforcement learning example presented by Jacob (grid world):</p>

<p><img class="center" src="http://lopespm.github.com/files/dqn_outrun/gridworld.png" /></p>

<p>To implement this algorithm, we need to build the Q-function (<a href="https://www.youtube.com/watch?v=XLkW_WGJoyQ">one of the forms</a> of the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bell-Equation</a>) by using the Q-value iteration update:</p>

<p><img class="center" src="http://lopespm.github.com/files/dqn_outrun/q_value_iteration_update.svg" /></p>

<p>In the above grid, there are 9 actionable states, 2 terminal states and 4 possible actions (left, right, up, down), resulting in 36 (9 actionable states x 4 possible actions) Q-values.</p>

<p>This project aims to train an agent to play Out Run via its game screens, so for the sake of argument, let´s consider that each game screen is transformed into a 80x80 greyscale image (each pixel value ranging from 0 to 255), and that each transformed image represents a state. 6400 pixels (80x80) and 256 possible values per pixel translate to 256<sup>6400</sup> possible states. This value alone is a good indicator of how inflated the number of possible Q-values will be.</p>

<p>Multiplying <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py">9 possible actions</a> by 256<sup>6400</sup> possible states results in 256<sup>6400</sup> x 9 possible Q-values. If we use multiple and/or colored images for state representation, then this value will be even higher. Quite unwieldy if we want to store these values in a table or similar structure.</p>

<h2 id="enter-deep-neural-networks">Enter deep neural networks</h2>

<p>Deep neural networks work quite well for inferring the mapping implied by data, giving them the ability to predict an approximated output from an input that they never saw before. No longer do we need to store all state/action pair’s Q-values, we can now model these mappings in a more general, less redundant way. These networks also automatically learn a set of internal features which are useful in complex non-linear mapping domains, such as image processing, releasing us from laborious feature handcrafting tasks.</p>

<p>This is perfect. We can now use a deep neural network to approximate the Q-function: the network would accept a state/action combination as input and would output the corresponding Q-value. Training-wise, we would need the network’s Q-value output for a given state/action combo (obtained through a forward pass) and the target Q-value, which is calculated through the expression: <script type="math/tex">r_{t+1} + \gamma \underset{a}\max Q(s_{t+1}, a)</script>. With these two values, we can perform a gradient step on the squared difference between the target Q-value and the network’s output.</p>

<p>This is perfect, but there is still room for improvement. Imagine we have 5 possible actions for any given state: when calculating the target Q-value, to get the optimal future value estimate (consequent state’s maximum Q-value) we need to ask (forward pass) our neural network for a Q-value 5 times per learning step.</p>

<p>Another approach (used in <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DeepMind’s</a> <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">network</a>) would be to feed in the game’s screens and have the network output the Q-value for each possible action. This way, a single forward pass would output all the Q-values for a given state, translating into one forward pass per optimal future value estimate.</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/q_network_formulations.png"><img src="http://lopespm.github.com/files/dqn_outrun/q_network_formulations.png" /></a></center><figcaption class="media-caption"> Image courtesy of Tambet Matiisen’s <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning">Demystifying Deep Reinforcement Learning</a> - Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind papers. </figcaption></figure>

<p>Q-learning and deep neural networks are the center pieces of a deep Q-network reinforcement learning agent and I think that by understanding them and how they fit together, it can be easier to picture how the algorithm works as a whole.</p>

<h1 id="implementation">Implementation</h1>

<center><a href="http://lopespm.github.com/files/dqn_outrun/overall_view_play.png"><img align="center" src="http://lopespm.github.com/files/dqn_outrun/overall_view_play.png" alt="" /></a></center>

<p />

<p>Above is an overall representation of how the different components relate during a play evaluation, centered around the <code>deep Q-network for playing</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, the main decision component.</p>

<p>Each game screen is resized to a desaturated 80x80 pixels image (opposed to 84x84 on DeepMind’s papers), and if you might be wondering why each state is a sequence of four game screens instead of one, that is because the agent’s history is used for better motion perception. Achieving this requires a sequence of preprocessed images to be stacked in channels (like you would stack RGB channels on a colored image) and fed to the network. Note that RGB channels and agent history could be used simultaneously for state representation. For example, with three channels per (RGB) image and an agent history length of four, the network would be fed twelve channels per input state.</p>

<p>The <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/q_network.py#L121">network’s architecture</a> is essentially the same used by DeepMind, except for the first convolutional neural network’s input (80x80x4 instead of 84x84x4, to account for the different input sizes) and the linear layer’s output (9 instead of 18, to account for the different number of actions available)</p>

<p>The algorithm used to train this network is well described <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">here (page 7)</a> and <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">here</a>, but I would like to present it graphically, to hopefully provide some further intuition.</p>

<p>Below is <a href="https://github.com/lopespm/agent-trainer">agent trainer</a>´s implementation of the aforementioned algorithm. It adds some new concepts which were not approached by this article:</p>

<ul>
  <li>Experience replay mechanism sported by replay memories: randomly samples previous transitions, thereby smoothing the training distribution over many past behaviors</li>
  <li>Separate training network, cloned at fixed intervals to the target playing network, making the algorithm more stable when compared with standard online Q-learning</li>
  <li>ε-greedy policy to balance exploitation/exploration</li>
</ul>

<center><a href="http://lopespm.github.com/files/dqn_outrun/overall_view_train_merged.png"><img align="center" src="http://lopespm.github.com/files/dqn_outrun/overall_view_train_merged.png" alt="" /></a></center>

<p />

<h2 id="reward-function">Reward function</h2>

<p>The reward function’s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. April Yu et al. have an interesting paper on <a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">simulated autonomous vehicle control</a> which details a DQN agent used to drive a game that strongly resembles Out Run (<a href="http://codeincomplete.com/games/racer/v4-final/">JavaScript Racer</a>). Based on their reward function experiments, I’ve built a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L60">function</a> which rewards logarithmically based on speed and penalizes when the car is off-road, crashed or stopped. </p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/reward_function_plot.png"><img src="http://lopespm.github.com/files/dqn_outrun/reward_function_plot.png" /></a></center><figcaption class="media-caption"> Reward values for when the car is not crashed or off-road </figcaption></figure>

<h1 id="deployment">Deployment</h1>

<p>Run the trainer and emulator on your local machine by following the guide available on <a href="https://github.com/lopespm/agent-trainer/blob/master/README.md">agent-trainer’s readme</a>.</p>

<p>It is also possible to deploy the agent to an AWS EC2 instance or generic Linux remote machine by using a set of bash scripts offered by <a href="https://github.com/lopespm/agent-trainer-deployer">agent-trainer-deployer</a>.</p>

<h3 id="aws-ec2">AWS EC2</h3>

<p>Amazon allows you to bid on spare EC2 computing capacity via <a href="https://aws.amazon.com/ec2/spot/">spot instances</a>. These can cost a fraction of on-demand ones, and for this reason were chosen as the prime method for training in this project, leading to the need for mid-training instance termination resilience.</p>

<p>To accommodate this scenario, the deployment scripts and agent-trainer are designed to support train session resumes. To persist results and decrease boot up time between sessions, a long-lived EBS volume is attached to the live instance. The volume contains the training results, agent-trainer´s source code, cannonball’s source code, dockerfiles and their respective docker images. </p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/deployed-diagram.png"><img src="http://lopespm.github.com/files/dqn_outrun/deployed-diagram.png" /></a></center><figcaption class="media-caption"> Relationship between components when deploying agent-trainer to an AWS EC2 instance </figcaption></figure>

<h1 id="results">Results</h1>

<p>The hyperparameters used on all sessions mimic the ones used on DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a>, except for the number of frames skipped between actions, which are spaced apart by 450ms (equivalent to 13 frames) on agent-trainer.</p>

<p>The Out Run game, as you would play it in an arcade, clutters the road with various cars in order to make the game more challenging. In-game traffic was disabled for both training and evaluation plays, rendering a more achievable starting point for these experiments. Training with random traffic could be an interesting posterior experiment.</p>

<p>Some experiments were made by increasing the discount factor up its final value during training, as proposed on <a href="https://arxiv.org/abs/1512.02011">“How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies”</a>, but did not achieve better stability or rewards when compared to a fixed 0.99 discount factor. The aforementioned paper also proposes decaying the learning rate during training, which increased stability and performance significantly. Decaying the learning rate without minimum value clipping yielded the best results.</p>

<p>Another improvement was to train the game without a time limit, meaning that the training episode would only finish when the car reached the last stage’s end. This allowed for a broader replay memory training set, since the agent traversed a wide range of different tracks and settings.</p>

<p>Play evaluation was the same between all experiments, this is, the agent was evaluated by playing on the default 80 second, easy mode.</p>

<p>Here is a summary of the most relevant training sessions (you can find their models, metrics and visualizations on <a href="https://github.com/lopespm/agent-trainer-results">agent-trainer-results</a>):</p>

<table>
  <thead>
    <tr>
      <th>Session</th>
      <th>M</th>
      <th>Training<br />game mode</th>
      <th>Learning rate decay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>201609040550_5010eps</td>
      <td>a)</td>
      <td>timed; easy</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_2700eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_7300eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609160922_54eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609171218_175eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>unclipped learning rate decay</td>
    </tr>
  </tbody>
</table>

<p class="media-caption"><a href="https://github.com/lopespm/agent-trainer-results">Training sessions</a> summary: session names are formed by &#60;session ID&#62;_&#60;number of episodes trained&#62;<br />(M)achine used:  a) AMD Athlon(tm) II X2 250 Processor @ 3GHz; 2GB RAM DDR3-1333 SDRAM; SSD 500 GB: Samsung 850 EVO (CPU only training); b) AWS EC2 g2.2xlarge (GPU enabled instance), 200 GB General Purpose SSD (GP2)</p>

<p />

<center>
  <div style=" display: inline-block; ">
    <iframe width="350" height="197" src="https://www.youtube.com/embed/1Gpl9Xc-E8M" frameborder="0" allowfullscreen=""></iframe>
    <p class="media-caption">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609111241_2700eps</p>
  </div>
    <div style=" display: inline-block;">
    <iframe width="350" height="197" src="https://www.youtube.com/embed/6F3eCoCw57E" frameborder="0" allowfullscreen=""></iframe>
    <p class="media-caption">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609171218_175eps</p>
  </div>
</center>

<p>Notice on the videos above how the timed mode trained session <code>201609111241_2700eps</code> reaches the first checkpoint about five seconds earlier than the unlimited time mode trained session <code>201609171218_175eps</code>, but proceeds to drive off-road two turns after. Its stability gets increasingly compromised as more episodes are trained, which can be observed by the rampant loss increase before 7300 episodes are reached (<code>201609111241_7300eps</code>):</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609111241_7300eps/metrics_in_train.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609111241_7300eps/metrics_in_train.png" /></a></center><figcaption class="media-caption"> Training metrics for session 201609111241_7300eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609111241_7300eps/metrics_trained_play.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609111241_7300eps/metrics_trained_play.png" /></a></center><figcaption class="media-caption"> Play evaluation metrics for session 201609111241_7300eps: using ε=0.0; evaluation made at the end of every 20 training episodes </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/metrics_in_train.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/metrics_in_train.png" /></a></center><figcaption class="media-caption"> Training metrics for session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/metrics_trained_play.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/metrics_trained_play.png" /></a></center><figcaption class="media-caption"> Play evaluation metrics 201609171218_175eps: using ε=0.0; evaluation made at the end of every training episode </figcaption></figure>

<p>Both <code>201609111241_2700eps</code> and <code>201609111241_7300eps</code> timed trained sessions mostly drive off-road and stall after the first stage, whereas the unlimited time mode trained session <code>201609171218_175eps</code> can race through all the stages crashing <em>only</em> three times (as shown on the article’s first video) and is able to match the performance of a timed trained session when evaluated on the default easy timed mode. </p>

<p>Below is the loss plot for <code>201609160922_54eps</code> and <code>201609171218_175eps</code>, both trained using the game’s unlimited time mode, difference being that <code>201609160922_54eps</code> keeps a fixed learning rate and <code>201609171218_175eps</code> decays it every 50100 steps:</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/loss_201609171218_201609160922.png"><img src="http://lopespm.github.com/files/dqn_outrun/loss_201609171218_201609160922.png" /></a></center><figcaption class="media-caption"> Loss comparison between sessions <font color="#9c27b0">■</font> 201609160922_54eps and <font color="#009688">■</font> 201609171218_175eps, as viewed on <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a> </figcaption></figure>

<p>Other representative visualizations:</p>

<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/t-SNE_timed_easy_mode.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/t-SNE_timed_easy_mode.png" /></a></center><figcaption class="media-caption"> t-SNE visualization, generated by letting the agent play one game on timed easy mode. Agent is using the network trained on session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/t-SNE_no_time_mode.png"><img src="http://lopespm.github.com/files/dqn_outrun/201609171218_175eps/t-SNE_no_time_mode.png" /></a></center><figcaption class="media-caption"> t-SNE visualization, generated by letting the agent play one game on unlimited time mode. Agent is using the network trained on session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.com/files/dqn_outrun/conv_net_filters.png"><img src="http://lopespm.github.com/files/dqn_outrun/conv_net_filters.png" /></a></center><figcaption class="media-caption"> Visualization of the first convolutional network layer’s filters. These can be viewed via <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a></figcaption></figure>

<p><br /></p>

<h1 id="final-notes">Final Notes</h1>

<h2 id="plugging-other-problems-and-games">Plugging other problems and games</h2>

<p>Agent-trainer was not built from the get-go to train games or problems other than Out Run, but I think it would be interesting to perform a thought exercise on what would be necessary to do so.</p>

<p>There are three main areas in which <a href="https://github.com/lopespm/agent-trainer">agent-trainer</a> has domain knowledge about Out Run:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer/tree/master/agent/game"><code>game</code></a> package, which contains
    <ul>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py"><code>Action</code></a> enumeration: describes all the possible actions in the game.</li>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/cannonball_wrapper.py"><code>cannonball_wrapper</code></a> module: only this module has access to the cannonball emulator. It translates the aforementioned actions into game actions and is accessed by methods such as <code>start_game()</code>, <code>reset()</code> and <code>speed()</code>.</li>
    </ul>
  </li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> class: contains the reward function. Instead of using a generic reward function like DeepMind, it was chosen to have a tailor-made reward function for Out Run, which takes into account the car’s speed and its off-road and crash status.</li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/visualization/metrics.py"><code>metrics</code></a> module: aware of the <code>speed</code> metric, which is Out Run specific, and <code>score</code>, which is game specific domain knowledge.</li>
</ul>

<p>Training another game would require the creation of a new wrapper with the same interface as <code>cannonball_wrapper</code>, a new <code>Action</code> enumerator specific to the game, a new <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> with a different reward function and the removal/replacement of the <code>speed</code> metric.</p>

<p>Apart from the previously mentioned steps, solving generic problems would require the preprocessor to be changed/replaced if images were not to be used for state representation. An option would be to create a new preprocessor class with a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/image_preprocessor.py#L13"><code>process(input)</code></a> method, tweak the hyperparameters as required (so that the network knows which dimensions to expect on its input), and finally inject the newly created class in <a href="https://github.com/lopespm/agent-preprocessor/blob/master/agent/trainer/episode.py#L300"><code>EpisodeRunner</code></a>, replacing the old preprocessor class.</p>

<h2 id="further-references">Further references</h2>

<p>I am not a machine learning expert, but from my learner’s point of view, if you are interested in getting your feet wet, Andrew Ng’s Machine Learning Course is as a great starting point. It is freely available on the <a href="https://www.coursera.org/learn/machine-learning">Coursera online learning platform</a>. This was my first solid contact with the subject and served as a major stepping stone for related topics such as Reinforcement Learning.</p>

<p><a href="https://www.udacity.com/course/deep-learning--ud730">Udacity Google Deep Learning</a>: this free course tackles some of the popular deep learning techniques, all the while using tensorflow. I did this right after Andrew Ng’s course and found it to leave the student with less support during lessons - less hand-holding if you will - and as result I spent a good amount of time dabbling to reach a solution for the assignments. </p>

<p>As a side note, I started building this project by the end of the Deep Learning course, mostly because I wanted to apply and consolidate the concepts I learnt into something more practical and to share this knowledge further, so it could hopefully help more people who are interested in this.</p>

<p>Other useful resources:</p>

<ul>
  <li>DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a> and its respective <a href="https://sites.google.com/a/deepmind.com/dqn/">source code</a></li>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></li>
  <li><a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">Deep Reinforcement Learning for Simulated Autonomous Vehicle Control</a></li>
  <li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
  <li><a href="https://www.udacity.com/course/reinforcement-learning--ud600">Udacity Reinforcement Learning by Georgia Tech</a></li>
  <li><a href="https://www.youtube.com/watch?v=dV80NAlEins">Deep learning lecture by Nando de Freitas</a></li>
  <li><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/">Learning reinforcement learning (with code, exercises and solutions)</a></li>
  <li><a href="https://gym.openai.com/">OpenAI Gym</a>: quoting the project’s page: <em>”a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Go”</em></li>
  <li><a href="https://github.com/devsisters/DQN-tensorflow">Tensorflow implementation</a> of Human-Level Control through Deep Reinforcement Learning, by Devsisters corp.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>All source code is available on GitHub:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer">Agent Trainer</a>: the core python+tensorflow application</li>
  <li><a href="https://github.com/lopespm/cannonball">Cannonball</a>: custom Cannonball (Out Run game emulator) fork which contains the changes needed to access the emulator externally</li>
  <li><a href="https://github.com/lopespm/agent-trainer-deployer">Agent Trainer Deployer</a>: bash scripts to deploy agent-trainer to a generic remote machine or AWS EC2 instance</li>
  <li><a href="https://github.com/lopespm/agent-trainer-docker">Agent Trainer Docker</a>: Dockerfiles used when deploying agent-trainer to a remote machine</li>
  <li><a href="https://github.com/lopespm/agent-trainer-results">Agent Trainer Results</a>: Collection of training sessions, each containing their resulting network, metrics and visualizations</li>
</ul>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>“Deep Q-network for playing” in this project is equivalent to DeepMind’s “target network $\hat Q$” and “Deep Q-network for training” is equivalent to DeepMind’s “network Q”<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Compressed Deque]]></title>
    <link href="http://lopespm.github.com/libraries/2016/08/27/compressed-deque.html"/>
    <updated>2016-08-27T00:00:00+01:00</updated>
    <id>http://lopespm.github.com/libraries/2016/08/27/compressed-deque</id>
    <content type="html"><![CDATA[<p>This post aims to give some insights about a recently <a href="https://github.com/lopespm/compressed-deque">open-sourced</a> python deque which compresses its items for a decreased volatile and persistent memory footprint.</p>

<!--more-->

<h2 id="why-how-can-it-be-useful">Why? How can it be useful?</h2>

<p>Compressed Deque came up while buiding an algorithm which relied on fast random accesses of numerous complex objects. When compressed, this blob of data could easily fit in fast volatile memory, as opposed to being fetched from a slow hard drive. Even though there was a performance penalty from compressing/decompressing items from volatile memory, this solution proved to be faster than accessing these items from indexed files on a hard disk (on a SSD the gains are more negligible though).</p>

<p>This collection might also be useful for programs running on devices with very limited memory available.</p>

<h2 id="structure">Structure</h2>

<p>Compressed Deque inherits from <a href="https://docs.python.org/2/library/collections.html#collections.deque">deque</a> and stores its items as zlib compressed pickles. The middle pickle layer only serves as a generic serialization method which can provide a serialized object string for zlib to compress. Although pickle can compress objects, its compression rate does not match zlib’s, even using <a href="https://docs.python.org/2/library/pickle.html#data-stream-format">higher protocols</a>.</p>

<p><img src="http://lopespm.github.com/files/compressed_deque/value_layers.png" alt="image" /></p>

<p><code>save_to_file()</code> and <code>load_from_file()</code> static methods are provided to persist the collection directly into disk in its compressed representation, without much overhead. </p>

<p>The persisted file contains a sequence of header/compressed_value pairs: the header is a 4 byte integer description of the compressed value’s length and the compressed value is similiar to its in-memory representation.</p>

<p><img src="http://lopespm.github.com/files/compressed_deque/persisted_values.png" alt="image" /></p>

<h2 id="how-to-use-it">How to Use it</h2>

<p>An easy way to install this package is by using <code>pip</code>:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ pip install compressed-deque
</pre></div>
</div>
</div>

<p><br /></p>

<p>You can then import the class and use it:</p>

<div><div class="CodeRay">
  <div class="code"><pre><span class="keyword">from</span> <span class="include">compdeque</span> <span class="keyword">import</span> <span class="include">CompressedDeque</span>

<span class="comment"># Instantiate the Deque</span>
collection = CompressedDeque()

<span class="comment"># Use it as a normal deque</span>
collection.append(<span class="integer">1</span>)

<span class="comment"># Persist to a file</span>
CompressedDeque.save_to_file(collection, file_path=<span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/collection.dat</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># ...</span>

<span class="comment"># and load it when you need it later</span>
loaded_collection = CompressedDeque.load_from_file(<span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/collection.dat</span><span class="delimiter">&quot;</span></span>)
</pre></div>
</div>
</div>

<p><br /></p>

<h2 id="source-code">Source Code</h2>

<p>Source code is available here: <a href="https://github.com/lopespm/compressed-deque">https://github.com/lopespm/compressed-deque</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Attenuate GoPro Chest Mount Heartbeat - using GarageBand/iMovie]]></title>
    <link href="http://lopespm.github.com/audio/2015/12/08/gopro-heartbeat.html"/>
    <updated>2015-12-08T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/audio/2015/12/08/gopro-heartbeat</id>
    <content type="html"><![CDATA[<p>If you have a chest mount (chesty) for your GoPro, then you might have had the surprise of hearing your heartbeat on the recording, since it may lay fairly close to your heart, picking up its beat. This post will:</p>

<ul>
  <li>show you how to attenuate the heartbeat sound to a point where it is almost imperceptible, while maintaining the overall sound</li>
  <li>attenuate the difference between quiet and loud sounds, so the listener gets less startled by a sudden noise created when entering with the bike on rought terrain or the common bike bell sound.</li>
</ul>

<!--more-->

<h2 id="steps-using-garagebandimovie">Steps, using GarageBand/iMovie</h2>

<p>In this specific case, GarageBand 10.0.3 and iMovie 10.0.5 were used, but other versions should work just as fine.</p>

<ol>
  <li>Open iMovie
    <ol>
      <li>Import your GoPro file(s) into your iMovie project. This is the best point in time to edit them and arrange the movie to your liking</li>
      <li>Export your movie (File-&gt;Share-&gt;File…)
        <ul>
          <li>Open GarageBand</li>
        </ul>
      </li>
      <li>Drag the exported movie into your GarageBand project
        <ul>
          <li>Decompress <a href="http://lopespm.github.com/files/gopro_heartbeat/gopro_master_track.patch.zip">gopro_master_track.patch.zip</a> and copy <strong>gopro_master_track.patch</strong> to the “~/Music/Audio Music Apps/Patches/Output”</li>
          <li>Go to your Master Track (if you do not see it, click “Track-&gt;Show MasterTrack”) and click “User Patches” in your Library tab (if you do not see it, click “View-&gt;Show Library”)</li>
          <li>You should now see inside the “User Patches” the patch file we just copied, select it</li>
          <li><em>Your audio should now have an attenuated heartbeat and a more audible and balanced sound</em></li>
          <li>Export the audio (Share-&gt;Export to Disk). AIFF or MP3 will work great. I personally use the MP3 192kBit/s export, since the GoPro audio is not extra pristine to start with, so the tiny losses in compression are pretty much neglegible</li>
          <li>Go back to iMovie</li>
        </ul>
      </li>
      <li>Remove the sound on your original GoPro video(s)
        <ul>
          <li>Drag the export audio from GarageBand, and place it below the video timeline</li>
          <li>Done!</li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<h2 id="why-did-this-hopefully-worked-for-you">Why did this *hopefully* worked for you?</h2>

<p>The whole concept is very simple really, the heartbeat is simply being attenuated on specific frequencies by a Channel EQ plugin included in the above Master Track patch. This is how it looks visually:</p>

<p><img src="http://lopespm.github.com/files/gopro_heartbeat/heartbeat_channel_eq.png" alt="image" /></p>

<p>This Channel EQ alone will turn this:</p>

<audio controls=""><source src="http://lopespm.github.com/files/gopro_heartbeat/gopro_sample_with_heartbeat.mp3" /></audio>

<p>into this:</p>

<audio controls=""><source src="http://lopespm.github.com/files/gopro_heartbeat/gopro_sample_without_heartbeat.mp3" /></audio>

<p>And that is it! The rest of the plugins included on the Master Track Patch are:</p>

<ul>
  <li>An additional Channel EQ that attenuates clicking sounds I heard along my videos</li>
  <li>Two compressors and a limiter which increase the overall loudness and diminish the difference between quiet and loud sounds</li>
</ul>

<h2 id="support-files">Support Files:</h2>

<ul>
  <li><a href="http://lopespm.github.com/files/gopro_heartbeat/gopro_master_track.patch.zip">Master Track Patch</a> (which includes all the above plugins)</li>
  <li><a href="http://lopespm.github.com/files/gopro_heartbeat/gopro_eq_heartbeat_attenuate.pst">Channel EQ preset</a> to attenuate the heartbeat sound. This can be used by opening a Channel EQ plugin in any of your tracks and clicking “More-&gt;Load..” inside it</li>
  <li><a href="https://www.youtube.com/watch?v=sczmkokpEuY">Example video</a> in which these plugins were applied</li>
</ul>

<p><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quickly Customize OSX Terminal Style and Functionality]]></title>
    <link href="http://lopespm.github.com/workflow/2015/01/27/customizing-terminal.html"/>
    <updated>2015-01-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/workflow/2015/01/27/customizing-terminal</id>
    <content type="html"><![CDATA[<p><img src="http://lopespm.github.com/images/blog_images/terminal_custom/terminal_before_after.png" alt="image" /></p>

<!--more-->

<h3 id="style---how-it-looks">Style - how it looks</h3>
<p>Add the <em>real world tested</em> <a href="http://ethanschoonover.com/solarized">Solarized (dark/light)</a> Terminal profiles: <a href="https://github.com/tomislav/osx-terminal.app-colors-solarized">https://github.com/tomislav/osx-terminal.app-colors-solarized</a></p>

<ul>
  <li>After cloning the repo, just open the profile files, which will add them to your Terminal.app preferences</li>
  <li>You can set them as the default profiles on your Terminal.app preferences</li>
  <li>To enable the terminal colors, add the line <code>export CLICOLOR=1</code> to your .bash_profile</li>
</ul>

<h3 id="function---what-it-does">Function - what it does</h3>
<p>Grab the .bash_profile here: <a href="https://gist.github.com/lopespm/3877ff1ca4064a392686">https://gist.github.com/lopespm/3877ff1ca4064a392686</a>, which is heavily based on <a href="http://natelandau.com/my-mac-osx-bash_profile/">Nathaniel Landau’s profile</a></p>

<ul>
  <li>In OSX, .bash_profile is executed <a href="http://www.joshstaiger.org/archives/2005/07/bash_profile_vs.html">everytime you open a new Terminal window</a>, and it lives in your user’s home directory, i.e. <code>~/.bash_profile</code></li>
  <li>If it does not exist, just <code>touch ~/.bash_profile</code> to create it</li>
  <li>If you do not want to clutter your .bash_profile, you can create a file like <code>~/scripts/utilities.bash</code> with the portions you find useful and then add this line to your .bash_profile: <code>source ~/scripts/utilities.bash</code></li>
</ul>

<p>Hopefully now your Terminal is even more inviting :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Survival Ball on OUYA]]></title>
    <link href="http://lopespm.github.com/games/2013/09/08/survival-ball-ouya.html"/>
    <updated>2013-09-08T00:00:00+01:00</updated>
    <id>http://lopespm.github.com/games/2013/09/08/survival-ball-ouya</id>
    <content type="html"><![CDATA[<p>Survival Ball in now available on OUYA, and its publishing process was fairly straightforward. From the day of submission (2nd of September) to its approval (5th of September), it took a mere 3 days, counting that a re-submission had to be made in-between.</p>

<p>Survival Ball was already available on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.survivalball">Google Play</a>, <a href="http://www.amazon.com/RockByte-Software-Survival-Ball/dp/B00ARVZ7F0/ref=sr_1_1?s=mobile-apps&amp;ie=UTF8&amp;qid=1378394674&amp;sr=1-1">Amazon </a>and <a href="http://www.kongregate.com/games/rockbyte/survival-ball">Kongregate</a>, but OUYA just felt like the perfect platform for the game. With it, you could just grab the OUYA controller and/or a spare XBox/PS3 controller, and just have a quick match. I really loved the concept, and re-iterated on the previous version of the game to give it more controller friendly menus, better graphics, refined textures, and an overall revamp.</p>

<!--more-->

<p>After finishing up the port to OUYA, on the 2nd of September, I’ve made the first submission. I’ll just point out that upon this moment, it seemed to be stuck with a red status message which read “<em>Still verifying apk…</em>”. To resolve this, I edited the submission, re-submitted the APK, and waited a few seconds. The game was then submitted for approval.</p>

<p>On the 4th of September, a re-submission was asked by the OUYA review team (which were very friendly by the way), since some elements fell of the safe-frame, which is safe-guard for gamers who have screens that perform <a href="https://devs.ouya.tv/developers/docs/content-review-guidelines">too much overscan</a>.</p>

<p>The game was re-submitted on the same day, and was approved on the 5th of September. From this point on, one can release the game whenever it seems more appropriate. The game was released on the same day.</p>

<p>I’ve came to learn during this process, after publishing the game, it first goes to the “Sandbox” section. Only after a major acceptance from the game community, can the game be promoted to the primary categories. So, if you would like to take a quick peek at the game, you can find it at the “Sandbox” section.</p>

<center><iframe width="560" height="315" src="http://lopespm.github.com//www.youtube.com/embed/ZrqakDKWwjg" frameborder="0" allowfullscreen=""></iframe></center>

<p><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nordic Game Jam’s winners]]></title>
    <link href="http://lopespm.github.com/games/2013/01/27/nordic-game-jams-winners.html"/>
    <updated>2013-01-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.com/games/2013/01/27/nordic-game-jams-winners</id>
    <content type="html"><![CDATA[<p>This year’s Nordic Game Jam was crowned as the world’s largest game jam with about 470 participants. The winners have been announced, and can see the full list here: <a href="http://nordicgamejam.org/2013/01/21/winners-ngj13/">http://nordicgamejam.org/2013/01/21/winners-ngj13/</a></p>

<p>My personal highlight goes to Stalagflight (<a href="https://dl.dropbox.com/u/85666/Stalagflight-web/webBuild.html">https://dl.dropbox.com/u/85666/Stalagflight-web/webBuild.html</a>), which is as simple as stupidly fun! Stalagflight can be played with the keyboard, but it is tricky. If you have a gamepad the experience is much better.</p>

<!--more-->

<p>On other lines, if you want to stretch the face of Justin Bieber and force him to make heavenly sounds, rejoice with this: <a href="http://unicorn7.org/static/be/">http://unicorn7.org/static/be/</a></p>
]]></content>
  </entry>
  
</feed>
