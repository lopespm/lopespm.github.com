<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Byte Tank - Pedro Lopes Blog]]></title>
  <link href="http://lopespm.github.io/atom.xml" rel="self"/>
  <link href="http://lopespm.github.io/"/>
  <updated>2021-07-06T22:02:46+01:00</updated>
  <id>http://lopespm.github.io/</id>
  <author>
    <name><![CDATA[Pedro Lopes]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hacker News Daily]]></title>
    <link href="http://lopespm.github.io/2020/12/25/hackernews-daily.html"/>
    <updated>2020-12-25T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/2020/12/25/hackernews-daily</id>
    <content type="html"><![CDATA[<p>This article will dive into the system behind the generation of <a href="https://lopespm.github.io/hackernews_daily">Hacker News Daily</a>, an <a href="https://github.com/lopespm/hackernews_daily">open source</a> lightweight daily Hacker News best stories website, with screenshots and top comments.</p>

<center>
          <picture>
             <source type="image/webp" srcset="/files/hackernews_daily/screenshots_cropped2_705.webp" />
             <source type="image/png" srcset="/files/hackernews_daily/screenshots_cropped2_705.png" />
             <img src="http://lopespm.github.io/files/hackernews_daily/screenshots_cropped2_705.png" />
           </picture>
         </center>

<!--more-->

<p><br /></p>

<h1 id="motivation">Motivation</h1>

<p>Hacker News is my go-to source for relevant, interesting and constructive discussions on a wide range of topics. I usually consume it via <a href="https://www.daemonology.net/hn-daily/">Daemonology’s Hacker News Daily</a>, to catch up on the most active topics in the community.</p>

<p><a href="https://www.daemonology.net/hn-daily/">Daemonology’s Hacker News Daily</a> presents the title, story link, Hacker News discussion link, and is optimized for desktop. I usually consult it on mobile, and when I am several days behind, I sweep through the archives and open several story/discussion links in separate mobile tabs, in order to triage the stories with a quick glance at their web page and their discussion’s top comments.</p>

<p>This project aims to ease that process, by generating a set of web pages presenting the best daily Hacker News for mobile or desktop, with screenshots and top comments for each story, while aiming to have a low footprint to the end user.</p>

<h1 id="architecture">Architecture</h1>

<figure class="image"><center><a href="http://lopespm.github.io/files/hackernews_daily/hn_daily_diagram.png"><img src="http://lopespm.github.io/files/hackernews_daily/hn_daily_diagram.png" /></a></center><figcaption class="media-caption"> Information flow diagram between the different components of the system </figcaption></figure>

<p>The basilar idea is to recurrently get the <a href="https://news.ycombinator.com/best">best Hacker News</a> stories via its <a href="https://github.com/HackerNews/API">API</a>; take screenshots of the web pages that these stories link to; and have this workflow executed via <a href="https://github.com/features/actions">GitHub Actions</a>.</p>

<p>In specific, these are the steps taken to generate the final web pages (illustrated by the above diagram):</p>

<p><strong>1.</strong> Every day, a new <a href="https://github.com/lopespm/hackernews_daily/actions">Github Workflow</a> is spawned, kickstarting the entire process.</p>

<p><strong>2.</strong> Update History</p>

<ul>
  <li>The first step is to get the past days best stories from the last successful run, and add the current best stories by
 reaching out to the Hacker News API to fetch the current list of best stories, removing the ones that were already featured, and adding the top ones to the current history
    <ul>
      <li>To persist the past best stories history, an <code>days_history.dat</code> artifact is created in every run, which is a simple <a href="https://docs.python.org/3/library/pickle.html">Python Pickle</a> containing the story ids from the past days. These days are stored in form of a deque, in order to pop the older days as the history grows larger</li>
      <li>The final result of this step is a new <code>days_history.dat</code>, which will be used in the next step below, and will also serve as a base for the next workflow</li>
      <li>If a viable <code>days_history.dat</code> artifact cannot not be found in the previous successful runs, it is rebuilt by parsing the <a href="https://www.daemonology.net/hn-daily/">Daemonology’s Hacker News Daily</a> web page</li>
    </ul>
  </li>
</ul>

<p><strong>3.</strong> Create Day/Story Models</p>

<ul>
  <li>From the story ids provided by the previously built <code>days_history.dat</code>, a list of hydrated days with their respective stories are built. These models will posteriorly provide all the information needed to create the final web page views
    <ul>
      <li>Each story is composed of an ID, title, story link, Hacker News discussion link, and its top comments. This information is obtained via the Hacker News API (example <a href="https://hacker-news.firebaseio.com/v0/item/25518730.json?print=pretty">call</a>)</li>
    </ul>
  </li>
</ul>

<p><strong>4.</strong> Create the <code>generated</code> folder. This is where the generated web pages and screenshots will be placed, in order to be later deployed to GitHub pages</p>

<p><strong>5.</strong> Gather Screenshots</p>

<ul>
  <li>Using <a href="https://pypi.org/project/pyppeteer/">pyppeteer</a>, an headless browser is created to navigate through all the story links
    <ul>
      <li>A screenshot is taken for each of these pages (after attempting to dismiss a possible “Allow Cookies” prompt), and stored as PNGs in the <code>generated</code> folder</li>
      <li>Each of these screenshots are re-encoded in <a href="https://en.wikipedia.org/wiki/WebP">WebP</a>, which allows for a smaller image footprint in browser’s that support it</li>
    </ul>
  </li>
  <li>If there is an unexpected error while processing the screenshots, it will not halt the overall workflow, since the final web pages can still function without screenshot images</li>
</ul>

<p><strong>6</strong>. Generate Web Pages</p>

<ul>
  <li><a href="https://jinja.palletsprojects.com/en/2.11.x/">Jinja</a>, a templating engine for Python, is used to define and generate each of the final web pages. Four web pages are generated from a <a href="https://github.com/lopespm/hackernews_daily/blob/main/templates/page.html">single template</a>. These four variants are permutations of either presenting all past 10 days or not, and either showing images or not:
    <ul>
      <li>Latest day, with images</li>
      <li>Latest day, without images</li>
      <li>Past 10 days, with images</li>
      <li>Past 10 days, without images</li>
    </ul>
  </li>
  <li>It was chosen to keep these in separate static web pages, in order to keep them simple, static, and without additional dynamic logic</li>
</ul>

<p><strong>7.</strong> The <code>generated</code> folder is deployed to the <code>gh-pages</code> branch, which is deployed as a GitHub page, making these generated contents <a href="https://lopespm.github.io/hackernews_daily">publicly accessible</a></p>

<h1 id="source-code">Source Code</h1>

<p>The full source code is accessible at <a href="https://github.com/lopespm/hackernews_daily">https://github.com/lopespm/hackernews_daily</a> and the generated website at <a href="https://lopespm.github.io/hackernews_daily">https://lopespm.github.io/hackernews_daily</a>, feel free to improve it or to leave some feedback.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Implementation: Autocomplete System Design for Large Scale]]></title>
    <link href="http://lopespm.github.io/2020/08/03/implementation-autocomplete-system-design.html"/>
    <updated>2020-08-03T00:00:00+01:00</updated>
    <id>http://lopespm.github.io/2020/08/03/implementation-autocomplete-system-design</id>
    <content type="html"><![CDATA[<p>This article will go through my implementation/design of a large scale autocomplete/typeahead suggestions system design, like the suggestions one gets when typing a Google search.</p>

<p>This design was implemented using Docker Compose<sup id="fnref:10"><a href="#fn:10" rel="footnote">1</a></sup>, and you can find the source code here: <a href="https://github.com/lopespm/autocomplete">https://github.com/lopespm/autocomplete</a></p>

<div style="margin-bottom: 20px">
    <div class="youtube-player" data-id="UFr5Mb7YXRg"></div>
</div>
<p class="media-caption media-caption-one">YouTube video overview of this implementation</p>

<!--more-->

<h1 id="requirements">Requirements</h1>

<p>The design has to accommodate a Google-like scale of about 5 billion daily searches, which translates to about 58 thousand queries per second. We can expect 20% of these searches to be unique, this is, 1 billion queries per day.</p>

<p>If we choose to index 1 billion queries, with 15 characters on average per query<sup id="fnref:12"><a href="#fn:12" rel="footnote">2</a></sup> and 2 bytes per character (we will only support the english locale), then we will need about 30GB of storage to host these queries.</p>

<h2 id="functional-requirements">Functional Requirements</h2>

<ul>
  <li>Get a list of top phrase suggestions based on the user input (a prefix)</li>
  <li>Suggestions ordered by weighting the frequency and recency of a given phrase/query<sup id="fnref:11"><a href="#fn:11" rel="footnote">3</a></sup></li>
</ul>

<p>The main two APIs will be:</p>

<ul>
  <li><em>top-phrases(prefix)</em>: returns the list of top phrases for a given prefix</li>
  <li><em>collect-phrase(phrase)</em>: submits the searched phrase to the system. This phrase will later be used by the assembler to build a data structure which maps a prefix to a list of top phrases</li>
</ul>

<h2 id="non-functional-requirements">Non-Functional Requirements</h2>

<ul>
  <li><em>Highly Available</em></li>
  <li><em>Performant</em> - response time for the top phrases should be quicker than the user’s type speed (&lt; 200ms)</li>
  <li><em>Scalable</em> - the system should accommodate a large number of requests, while still maintaining its performance</li>
  <li><em>Durable</em> - previously searched phrases (for a given timespan) should be available, even if there is a hardware fault or crash</li>
</ul>

<h1 id="design--implementation">Design &amp; Implementation</h1>

<h2 id="high-level-design">High-level Design</h2>

<div style="margin-bottom: 7px">
<center>
          <picture>
             <source type="image/webp" srcset="/files/autocomplete/phigh_level_design.webp" />
             <source type="image/png" srcset="/files/autocomplete/phigh_level_design.png" />
             <img src="http://lopespm.github.io/files/autocomplete/phigh_level_design.png" />
           </picture>
         </center>
</div>

<p>The two main sub-systems are: </p>

<ul>
  <li>the distributor, which handles the user’s requests for the top phrases of a given prefix</li>
  <li>the assembler, which collects user searches and assembles them into a data structure that will later be used by the distributor</li>
</ul>

<h2 id="detailed-design">Detailed Design</h2>

<div style="margin-top: 25px">
<center>
          <picture>
             <source type="image/webp" srcset="/files/autocomplete/psystem_design_diagram.webp" />
             <source type="image/png" srcset="/files/autocomplete/psystem_design_diagram.png" />
             <img src="http://lopespm.github.io/files/autocomplete/psystem_design_diagram.png" />
           </picture>
         </center>
</div>

<p>This implementation uses off-the-shelf components like kafka (message broker), hadoop (map reduce and distributed file system), redis (distributed cache) and nginx (load balancing, gateway, reverse proxy), but also has custom services built in python, namely the trie distribution and building services. The trie data structure is custom made as well.</p>

<p>The backend services in this implementation are built to be self sustainable and don’t require much orchestration. For example, if an active backend host stops responding, it’s corresponding ephemeral znode registry eventually disappears, and another standby backend node takes its place by attempting to claim the position via an <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Ephemeral+Nodes">ephemeral</a> <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Sequence+Nodes+--+Unique+Naming">sequential</a> znode on zookeeper.</p>

<h3 id="trie-the-bedrock-data-structure">Trie: the bedrock data structure</h3>

<p>The data structure used by, and provided to the distributor is a <a href="https://en.wikipedia.org/wiki/Trie">trie</a>, with each of its prefix nodes having a list of top phrases. The top phrases are referenced using the <a href="https://en.wikipedia.org/wiki/Flyweight_pattern">flyweight pattern</a>, meaning that the string literal of a phrase is stored only once. Each prefix node has a list of top phrases, which are a list of references to string literals.</p>

<p>As we’ve seen before, we will need about 30GB to index 1 billion queries, which is about the memory we would need for the above mentioned trie to store 1 billion queries. Since we want to keep the trie in memory to enable fast lookup times for a given query, we are going to partition the trie into multiple tries, each one on a different machine. This relieves the memory load on any given machine.</p>

<p>For increased availability, the services hosting these tries will also have multiple replicas. For increased durability, the serialized version of the tries will be available in a distributed file system (HDFS), and these can be rebuilt via map reduce tasks in a predictable, deterministic way.</p>

<h2 id="information-flow">Information Flow</h2>

<h3 id="assembler-collect-data-and-assemble-the-tries">Assembler: collect data and assemble the tries</h3>

<ol>
  <li>Client submits the searched phrase to the gateway via <code>http://localhost:80/search?phrase="a user query"</code></li>
  <li>Since the search service is outside the scope of this implementation, the gateway directly sends the searched phrase to the collector’s load balancer via <code>http://assembler.collector-load-balancer:6000/collect-phrase?phrase="a user query"</code></li>
  <li>The collector’s load balancer forwards the request to one of the collector backends via <code>http://assembler.collector:7000/collect-phrase?phrase="a user query"</code></li>
  <li>The collector backend sends a message to the <em>phrases</em> topic to the message broker (kafka). The key and value are the phrase itself <sup id="fnref:1"><a href="#fn:1" rel="footnote">4</a></sup></li>
  <li>The Kafka Connect HDFS Connector <code>assembler.kafka-connect</code> dumps the messages from the <em>phrases</em> topic into the <code>/phrases/1_sink/phrases/{30 minute window timestamp}</code><sup id="fnref:2"><a href="#fn:2" rel="footnote">5</a></sup> folder <sup id="fnref:3"><a href="#fn:3" rel="footnote">6</a></sup></li>
  <li>Map Reduce jobs are triggered <sup id="fnref:4"><a href="#fn:4" rel="footnote">7</a></sup>: they will reduce the searched phrases into a single HDFS file, by weighting the recency and frequency of each phrase <sup id="fnref:5"><a href="#fn:5" rel="footnote">8</a></sup>
    <ol>
      <li>A <code>TARGET_ID</code> is generated, according to the current time, for example <code>TARGET_ID=20200807_1517</code></li>
      <li>The first map reduce job is executed for the <em>K</em> <sup id="fnref:6"><a href="#fn:6" rel="footnote">9</a></sup> most recent <code>/phrases/1_sink/phrases/{30 minute window timestamp}</code> folders, and attributes a base weight for each of these (the more recent, the higher the base weight). This job will also sum up the weights for the same phrase in a given folder. The resulting files will be stored in the <code>/phrases/2_with_weight/2_with_weight/{TARGET_ID}</code> HDFS folder</li>
      <li>The second map reduce job will sum up all the weights of a given phrase from <code>/phrases/2_with_weight/2_with_weight/{TARGET_ID}</code> into <code>/phrases/3_with_weight_merged/{TARGET_ID}</code></li>
      <li>The third map reduce job will order the entries by descending weight, and pass them through a single reducer, in order to produce a single file. This file is placed in <code>/phrases/4_with_weight_ordered/{TARGET_ID}</code></li>
      <li>The zookeper znode <code>/phrases/assembler/last_built_target</code> is set to the <code>TARGET_ID</code></li>
    </ol>
  </li>
  <li>The Trie Builder service, which was listening to changes in the <code>/phrases/assembler/last_built_target</code> znode, builds a trie for each partition<sup id="fnref:7"><a href="#fn:7" rel="footnote">10</a></sup>, based on the <code>/phrases/4_with_weight_ordered/{TARGET_ID}</code> file. For example, one trie may cover the prefixes until <em>mod</em>, another from <em>mod</em> to <em>racke</em>, and another from <em>racke</em> onwards.
    <ol>
      <li>Each trie is serialized and written into the <code>/phrases/5_tries/{TARGET_ID}/{PARTITION}</code> HDFS file (e.g. <code>/phrases/5_tries/20200807_1517/mod\|racke</code>), and the zookeeper znode <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/trie_data_hdfs_path</code> is set to the previously mentioned HDFS file path.</li>
      <li>The service sets the zookeper znode <code>/phrases/distributor/next_target</code> to the <code>TARGET_ID</code></li>
    </ol>
  </li>
</ol>

<h3 id="transfering-the-tries-to-the-distributor-sub-system">Transfering the tries to the Distributor sub-system</h3>

<ol>
  <li>The distributor backend nodes can either be in active mode (serving requests) or standby mode. The nodes in standby mode will fetch the most recent tries, load them into memory, and mark themselves as ready to take over the active position. In detail:
    <ol>
      <li>The standby nodes, while listening to changes to the znode <code>/phrases/distributor/next_target</code>, detect its modification and create an <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Ephemeral+Nodes">ephemeral</a> <a href="https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#Sequence+Nodes+--+Unique+Naming">sequential</a> znode, for each partition, one at a time, inside the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/</code> znode. If the created znode is one of the first R znodes (R being the number of replica nodes per partition <sup id="fnref:8"><a href="#fn:8" rel="footnote">11</a></sup>), proceed to the next step. Otherwise, remove the znode from this partition and try to join the next partition.</li>
      <li>The standby backend node fetches the serialized trie file from <code>/phrases/5_tries/{TARGET_ID}/{PARTITION}</code>, and starts loading the trie into memory.</li>
      <li>When the trie is loaded into memory, the standby backend node marks itself as ready by setting the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/{CREATED_ZNODE}</code> znode to the backend’s hostname.</li>
    </ol>
  </li>
  <li>The Trie Backend Applier service polls the <code>/phrases/distributor/{TARGET_ID}/</code> sub-znodes (the <code>TARGET_ID</code> is the one defined in <code>/phrases/distributor/next_target</code>), and checks if all the nodes in all partitions are marked as ready
    <ol>
      <li>If all of them are ready for this next <code>TARGET_ID</code>, the service, in a single transaction, changes the value of the <code>/phrases/distributor/next_target</code> znode to empty, and sets the <code>/phrases/distributor/current_target</code> znode to the new <code>TARGET_ID</code>. With this single step, all of the standby backend nodes which were marked as ready will now be active, and will be used for the following Distributor requests.</li>
    </ol>
  </li>
</ol>

<h3 id="distributor-handling-the-requests-for-top-phrases">Distributor: handling the requests for Top Phrases</h3>

<p>With the distributor’s backend nodes active and loaded with their respective tries, we can start serving top phrases requests for a given prefix:</p>

<ol>
  <li>Client requests the gateway for the top phrases for a given prefix via <code>http://localhost:80/top-phrases?prefix="some prefix"</code></li>
  <li>The gateway sends this request to the distributor’s load balancer via <code>http://distributor.load-balancer:5000/top-phrases?prefix="some prefix"</code></li>
  <li>The load balancer forwards the request to one of the frontends via <code>http://distributor.frontend:8000/top-phrases?prefix="some prefix"</code></li>
  <li>The frontend service handles the request:
    <ol>
      <li>The frontend service checks if the distributed cache (redis) has an entry for this prefix <sup id="fnref:9"><a href="#fn:9" rel="footnote">12</a></sup>. If it does, return these cached top phrases. Otherwise, continue to next step</li>
      <li>The frontend service gets the partitions for the current <code>TARGET_ID</code> from zookeeper (<code>/phrases/distributor/{TARGET_ID}/partitions/</code> znode), and picks the one that matches the provided prefix</li>
      <li>The frontend service chooses a random znode from the <code>/phrases/distributor/{TARGET_ID}/partitions/{PARTITION}/nodes/</code> znode, and gets its hostname</li>
      <li>The frontend service requests the top phrases, for the given prefix, from the selected backend via <code>http://{BACKEND_HOSTNAME}:8001/top-phrases="some prefix"</code>
        <ol>
          <li>The backend returns the list of top phrases for the given prefix, using its corresponding loaded trie</li>
        </ol>
      </li>
      <li>The frontend service inserts the list of top phrases into the distributed cache (cache aside pattern), and returns the top phrases</li>
    </ol>
  </li>
  <li>The top phrases response bubbles up to the client</li>
</ol>

<h2 id="zookeper-znodes-structure">Zookeper Znodes structure</h2>

<p>Note: Execute the shell command <code>docker exec -it zookeeper ./bin/zkCli.sh</code> while the system is running to explore the current zookeeper’s znodes.</p>

<ul>
  <li>phrases
    <ul>
      <li>distributor</li>
      <li>assembler
        <ul>
          <li>last_built_target <em>- set to a <code>TARGET_ID</code></em></li>
        </ul>
      </li>
      <li>distributor
        <ul>
          <li>current_target <em>- set to a <code>TARGET_ID</code></em></li>
          <li>next_target <em>- set to a <code>TARGET_ID</code></em></li>
          <li><code>{TARGET_ID}</code> <em>- e.g. 20200728_2241</em>
            <ul>
              <li>partitions
                <ul>
                  <li>|<em>{partition 1 end}</em>
                    <ul>
                      <li>trie_data_hdfs_path <em>- HDFS path where the serialized trie is saved</em></li>
                      <li>nodes
                        <ul>
                          <li>0000000000</li>
                          <li>0000000001</li>
                          <li>0000000002</li>
                          <li>…</li>
                        </ul>
                      </li>
                    </ul>
                  </li>
                  <li><em>{partition 2 start}</em>|<em>{partition 2 end}</em>
                    <ul>
                      <li>…</li>
                    </ul>
                  </li>
                  <li><em>{partition 3 start}</em>|
                    <ul>
                      <li>…</li>
                    </ul>
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="hdfs-folder-structure">HDFS folder structure</h2>

<p>Note: Access <code>http://localhost:9870/explorer.html</code> in your browse while the system is running to browse the current HDFS files and folders.</p>

<ul>
  <li>phrases
    <ul>
      <li>1_sink *- the searched phrases are dumped here, partitioned into 30min time blocks, *
        <ul>
          <li><em>{e.g 20200728_2230}</em></li>
          <li><em>{e.g 20200728_2300}</em></li>
        </ul>
      </li>
      <li>2_with_weight <em>- phrases with their initial weight applied, divided by time block</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>3_with_weight_merged <em>- consolidation of all the time blocks: phrases with their final weight</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>4_with_weight_ordered <em>- single file of phrases ordered by descending weight</em>
        <ul>
          <li><code>{TARGET_ID}</code></li>
        </ul>
      </li>
      <li>5_tries <em>- storage of serialized tries</em>
        <ul>
          <li><code>{TARGET_ID}</code>
            <ul>
              <li>|<em>{partition 1 end}</em></li>
              <li><em>{partition 2 start}</em>|<em>{partition 2 end}</em></li>
              <li><em>{partition 3 start}</em>|</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="client-interaction">Client interaction</h2>

<p>You can interact with the system by accessing <code>http://localhost</code> in your browser. The search suggestions will be provided by the system as you write a query, and you can feed more queries/phrases into the system by submitting more searches.</p>

<center>
          <picture>
             <source type="image/webp" srcset="/files/autocomplete/pclient_web_705.webp" />
             <source type="image/png" srcset="/files/autocomplete/pclient_web_705.png" />
             <img src="http://lopespm.github.io/files/autocomplete/pclient_web_705.png" />
           </picture>
         </center>

<h1 id="source-code">Source Code</h1>

<p>You can get the full source code at <a href="https://github.com/lopespm/autocomplete">https://github.com/lopespm/autocomplete</a>. I would be happy to know your thoughts about this implementation and design.</p>

<p><br /></p>
<hr />

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:10">
      <p>Docker compose was used instead of a container orchestrator tool like Kubernetes or Docker Swarm, since the main objective of this implementation was to build and share a system in simple manner.<a href="#fnref:10" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p>The average length of a search query was <a href="https://en.wikipedia.org/wiki/Web_search_query">2.4 terms</a>, and the average word length in English language is <a href="https://medium.com/@wolfgarbe/the-average-word-length-in-english-language-is-4-7-35750344870f">4.7 characters</a><a href="#fnref:12" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p><em>Phrase</em> and <em>Query</em> are used interchangeably in this article. Inside the system though, only the term <em>Phrase</em> is used.<a href="#fnref:11" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:1">
      <p>In this implementation, only one instance of the broker is used, for clarity. However, for a large number of incoming requests it would be best to partition this topic along multiple instances (the messages would be partitioned according to the <em>phrase</em> key), in order to distribute the load.<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p><em>/phrases/1_sink/phrases/{30 minute window timestamp} folder</em>: For example, provided the messages A[time: 19h02m], B[time: 19h25m], C[time: 19h40m], the messages A and B would be placed into folder <em>/phrases/1_sink/phrases/20200807_1900</em>, and message C into folder <em>/phrases/1_sink/phrases/20200807_1930</em><a href="#fnref:2" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>We could additionally pre-aggregate these messages into another topic (using Kafka Streams), before handing them to Hadoop<a href="#fnref:3" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>For clarity, the map reduce tasks are triggered manually in this implementation via <em>make do_mapreduce_tasks</em>, but in a production setting they could be triggered via cron job every 30 minutes for example.<a href="#fnref:4" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>An additional map reduce could be added to aggregate the <em>/phrases/1_sink/phrases/</em> folders into larger timespan aggregations (e.g. 1-day, 5-week, 10-day, etc)<a href="#fnref:5" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Configurable in <em>assembler/hadoop/mapreduce-tasks/do_tasks.sh</em>, by the variable MAX_NUMBER_OF_INPUT_FOLDERS<a href="#fnref:6" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>Partitions are defined in <em>assembler/trie-builder/triebuilder.py</em><a href="#fnref:7" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>The number of replica nodes per partition is configured via the environment variable NUMBER_NODES_PER_PARTITION in docker-compose.yml<a href="#fnref:8" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>The distributed cache is disabled by default in this implementation, so that it is clearer for someone using this codebase for the first time to understand what is happening on each update/step. The distributed cache can be enabled via the environment variable DISTRIBUTED_CACHE_ENABLED in docker-compose.yml<a href="#fnref:9" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Survival Ball: Making the Game]]></title>
    <link href="http://lopespm.github.io/2019/02/06/survival-ball-making-the-game.html"/>
    <updated>2019-02-06T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/2019/02/06/survival-ball-making-the-game</id>
    <content type="html"><![CDATA[<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
   <div class="youtube-player video-frame-two" data-id="VO6P3h9RRcg"></div></div>
 <p class="media-caption media-caption-two"><b>Before:</b> Initial prototype, built 6 years ago</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="DBqUGRCwBGc"></div></div>
 <p class="media-caption media-caption-two"><b>After:</b> Current version, released last November<br class="video-br-end" /></p>
</div>
</center>

<p>This article/postmortem provides an in-depth look into the process of building <a href="https://survivalball.com/">Survival Ball</a>, a Single / Local Co-Op physics-based game available on Steam for Windows and macOS. From prototype until showcase at Lisboa Games Week, passing by the related principles, design decisions, level creation process, tools and technical details.  </p>

<!--more-->

<hr />

<p><br /></p>

<ul id="markdown-toc">
  <li><a href="#prototype">Prototype</a></li>
  <li><a href="#reboot">Reboot</a>    <ul>
      <li><a href="#root-ideas-and-game-modes">Root ideas and game modes</a></li>
      <li><a href="#difficulty">Difficulty</a></li>
      <li><a href="#control-and-movement">Control and movement</a></li>
      <li><a href="#powerups">Powerups</a></li>
      <li><a href="#visual-aesthetics">Visual Aesthetics</a></li>
      <li><a href="#codebase">Codebase</a></li>
      <li><a href="#levels">Levels</a>        <ul>
          <li><a href="#first-sandbox">First Sandbox</a></li>
          <li><a href="#rocketx">RocketX</a></li>
          <li><a href="#garfunkel">Garfunkel</a></li>
          <li><a href="#beatmill">Beatmill</a></li>
          <li><a href="#big-giant-head">Big Giant Head</a></li>
          <li><a href="#unfair-fair">Unfair Fair</a></li>
          <li><a href="#venom-rig">Venom Rig</a></li>
          <li><a href="#hex-elevator">Hex Elevator</a></li>
          <li><a href="#tutorial">Tutorial</a></li>
          <li><a href="#extra-block-towers">Extra: Block Towers</a></li>
        </ul>
      </li>
      <li><a href="#menus-and-user-interface">Menus and User Interface</a></li>
      <li><a href="#audio">Audio</a>        <ul>
          <li><a href="#keys">Keys</a></li>
          <li><a href="#eq">EQ</a></li>
          <li><a href="#organization">Organization</a></li>
        </ul>
      </li>
      <li><a href="#play-tests">Play Tests</a>        <ul>
          <li><a href="#ai-agents">AI agents</a></li>
          <li><a href="#user-tests">User Tests</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#launch">Launch</a></li>
  <li><a href="#indie-x">Indie X</a></li>
  <li><a href="#additional-notes--appendix">Additional Notes / Appendix</a>    <ul>
      <li><a href="#tools">Tools</a></li>
      <li><a href="#time-log">Time log</a>        <ul>
          <li><a href="#time-for-all-categories-per-milestone">Time for all categories, per milestone:</a></li>
          <li><a href="#time-until-now-per-category">Time until now, per category</a></li>
        </ul>
      </li>
      <li><a href="#final-notes">Final Notes</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h1 id="prototype">Prototype</h1>

<p>About six years ago I started learning Unity in my spare time, going through several official and unofficial tutorials. One was a simple tutorial on how to create a wire chain through physics. After its completion, I added a simple platform and a sphere. When arrow keys were pressed the sphere was torqued, and on spacebar press a vertical force was added to sphere, which caused it to jump. I was at awe. Unity’s physics were accessible and felt natural out of the box. Not only that, but a rough working scene could be quickly setup by gluing some basic concepts.</p>

<div>
    <div class="youtube-player" data-id="3goAGlKg92Q" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Wire chain and sphere physics</p>

<p>An idea started to bubble up about around this simple project: staying as long as possible on top of a platform. Antagonists, materialized in the form of various hazards, were added in order to coerce (directly or indirectly) the player off the platform. Frequency and power of these hazards increased over time, raising the probability of reaching the level’s finishing state. From this starting point, more elements were added, such as interactable platform edge blocks, destroyable floor tiles, backdrops, user interface and basic sound effects.</p>

<p>Two versions resulted from this. The first ad supported version was available for <a href="https://play.google.com/store/apps/details?id=com.rockbyte.survivalball">Android mobile</a> devices only, having one playable level in single-player mode. Afterwards, a new adless version was released for <a href="https://www.ouya.tv/game/Survival-Ball/">OUYA</a>, <a href="https://play.google.com/store/apps/details?id=com.rockbyte.survivalltv">Android TV</a> and Kongregate, featuring a new versus mode.</p>

<div>
    <div class="youtube-player" data-id="ZrqakDKWwjg"></div>
</div>
<p class="media-caption media-caption-one">Game prototype version for OUYA, Android TV and Kongregate</p>

<h1 id="reboot">Reboot</h1>

<p>I went to work on other projects after the prototype’s launch, but the feedback provided from gamers was kept in the back of my mind in the meanwhile. Several ideas eventually piled up for a new, improved version of the game, and about two years ago they were first brought to fruition. First at night after work, then full-time until completion.</p>

<p>Laying out a solid base was was essential in the beginning of the project, specially concept wise. Pre-production, if you will. Surviving on top of a platform was the root idea, but was there latitude for perpendicular ideas? Multiplayer mode? Versus or Cooperative? Online or local only? Powerups? Which player movements would be available, only jump? Progressive difficulty (the player does not choose the difficulty level) or bucket based difficulty (e.g. easy, normal, hard)? What will be the direction of visual style, audio, and code architecture? How many levels? What is the player’s motivation play these levels?</p>

<h2 id="root-ideas-and-game-modes">Root ideas and game modes</h2>

<p>Less is more when it comes to the game’s root ideas, and surviving on top of a platform seemed like a good premise. This idea was dissected and questioned thoroughly during the initial phase of development, in order to make sure it was well sustained. Other ideas such as going as high as possible or objective based levels were considered, but set aside in the initial stage of the project. They were not completely scrapped, such that both survival/time based and objective based levels would later cohabitate throughout the game’s development.</p>

<p>Assuming survival as the most important root idea, a question came up: survive what? It could be either you against the world, or you against other player(s), or a combination of both. The latter two options would prompt the need of a multiplayer versus mode and/or player AI, and the former one would naturally accommodate a single player and/or cooperation mode.</p>

<p>Allowing for a local multiplayer mode would be something desirable, since I had a personal bias towards games that physically join a group of friends in the same place. Also, online multiplayer was set aside due to the added entropy and complexity associated with it. </p>

<p>As for cooperative (co-op) games, there is usually an increased inclusion factor due to the wealth of opportunities to strike a balance between experienced and less experienced players, translating to less chances of frustration caused by huge proficiency gaps.</p>

<p>Given the above points, solo and local co-op modes were chosen to be included in the game.</p>

<h2 id="difficulty">Difficulty</h2>

<p>Again, having in mind that less is more, a progressive difficulty mode (where the player does not choose the difficulty level) was strived for, instead of a bucket based approach (e.g. easy, normal, hard). Choosing a difficulty level when starting a simple game like this one, forces the player to commit to a given difficulty level at start, and accepting that decision. The responsibility is on the player. Often happens to me that when I choose a difficulty level like “easy” or “normal”, I keep wondering throughout the game if I am really good at that game, or am I just being thrown softballs. Progressive difficulty resolves that problem immediately, but it falls on the developer shoulders to craft the game in such a way that all difficulty ranges are covered just right.</p>

<p>Progression would be 2-fold: each level would start in an accessible way, but would gradually get more difficult as time passes. On the other hand, each level would be more challenging than the last one during the campaign. </p>

<h2 id="control-and-movement">Control and movement</h2>

<p>There was the need to have offensive manuevers against hazards, and to provide mid-air control to the player. Two new movements were added to the already existing jump and double jump movements: dash and stomp. Dash provided mid-air control and allowed the player to interact with the world’s objects and attack hazards. Stomp, inspired by <a href="https://en.wikipedia.org/wiki/Super_Mario_64">Super Mario 64</a>, allowed for button interaction, and mid-air stop and plunge for precise vertical landings.</p>

<p>With this set of movements, the player was able to gain full control over the ball. Easy to learn, hard to master. Additionally, it suppressed the need for aerial control via directional controls, which would be less physically coherent and could quench some of the fun of gaining control proficiency.  </p>

<h2 id="powerups">Powerups</h2>

<p>An assumption was made that powerups would be distributed randomly, either by items scattered throughout the level or by other means. Powerups were not used because of these four reasons: </p>

<ul>
  <li><strong>Added game design complexity</strong> - every new powerup would increase the realm of possible scenarios during a game session, which would increase the difficulty of striking good game balance.</li>
  <li><strong>Simplicity</strong> - simplicity was held at high value during development, and powerups would probably require the usage of a new button to trigger the power, more UI elements and new concepts for the player to grasp.</li>
  <li><strong>Skill based gameplay</strong> - the game aimed to be purely skill based. Since the randomized nature of hazards already abducted some of this intent, having powerups attributed by chance would further decrease the player’s perception that the level was completed by his/her own merit.</li>
  <li><strong>They could be used as crutches</strong> - powerups are outstanding when <a href="https://en.wikipedia.org/wiki/Rollcage_(video_game)">used</a> <a href="https://en.wikipedia.org/wiki/Mario_Kart_8">properly</a>, but can also be abused to cover up design flaws. Having a no-powerup limitation forced me think in alternative ways to make the game more interesting, a challenge I was drawn to. </li>
</ul>

<h2 id="visual-aesthetics">Visual Aesthetics</h2>

<p>Defining the game’s visual appearance early on was quite important, since it would influence the reasoning behind 3D models, textures, UI, colors, mood and even audio. The two most influential references were <a href="https://material.io/archive/guidelines/">Material Design (first version)</a> and <a href="https://store.steampowered.com/app/540840/Lara_Croft_GO/">Lara Croft GO</a>.</p>

<p>Coming from an Android development background, I was exposed to Material Design, which is a visual language that synthesizes the classic principles of good design. Its principles go beyond mobile UI development, and span to a swath of different form factors and use cases. I personally found it beautiful and applyed its principles throughout the game’s aesthetics, both in UI, but also in the overall style of the game. This influenced the game to have a simple, clean and coherent style.</p>

<p>Lara Croft GO is a beautiful game. The charming colors and elegant (yet simple) style made an impression on me. While building up the game’s aesthetics, I consulted several screenshots of Lara Croft GO and attempted to transpose its most pleasant design elements into Survival Ball.</p>

<p><img class="center" src="http://lopespm.github.io/files/sb_making_game/lara_croft_go_0.jpg" /></p>

<h2 id="codebase">Codebase</h2>

<p>The prototype was a literal extension of the initial <a href="#prototype">wire chain and ball physics experiment</a>, and features were added at the same time I learned Unity’s ropes. This, added to rapid prototyping and little to no code architecture planning, resulted in a codebase that was entangled, difficult to follow and hard to extend.</p>

<p>The new version was built from scratch and several arquitectures were explored in the beginning of the project, until one was found to fit the game best. The chosen approach followed these principles:</p>

<ul>
  <li>Each level had a single entry point, the game controller, which orquestrated the different actors.</li>
  <li>The game controller contained all top level parameters. Sub-parametrization was facilitated via <a href="https://docs.unity3d.com/Manual/class-ScriptableObject.html">Scriptable Objects</a>. For example, the PlayerController had PlayerParameters (which was a scriptable object) exposed on the game controller. The PlayerParameters could in turn have sub-parameters, if needed.</li>
  <li>Dynamic objects, such as the player, hazards and interactable level parts should be created programmatically whenever possible.</li>
  <li>The initial scene stage should only have static or decorative objects.</li>
  <li>Object behavior should be managed by its specific controller. Game object components should be presentational and have as little behaviour as possible. For example, a rocket thruster could have a component which changed the color and size of smoke and fire, but it would be the thruster’s controller responsibility to drive the input value for this component.</li>
  <li>Orquestration of events (from dynamic objects, user interaction and other data streams) was handled using a reactive approach, facilitated by <a href="https://github.com/neuecc/UniRx">UniRx</a></li>
</ul>

<h2 id="levels">Levels</h2>

<p>The level creation process went somewhere along these lines: I would go for a long walk without any audiobooks or entertainment, and allow my mind to wander around. If an idea came up, I would write it down in a <a href="https://keep.google.com/">Google Keep</a> note. At a later time, in a quiet place, I would dissect these ideas on paper to materialize its pros and cons, while further developing the ideas that seemed most promising. </p>

<h3 id="first-sandbox">First Sandbox</h3>

<p>Before building any of the levels, a scrappy sandbox level was built to work out the broader strokes of the player’s movement. In this sandbox, movement, jump, stomp and dash (immediate single charge at the time) were first developed and loosely tweaked.</p>

<div>
    <div class="youtube-player" data-id="7Fi5rzgwcVo"></div>
</div>
<p class="media-caption media-caption-one" />

<h3 id="rocketx">RocketX</h3>

<p>SpaceX was landing their first rockets on drone ships when the first idea of the bunch was developed. It revolved around the concept of a four thruster rocket platform. Each of these thrusters could be activated or deactivated by their respective button. An additional center button turned all the thrusters on or off. The platform had a finite amount of fuel that could be replineshed by shoving fuel crates, hazards or players in the center fuel intake.</p>

<p>This level would set the tone for the upcoming ones, so it was important to pin down the core gameplay, level structure and hazard dynamics before moving on to the next level. Just like changing a cosmological parameter would drastically change everything in the universe (like the possiblity of life), the game’s basilar parameters were set as tightly as possibile early on, because even a small change could break other already built levels. For example, a small change in how fast the player is, how high can it jump or how gravity affects it, could translate into several hours of extra work to re-adjust every level accordingly, or even render them obsolete.</p>

<p>For these reasons, it took around 354 hours to develop the bulk of RocketX, about one fifth of the project’s timespan.      </p>

<center>
  <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
      <div class="youtube-player video-frame-two" data-id="pNcz-92Qva0"></div></div>
    <p class="media-caption media-caption-two">Initial RocketX prototype, to access the <br class="video-br" />overall feel of the level</p>
  </div>
  <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
       <div class="youtube-player video-frame-two" data-id="QWNof5LEvAE"></div></div>
    <p class="media-caption media-caption-two">Textures, props, backdrop and hazards added, <br class="video-br" /> along with several adjustments to player gameplay</p>
  </div>
  </center>

<div>
    <div class="youtube-player" data-id="qRJ4ETvlLy8"></div>
</div>
<p class="media-caption media-caption-one">Four players playing the final version</p>

<h3 id="garfunkel">Garfunkel</h3>

<p>Original concept for the second level was a variation of the <a href="https://en.wikipedia.org/wiki/Simon_(game)">Simon</a> game, hence <a href="https://en.wikipedia.org/wiki/Simon_%26_Garfunkel">Garfunkel</a>. A sequence of lights would be played out first, and the player(s) would then have to mimic the same sequence afterwards by orderly hitting the respective target pieces, at the cost of the piece falling off if it were to be hit outside the sequence. There would exist as many platforms as the number of players.</p>

<p>A quick level layout was built. Soon after the first plays and it became apparent that the concept was flawed. Mimicking a sequence would mean that during the first phase, while the model sequence is played out, the player would be left to do nothing movementwise. Moreover, after the second or third prompt of the sequence, the player would most likely forget which came next, because his/her main focus would be getting the ball movement right and not hit any of the non sequenced tiles. Simon is about memory, Survival Ball is about motor skill.   </p>

<div>
    <div class="youtube-player" data-id="g13i_KKoPAM" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Initial Garfunkel prototype, exploring a variation of the Simon electronic game</p>

<p>Pivoting from this, came the idea of collapsing the two phases into a single phase, no memorization needed. The platform piece prompts would be immediate and coupled with the background music rhythm, which would progressivly get faster as time passed. The player would have limited time to hit these prompts, as the unhit prompted pieces would fall after each music measure.</p>

<p>There was only one platform, since having multiple platforms would elicit a natural exploit. The player could quickly notice that the best strategy would be to focus all energy in a single platform. There was no real gain to be had on saving all platforms. One would suffice, and would be easier for the player to manage.</p>

<p>The background music was programmatically played. The easiest approach was to individually play each audio sample, meaning that the rhythm was dictated by the frequency to which these were played. Long and complex audio samples were not fit for this purpose, since they had their own tempo, and only transforming these samples would correctly match them with the overall music tempo. Simple and short audio samples were used, specially drum samples, which worked great for this use case.</p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="O4v96qUWg5g"></div></div>
   <p class="media-caption media-caption-two">1. First tribal backdrop experiment</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="1Q3wCCTs1jk"></div></div>
   <p class="media-caption media-caption-two">2. Waterfall backdrop, later re-used in Unfair Fair</p>
 </div>
 </center>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="8CNEBxzrX8c"></div></div>
   <p class="media-caption media-caption-two">3. Level exploit,<br class="video-br" /> bypassing piece decay, platform rotations and prompts</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="Vxnas8eIIkY"></div></div>
   <p class="media-caption media-caption-two">4. Simple particle system backdrop,<br class="video-br" />synchronized with the music</p>
 </div>
 </center>

<p>The music engine and platform movement were first tackled, and took a considerable amount of time to build. After a few playthroughs with these basic systems in place, something did not feel quite right. At the time, it seemed related with how the background looked like. Predominantly using drum samples, the overall music style suggested a tribal feel, and seemed important at the time to have a good match between the music and the level’s visual appearance, hence the different backdrop experiments.    </p>

<p>After these four different backdrops, something was still off. Progressively I realized that the real problem was related with the platform’s shape. In a circular platform, the best strategy is to safeguard the inner pieces, since they are easily reachable and have an inferior angular speed compared to the outer ones. Falling prey to <a href="https://en.wikipedia.org/wiki/Sunk_cost#Loss_aversion_and_the_sunk_cost_fallacy">sunk cost fallacy</a>, some attempts were made to save this concept: rotate the inner pieces at a higher speed or to rotate them in different axes; to implement piece decay, which released the piece if the player stayed on top of it for too long. None of them worked, since these obstacles could be easily bypassed, as shown by the above video number 3.</p>

<div>
    <div class="youtube-player" data-id="55rM9iCQBSw" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Final Garfunkel state. The level was scrapped but most of its elements were re-used in Beatmill and Big Giant Head</p>

<p>The last backdrop experiment was a simple water floor, which focused the player’s attention in the platform and had the additional benefit of having a near plane to which the player’s shadow could be casted. The vertical player shadow was a very important reference for the player’s position.</p>

<p>The level would need to be completely redesigned or scrapped altogether, since it was not acceptable in its current form. It ended up being scrapped, but the waterfall backdrop (video 2. above) was later reused by <a href="#unfair-fair">Unfair Fair</a>, and two new levels were forked from Garfunkel’s base elements: <a href="#beatmill">Beatmill</a> and <a href="#big-giant-head">Big Giant Head</a>.</p>

<h3 id="beatmill">Beatmill</h3>

<p>Beatmill addressed one of the major issues in Garfunkel, the easy exploit of the circular platform shape. Solution was to have several loose square pieces which were part of a cross or lengthwise treadmill. Randomly moving each piece along these two directions meant that no piece was permanently in the same area (like the center area) of the platform. Every single piece was important to treasure and save.</p>

<p>The treadmill system was developed from scratch, and Garfunkel’s music engine and water floor backdrop were reused. </p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="QoVlL5SxoKg"></div></div>
   <p class="media-caption media-caption-two">Single piece treadmill logic,<br class="video-br" /> which would generalize in to a treadmill system</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="D9mfA-_3XU4"></div></div>
   <p class="media-caption media-caption-two">Treadmill system<br class="video-br-end" /><br class="video-br-end" /></p>
 </div>
 </center>

<p>After developing the level’s basic game elements and validating their gameplay via play testing, the decay concept was recycled from Garfunkel, with slight modifications. A special black button was placed as one of the platform’s pieces. If pressed, it decayed the adjacent pieces. If a decayed piece was touched, it would fall.</p>

<p>This concept apparently worked well, but when play tested with a group of friends, it was pointed out as being too punishing. When the special black button was pressed, the level’s difficulty skyrocketed, making the already difficult task of reaching prompted pieces even harder. As a result, pressing the black button was faced as a death kneel, urging players to restart the level right after the black button was pressed. After a few rounds of these, collective despair would soon grow due to the overwhelming difficulty.</p>

<p>Solution was to invert the black button’s function. Instead of decaying the adjacent pieces, it would revive them. This time around, the special black button was not avoided in fear, but in greed. It was a precious resource, to be used as a late as possible.  </p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="yq5UPIoTyLo"></div></div>
   <p class="media-caption media-caption-two">Four players playing an early version,<br class="video-br" />with the special black button decaying the adjacent pieces</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="qPaBAVRu8xM"></div></div>
   <p class="media-caption media-caption-two">Four players playing the final version,<br class="video-br" />with the special black button reviving the adjacent pieces</p>
 </div>
 </center>

<h3 id="big-giant-head">Big Giant Head</h3>

<p>Garfunkel’s platform did not work in a survival setting, but could it work in a different setting? In order for it to work, the level’s progress could only advance when the player touched his/her respective prompt. An objective sequence. The level also needed a long term objective for these sequences to link up. Turns out the game campaign was lacking a final boss, and this could be a good opportunity to salvage Garfunkel’s platform (and its respective movement and prompt system) to a new level, the final one. The final boss.</p>

<p>The defined long term objective was to deplete the boss’s life, but the players needed a way to interact with it. The first idea was to drain the boss’s life every time a prompted platform piece was hit, but that bore two problems: interaction was too indirect and there was no opposite force that gave the boss a chance to defend itself. Hazards could be used to fulfil this purpose, but that dynamic was too easy and insipid. The final boss was expected to be more challenging than all its previous levels, and was expected to require the player to use most, if not all, of the previously aquired learnings.</p>

<p>Inspired by classic game bosses, which stack different boss stages, solution was to materialize the boss into an anthropomorphic head, and have the head appear in the center, equipped with a turret, when it was sufficiently small to fit platform’s center gap. The head shrank every time a sequence of prompted platform pieces were hit. The players could directly interact with the center head by hitting it, draining the boss’s life. Difficulty progression was accomplished by decaying a set of platform pieces every time the small center head was defeated.</p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="Zd5Qi1Z-0Ww"></div></div>
   <p class="media-caption media-caption-two">Interactable boss turret prototype<br class="video-br" />added to the Garfunkel platform</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
      <div class="youtube-player video-frame-two" data-id="9cJ6whQak_E"></div></div>
   <p class="media-caption media-caption-two">Three players playing the final version<br class="video-br-end" /><br class="video-br-end" /></p>
 </div>
 </center>

<h3 id="unfair-fair">Unfair Fair</h3>

<p>“Giant washing machine drum” was the initial concept behind this level. Challenge was to transform this germinal idea into a fun level, coherent with the game’s survival motto. Some scattered ideas were drafted on paper, such as having interactable controls to make the drum rotate in a given direction or to stop the rotation entirelly, but none of them were successfuly transformed in to a viable challenge or part of a larger challenge.</p>

<div>
    <div class="youtube-player" data-id="ogk1-BE4Wgo" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Hardcoded drum to validate to overall feel of the level</p>

<p>The most interesting avenue was to reuse the piece decay concept from Beatmill. In Unfair Fair, the decay was caused by a specific hazard, which if not hit by its respective player, exploded and decayed the nearby platform pieces. Once these pieces were touched, they fell. The pieces near the platform spokes were decayed at start, to increase the chances of partial or complete piece group detachment.</p>

<p>The first iteration of this hazard exploded after bouncing a given amount of times, which proved to be unfair for when the hazard bounces close to the ground, giving little opportunity for the player(s) to react. </p>

<p>The next and final iteration had the hazard explode a given time amount after its first bounce, which gave a fair chance for its dismantling. Visual presentation was reviewed to better present how close the hazard was to explode. The time taken to explode would decrease as the level progressed. </p>

<p>Notice the waterfall backdrop below, repurposed from one of Garfunkel’s experiments.     </p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="lYch8E96Sdw"></div></div>
   <p class="media-caption media-caption-two">First version<br class="video-br" />Decay hazards exploding after a given amount of bounces</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="nP65k-Fp9X0"></div></div>
   <p class="media-caption media-caption-two">Four players playing the final version<br class="video-br" />Decay hazards explode after a given amount of time</p>
 </div>
 </center>

<h3 id="venom-rig">Venom Rig</h3>

<p>Upon Garfunkel’s closure, began the development of a concept revolving around an oil rig shaped platform. The platform had four pillars, each destroyed when their respective button was pressed. The player had no motivation to press them, but these could be pressed by stomper hazard developed previously for RocketX. Their only task was to press these buttons.</p>

<p>To add another challenge dimension, a concept from the game’s prototype was added: decay rockets (in the prototype they were represented as rectangular parallelepipeds). Upon landing, these hazards decayed the platform piece below until the piece was completely detached from the platform. Other hazards would be spawned throughout the lifecyle of the level for increased diversity.</p>

<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="8wkLZoBdFGw"></div></div>
   <p class="media-caption media-caption-two">First iteration of the level<br class="video-br" />with a dynamically generated platform</p>
 </div>
 <div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="WHGM0U-mF0s"></div></div>
   <p class="media-caption media-caption-two">Added decay rockets and other hazards,<br class="video-br" />along with model, backdrop and texture work</p>
 </div>
 </center>

<div>
    <div class="youtube-player" data-id="L89rCKa2zU4" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Four players playing the final version</p>

<h3 id="hex-elevator">Hex Elevator</h3>

<p>The initial concept on paper was a 2.5D level in which a fluid (water/lava) would progressively rise as the player(s) made their way up a series of platforms and objects. The rising fluid concept seemed promising, but having a series of fixed 2.5D platforms seemed somewhat bland and left little space for cooperation dynamics.</p>

<p>Solution was to have an infinite amount of procedurally generated platforms, each higher than the previous one. The only way to reach the next platform was via an elevator assigned to a specific player, which first had to be activated by touching all of its respective prompts. The elevators would only rise when all players occupied their respective elevator.</p>

<p><a href="https://www.redblobgames.com/grids/hexagons/">Reb Blob Games’ guides</a> on hex grids were immensly useful when building the procedurally generated honeycomb platforms. These guides couple well explained theory with concise practical implementations, and I highly recommend them to anyone doing hex grid work.</p>

<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="7_Dx6vlX-vo"></div></div>
  <p class="media-caption media-caption-two">First iteration of the level with procedurally generated<br class="video-br" />platforms and their respective prompts and elevators</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="hV-HI7lUlsA"></div></div>
  <p class="media-caption media-caption-two">Added rising water which was<br class="video-br" />replaced by lava in the upcoming iteration</p>
</div>
</center>

<p>Upon the first play test session with friends, it became apparent that small tweaks were required to the behaviour of elevators. Because elevators would only rise when all players were sitting on top of their respective elevator, a problem arised if, near the upper platform, someone abandoned the elevator earlier than others. At that point, all elevators would drop, taking most of the players with them.</p>

<p>Solution was to make each elevator independent when they were close the upper platform, allowing for a smoother team transition between platforms.  </p>

<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="-eQAA99UQrQ"></div></div>
  <p class="media-caption media-caption-two">Four players playing the early version without<br class="video-br" />elevator independence upon reaching the upper platform</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="n5eVn3ieY3c"></div></div>
  <p class="media-caption media-caption-two">Four players playing the final version with<br class="video-br" />elevator independence upon reaching the upper platform</p>
</div>
</center>

<h3 id="tutorial">Tutorial</h3>

<p>Is is generally a good idea to write the introduction of a book, essay or paper last, since you need to have a very solid idea of the shape of the finished product, and exactly what you need to mention up front for everything to hold together. The tutorial level was left for last for the same reasons.</p>

<p>It was the game’s first level, and had the responsability to introduce the game and provide the required tools and knowledge to play it. Specifically, how to jump, double jump, dash and stomp. The overall concept of the level was inspired by an early version of <a href="https://www.youtube.com/watch?v=eQgWTt5usQ4&amp;feature=youtu.be&amp;t=17">Gang Beasts’ tutorial</a>, which was pedagogic andp fun. Having the controls layed out as part of the scenario, instead of using an UI overlay fixture, was something quite interesting as well.</p>

<div>
    <div class="youtube-player" data-id="rCCqNCzG7RE" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">First iteration. Functional, yet bland</p>

<p>The first iteration had some of the required pedagogic elements, but was static and insipid when played, even though it had some interactive elements.</p>

<p>The following idea was to have a series of small platforms sitting by a pond. The instability of these floating platforms imprinted a livelier dynamic and were more physically sound than the initial tutorial iteration.</p>

<center>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="Q1ewmcfZoCM"></div></div>
  <p class="media-caption media-caption-two">Second iteration, featuring <br class="video-br" /> several floating platforms inside a pond</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="7ACfcqM3zMQ"></div></div>
  <p class="media-caption media-caption-two">Refinements to the second iteration<br class="video-br-end" /><br class="video-br-end" /></p>
</div>
</center>

<p>After a play test with friends, it was noticeable that very few people understood how to dash, or how important the combination of jump, dash and stomps were to gain air control. </p>

<p>Solution was to add an animated billboard ilustrating how the dash charge worked, and add two additional sections focused on air control. The following play tests showed that these changes made a significant difference on a player’s comprehension of the game’s basilar movements and techniques. </p>

<div>
    <div class="youtube-player" data-id="bdmKDki32Og" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Final iteration with an animated dash charge billboard and new sections focused on air control</p>

<h3 id="extra-block-towers">Extra: Block Towers</h3>

<p>This level was not used, since no plausible game dynamics were found to fit the overall concept of the game, but the sheer fun and simplicity it provided while moving around high stacks of blocks were worth the special mention. </p>

<div>
    <div class="youtube-player" data-id="_7bUriE71NY" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one" />

<h2 id="menus-and-user-interface">Menus and User Interface</h2>

<p>Most of these interfaces were built in the late stages of development. Following the <a href="#visual-aesthetics">Visual Aesthetics</a> layed out earlier, the first rough screens were sketched out, starting by the home screen. Some experiments were made with the game’s logo and UI elements. UI buttons design for example, referred their colors and shape from a core element of the game, the hexagonal wave counter. </p>

<div>
    <div class="youtube-player" data-id="3_a3mHJhOuQ" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">First iteration of the home screen</p>

<p>At about the same time, the title screen was built. Presented when opening the game and right before the home screen, it was crafted to create an impactful first impression and to be later used for key art, showcasing the most important elements of the game: logo, players, enemies and some levels. Another important aspect was to illustrate that this was a co-op game, so the 4 players were placed in the forefront, aligned in such a way that no one was widely in front of anyone, with about equal highlight to all of them. The screen drew inspiration from other games’ key art, such as the one from <a href="https://en.wikipedia.org/wiki/Super_Mario_Land_2:_6_Golden_Coins">Super Mario Land 2</a>, which presented all game’s key elements in one powerful, stylized image. </p>

<div>
    <div class="youtube-player" data-id="OPnS6aaiGks" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Title screen</p>

<p>The home screen was later refined and the remaining screens were progressively built and polished. These were the options (video, audio and controls), player selection, pause, end game, game selection and stats screens.</p>

<div>
    <div class="youtube-player" data-id="OrAXiD4QQis" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Tour of all menus and graphical interfaces</p>

<h2 id="audio">Audio</h2>

<p>Music and sound effects were built using GarageBand. Many freely available sound libraries such as the <a href="https://sonniss.com/gameaudiogdc2017/">Sonniss GameAudioGDC Bundles</a> were used to mash up different samples into new sound effects.  </p>

<figure class="image"><center><a href="http://lopespm.github.io/files/sb_making_game/main_screen_main_theme2.png"><img src="http://lopespm.github.io/files/sb_making_game/main_screen_main_theme2.png" /></a></center><figcaption class="media-caption"> GarageBand project for the main theme </figcaption></figure>

<p>Audio aimed to be clean and coherent, thus a small subset of samples were used from the immense libraries available and reused whenever possible, trying to settle with a mostly electronic style for music and to use instruments or natural audio effects for SFX.</p>

<h3 id="keys">Keys</h3>

<p>I believe coherency is gained when using a small set of musical keys throughout the game. Survival Ball only used the C major key and its relative minor, A minor. Major keys are genererally recognized as happier and joyful and minor keys as darker and heavier. In the game, each level or screen used one of these keys, according to the desired environment and feel. </p>

<p>One interesting nugget of knowledge I came to learn while developing the audio, was that sound effects also fit within a certain key. The overall audio will sound much better when sound effects and music are built in key accordance.  </p>

<h3 id="eq">EQ</h3>

<p>Another interesting tidbit was to remove the extremely low and high frequencies in the final mix. The effect is threefold: audio fatigue is reduced when editing; makes the overall composition seem cleaner to the listener; leaves more headroom in the audible frequency range, improving the composition’s focus and clarity.  </p>

<figure class="image"><center><a href="http://lopespm.github.io/files/sb_making_game/eq_cutoff.png"><img src="http://lopespm.github.io/files/sb_making_game/eq_cutoff.png" /></a></center><figcaption class="media-caption"> Extremely low and high frequencies cutoff in the master bus&#8217; EQ </figcaption></figure>

<h3 id="organization">Organization</h3>

<p>The game ended up having more than 60 different sound effects and around <a href="https://store.steampowered.com/app/997540">20 musical segments</a>. A considerable amount of time was wasted checking on a given audio segment status, so a simple spreadsheet was used to track the name, category and its state (done, needs revision, missing). This simple sheet improved the audio creation process considerably. </p>

<h2 id="play-tests">Play Tests</h2>

<h3 id="ai-agents">AI agents</h3>

<p>Having the need to perform many iterations and tweaks without constant access to other testers, a simple AI was built to simulate multiplayer dynamics and to have a better grasp of how the level handles these dynamics.</p>

<p>AI agents were specific to each level, and driven by a series of objectives. For example, in Beatmill, the agent’s main objective was to reach its respective colored prompt. If no prompts existed, it would move towards the platform’s center, the optimal place to wait for a prompt. It also had passive behaviours such as avoidance of gaps or special black buttons via jumps. As seen in the video below, the agents are not perfect, but set a good work baseline.</p>

<div>
    <div class="youtube-player" data-id="L82f3-ckaYA" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Beatmill (non published early version): the blue, green and yellow players are controlled by AI</p>

<h3 id="user-tests">User Tests</h3>

<p>Once roughly tweaked, the game was presented (preferably for the first time) to a group of friends during a play test session. These sessions were of uttermost value, since they provided valuable feedback, critics and/or validation to the various game elements.</p>

<p>There were 6 play test sessions in total. Apart from how valuable these were to improve the game, they were also a great opportunity to gather a group of friends and have a good time. The game was modified and tweaked between each session, after digesting the impressions and feedback of the previous one.</p>

<p>The game had some backdoor hooks which allowed for quick macro difficulty tweaks, e.g. how many waves were necessary to unlock a level. These were specially useful in the first play session, since the game was extremelly punishing in its early incarnations.</p>

<p>Some guidelines I tried to follow:</p>

<ul>
  <li>Refrained from playing. I only joined if the backdoor hooks were not enough to lower the difficulty to a point that was enjoyable by the players. I could infrequently join near the session’s end and most of the feedback had been provided.</li>
  <li>I sat behind the players, writing impressions and feedback. Sporadically I asked for more details on a comment or behaviour that I observed. Being an observer, it is also easier to probe for mood fluctuations in the room. For example, when frustration starts to build up due to the extreme difficulty of a level, it is easily noticeable when observing from outside. The same applies for bursts of joy a given level or dynamic might cause.</li>
  <li>Gut reactions were very precious, so I would probe for more details on these reactions and encourage honest reactions and feedback.  </li>
  <li>After the game was finished, I asked the players about their impressions, what they liked, they did not like, the level they enjoyed the most and the level they enjoyed the least. On that note, RocketX, Hex Elevator and Big Giant Head were top favorites. Venom Rig, Unfair Fair and Beatmill were most often referred as the less enjoyable of the bunch.</li>
  <li>Only explained the reasons behind a given design decision after the game was finished and most of the feedback had been given, to diminish the influence of my opinion on the players feedback.</li>
</ul>

<h1 id="launch">Launch</h1>

<p>The game was launched on November 8th, 2018. One month earlier, the <a href="https://store.steampowered.com/app/918690">Steam store page</a>, <a href="https://survivalball.com/">official website</a> and <a href="https://twitter.com/survivalball">twiter account</a> were brought online. During that month, a closed beta ran on Steam, which served mostly for last mile validations, since the bulk of the iterations were made during the in-person play testing phase. At the same time, several dozens of keys were sent to youtubers, with notable regard to the ones specialized in couch co-op games. To the ones contacted by email, I took into attention <a href="https://www.youtube.com/watch?v=L3Qf1aQHqyc">this video by Stephen</a>, where he describes what kind of emails he expects to receive from developers. </p>

<p>After launch, I was advised by fellow developers that the above strategy was not optimal, because it is usually a good idea to build your following by sharing your progress and interacting with the community during development (via social networks or dev blogs for example), potentially increasing the game’s exposure and sales. In a possible new game, it would be interesting to adopt this strategy and take note of how workflow and game design would be influenced.</p>

<h1 id="indie-x">Indie X</h1>

<p>Shortly after completing the first beta build of the game, I submitted my application to <a href="https://indiex.pt/">Indie X</a>, the biggest indie game showcase/contest in Portugal. Fortunately, Survival Ball was accepted as one of the 55 finalists, meaning that it would be showcased at Lisboa Games Week 2018 a week after the game’s launch!</p>

<p>A custom build was specially crafted for the event. To better fit the event’s environment, the build offered an arcade experience through local leaderbords for the players with the highest number of completed waves, and leaderboards for the fastest players to finish Big Giant Head or the Tutorial. A simple controls cheat sheet was added to the pause menu, and the end game screens were changed to allow the players to enter their (group) name to the leaderboard. All levels were unlocked in this build, avoiding the need to finish the campaign to access a specific level.</p>

<div>
    <div class="youtube-player" data-id="m6X0yNZu-PI" data-thumbsize="1"></div>
</div>
<p class="media-caption media-caption-one">Highlights of the custom build for Lisboa Games Week 2018 showcase</p>

<p>The event surpassed all my expectations. It was the first time I saw, in person, swathes of anonymous people playing Survival Ball. It was pleasantly surprising to recurrently see many groups of gamers trying out the game, playing for hours, and even returning back to the booth at a later time. Not much sleep was had during those four days, but it was rewarding to witness such moments.</p>

<center><a href="https://twitter.com/survivalball/status/1064556805630697477"><img align="center" src="http://lopespm.github.io/files/sb_making_game/composite-3-players-photos_705.png" alt="" /></a></center>
<p class="media-caption media-caption-one">Some of the players at Lisboa Games Week. More photos and videos <a href="https://twitter.com/survivalball/status/1064556805630697477">here</a></p>

<p>The showcase was also a great opportunity to connect with other developers and get to know their stories and games. The overall experience was incredible and I am immensly thankful to the organizers for putting it all together 🙏  </p>

<h1 id="additional-notes--appendix">Additional Notes / Appendix</h1>

<h2 id="tools">Tools</h2>

<ul>
  <li><strong><a href="https://unity3d.com/">Unity</a></strong> - game engine in which the game was built
    <ul>
      <li><strong><a href="https://assetstore.unity.com/packages/tools/utilities/rewired-21676">Rewired (Unity plugin)</a></strong> - a godsend for managing input</li>
    </ul>
  </li>
  <li><strong><a href="https://www.jetbrains.com/rider/">Rider</a></strong> - .NET IDE used to develop and manage the game’s code</li>
  <li><strong><a href="https://www.blender.org/">Blender</a></strong> - used to build all the 3D models and to map their respective textures. Blender is one of the most powerful FOSS tools I have ever used, and puts up a good fight against proprietary packages such as 3ds Max</li>
  <li><strong><a href="https://inkscape.org/">Inkscape</a></strong> - all textures were crafted in Inkscape. Almost all textures were <a href="https://vector-conversions.com/vectorizing/raster_vs_vector.html">vector based</a>, and later rasterized for Unity in PNG format</li>
  <li><strong><a href="https://www.apple.com/lae/mac/garageband/">GarageBand</a></strong> - music and sound effects composition</li>
  <li><strong><a href="https://www.tyme-app.com/">Tyme</a></strong> - project time tracking</li>
  <li><strong><a href="https://git-scm.com/">Git</a></strong> - source control. <a href="https://gitlab.com/">GitLab</a> private repositories were used to host the game’s project and website</li>
</ul>

<h2 id="time-log">Time log</h2>

<p>Time was tracked during the entire span of the project, in various categories.</p>

<h3 id="time-for-all-categories-per-milestone">Time for all categories, per milestone:</h3>

<ul>
  <li><strong>Until first complete alpha build</strong> - 1334 hours </li>
  <li><strong>Until Steam release</strong> - 1642 hours</li>
  <li><strong>Total, until now</strong> - 1716 hours </li>
</ul>

<h3 id="time-until-now-per-category">Time until now, per category</h3>

<ul>
  <li><strong>Coding, visual work, tooling, research/study</strong> - 1417 hours</li>
  <li><strong>Audio (music, sfx)</strong> - 92 hours</li>
  <li><strong>Final builds play tests (does not include play tests with other people or tests during development)</strong> - 32 hours</li>
  <li><strong>Website</strong> - 22h</li>
  <li><strong>Marketing, screenshots, videos, social media</strong> - 123 hours</li>
  <li><strong>User support</strong> - 8 hours</li>
  <li><strong>Administrative, burocracies</strong> - 22h</li>
</ul>

<h2 id="final-notes">Final Notes</h2>

<ul>
  <li>The game was developed in macOS. Windows was mostly used to work out the particularities of its respective build, such as build post-processing work.</li>
  <li><a href="https://twitter.com/chetfaliszek/status/1085957214781595648">In case you need one more reason</a> on why to use version control for your game project: time travel. The game was under version control since day one, and its git repository has around 1.3K commits at the time of this writing. Going back in time was as simple as checking out an older commit. Almost all videos presented throughout this article were made possible because of this. Checkout, open project, record, repeat. Not only that, but it was fairly easy to see how work evolved over time, which helped immensely when writing this article.</li>
  <li>Most challenging aspect was to make sure everything was coherent. Once root concepts, design and objectives were aligned and the early level prototypes felt good when played, then filling out the gaps (refining gameplay, models, textures, audio, UI) was somewhat straightforward.</li>
  <li>The game’s development could be divided in three phases:
    <ul>
      <li><strong>First phase (until first alpha build)</strong> - bulk of the development work was done during this phase. Very intense in terms of making sure everything was coherent, sane and enjoyable to play. There were large temporally contiguous blocks of work on each level.</li>
      <li><strong>Second phase (first alpha to public launch)</strong> - development was scattered, moving from place to place, iterating where needed and finishing up peripheral features such as Steam integration. </li>
      <li><strong>Third phase (from launch onwards)</strong> - maintenance work, user support, optimizations and bug fixes.</li>
    </ul>
  </li>
  <li>I had an amazing experience building this game and am very happy with the final result. Not only did it materialize one of my dreams, but gave me the opportunity to explore and deepen areas outside my professional expertise, such as 3D modeling/texturing, audio production and game design. If you are a game developer or are thinking on becoming one, I hope this testimony was useful for your journey and hopefully brought more clarity into the process 🙌</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Unity: Dynamic Multi Target Camera]]></title>
    <link href="http://lopespm.github.io/libraries/games/2018/12/27/camera-multi-target.html"/>
    <updated>2018-12-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/libraries/games/2018/12/27/camera-multi-target</id>
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=HeJrQkkIfOI" style="white-space: nowrap;">
	<video autoplay="" loop="" muted="" playsinline="" style="width: auto; border: 0px; box-shadow: rgba(0, 0, 0, 0.02) 0px 1px 4px;">
		<source src="http://lopespm.github.io/files/camera_multi_target/demo.webm" type="video/webm" />
		<source src="http://lopespm.github.io/files/camera_multi_target/demo.mp4" type="video/mp4" />
		<!--[if lt IE 9]><img src="http://lopespm.github.io/files/camera_multi_target/demo.gif"><![endif]-->
	</video> 
</a></p>

<p>Mostly invisible, yet essential, camera work is key to any game with dynamic cameras. This article dissects a concise Unity <a href="https://github.com/lopespm/unity-camera-multi-target">open source</a> library which dynamically keeps a set of objects (e.g. players and important objects) in view, a common problem for a wide range of games.</p>

<p>The library was developed for, and used by my first Steam game, <a href="https://survivalball.com/">Survival Ball</a>. The game has an heavy shared screen local co-op component, which requires the camera to dynamically keep many key elements in view.</p>

<!--more-->

<p>There are good camera resources for Unity, but I found them to either do too much or too little for this specific problem, so I thought this could be a good opportunity to learn a bit more about dynamic camera movement and to share that knowledge and <a href="#where-to-get-it">code</a> with the community.</p>

<h1 id="overview">Overview</h1>

<p>The library is fed with the desired camera rotation (pitch, yaw and roll), the target objects that will be tracked and the camera that will be transformed. </p>

<p>The library’s sole responsibility is to calculate a camera position in which all targets lie inside its view. To achieve this, all target objects are projected onto a slice (plane) of the <a href="https://docs.unity3d.com/Manual/UnderstandingFrustum.html">camera’s view frustrum</a>. The projections located inside the view frustrum will be visible and the others will not. The idea is to trace back a new camera position from the outermost target projections, since this way we are guaranteed to include all projections inside the view.</p>

<p><img class="center" src="http://lopespm.github.io/files/camera_multi_target/side_view_screen_basis_with_captions.png" /></p>

<p>In order to make the bulk of the operations easier to compute, the process starts by multiplying the camera’s <a href="https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Using_quaternion_as_rotations">inverse rotation</a> with each of the targets positions, which will place them as they would if the camera’s axis would be aligned with the world’s axis (identity rotation). Once the camera position is calculated in this transformed space, the camera rotation is multiplied by this position, resulting in the final desired camera position. The actual camera position is then progressively interpolated towards this desired position, to smooth out the camera movement.</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_1.png"><img src="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_1.png" /></a></center><figcaption class="media-caption"> Original targets positions </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_2_with_captions.png"><img src="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_2_with_captions.png" /></a></center><figcaption class="media-caption"> Multiply the camera&#8217;s inverse rotation with each of the targets positions </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_3.png"><img src="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_3.png" /></a></center><figcaption class="media-caption"> Calculate the camera position in this transformed space </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_4_with_captions.png"><img src="http://lopespm.github.io/files/camera_multi_target/transformed_space/transformed_space_4_with_captions.png" /></a></center><figcaption class="media-caption"> Multiply the camera&#8217;s rotation with the calculated position in the previous transformed space, which will reveal the final camera position in the original space</figcaption></figure>

<h1 id="implementation">Implementation</h1>

<p>Most of the operations are performed in the transformed space where the camera’s axis would be aligned with the world’s axis (identity rotation). After the targets are rotated into the camera’s identity rotation space by multiplying the camera’s inverse rotation with each of the targets positions, the first task is to calculate their projections.</p>

<p>Please note that in all the figures below (with the exception of the <a href="#horizontal-projections"><em>horizontal field of view angle</em></a> section), the camera is present for reference only, as its final desired position will only be uncovered in the final step.</p>

<h2 id="targets-projections">Targets projections</h2>

<p>For each target, four projections are cast to a plane parallel to the view plane, sliced from the camera’s view frustrum. The line described from the target object to its respective projection is parallel to the camera’s view frustrum edges. Relative to the camera, two of these projections run horizontally, and the other two vertically.</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_1.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_1.png" /></a></center><figcaption class="media-caption"> Perspective view of the targets projections </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_2.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_2.png" /></a></center><figcaption class="media-caption"> Side view of the targets projections </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_3.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_intro/projections_3.png" /></a></center><figcaption class="media-caption"> Top view of the targets projections </figcaption></figure>

<p>If any of the target’s projections are outside the camera’s view frustrum (or its sliced plane), then the target object will not be visible. If they are inside, the target object will be visible. This means that the four outermost projections from all targets will define the limit of where the view frustrum must be in order to have all objects in view or partially in view. Adding some padding to these outermost projections (i.e. moving these projections away from the center of the view frustrum plane slice), will result in additional padding between the target object and the camera’s view borders.</p>

<h3 id="vertical-projections">Vertical projections</h3>

<p>For all vertical projections positions, we are interested in finding their Y component. In the figure below, notice the right triangle with one vertex on the target object and another one on the projection. If we discover the length of the side running parallel the projection plane, that value can be added to the Y component of the target’s position, resulting in the Y component for the upper projection.</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_calc/projections_calc_with_captions_vertical.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_calc/projections_calc_with_captions_vertical.png" /></a></center><figcaption class="media-caption"> Side view of the elements needed to calculate the vertical projections </figcaption></figure>

<p><script type="math/tex">\theta</script> is equal to half the camera’s vertical field of view angle <script type="math/tex">\theta'</script> (<script type="math/tex">\theta = \frac{\theta'}{2}</script>). The vertical field of view angle <script type="math/tex">\theta'</script> is provided by the camera’s <a href="https://docs.unity3d.com/ScriptReference/Camera-fieldOfView.html"><code>fieldOfView</code></a> in degrees, which needs to be converted to radians for our purposes (<script type="math/tex">\theta' = \theta'_{degrees} \times \frac{\pi}{180º}</script>).</p>

<p>The triangle’s adjacent edge length (relative to <script type="math/tex">\theta</script>) is known, thus we can find the length of the opposite side of the triangle by using <a href="https://www.khanacademy.org/math/geometry/hs-geo-trig/hs-geo-trig-ratios-intro/a/finding-trig-ratios-in-right-triangles">trigonometric ratios</a>.</p>

<script type="math/tex; mode=display">opposite = tan(\theta) \times adjacent</script>

<p>With this, the upper projection’s Y/Z components can be fully calculated. The bottom projection has the same Z component as the upper one, and its Y component is equal to the target’s Y component minus the calculated opposite triangle edge length.</p>

<h3 id="horizontal-projections">Horizontal projections</h3>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_calc/projections_calc_with_captions_horizontal.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_calc/projections_calc_with_captions_horizontal.png" /></a></center><figcaption class="media-caption"> Top view of the elements needed to calculate the horizontal projections </figcaption></figure>

<p>The horizontal projections follow a set of similar set of calculations, difference being that we are now interested in finding the X component (instead of Y), and the horizontal field of view angle is used instead of the vertical one. The horizontal field of view angle <script type="math/tex">\gamma'</script> and its half value <script type="math/tex">\gamma</script> (<script type="math/tex">\gamma = \gamma' \times 2</script>) need some further steps to be computed, which will be detailed in the following section.</p>

<h4 id="horizontal-field-of-view-angle">Horizontal field of view angle</h4>

<p>Consider the following figure, in which <script type="math/tex">\gamma</script> represents half the horizontal field of view angle, <script type="math/tex">\theta</script> represents half the vertical field of view angle, <script type="math/tex">w</script> the viewport width and <script type="math/tex">h</script> the viewport height:</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/horizontal_fov.png"><img src="http://lopespm.github.io/files/camera_multi_target/horizontal_fov.png" /></a></center><figcaption class="media-caption"> Elements for the calculation of the horizontal field of view angle </figcaption></figure>

<p>Using trigonometric ratios, these two equations can be devised:</p>

<script type="math/tex; mode=display">
\begin{cases}
tan(\gamma) = \frac{w / 2}{adj} \\ 
adj = \frac{h / 2}{tan(\theta)}
\end{cases} 
</script>

<p>Replacing <script type="math/tex">adj</script> in the first equation with the definition of the second one:</p>

<script type="math/tex; mode=display">
tan(\gamma) = \frac{w / 2}{\frac{h / 2}{tan(\theta)}} 
\Leftrightarrow \\
tan(\gamma) = \frac{w / 2}{h / 2} \times tan(\theta)
\Leftrightarrow \\
tan(\gamma) = \frac{w}{h} \times tan(\theta)
\Leftrightarrow \\
\gamma = arctan(\frac{w}{h} \times tan(\theta))
</script>

<p>Unity’s camera has an <a href="https://docs.unity3d.com/ScriptReference/Camera-aspect.html"><code>aspect</code></a> attribute (view canvas width divided by height, i.e. <script type="math/tex">aspect\ ratio = \frac{w}{h}</script>), with which we can finalize our equation and discover the horizontal field of view angle half value.</p>

<script type="math/tex; mode=display">
\gamma = arctan(aspect\ ratio \times tan(\theta))
</script>

<h3 id="outermost-projections">Outermost projections</h3>

<p>Having all target projections calculated, the four outermost ones are picked:</p>

<ul>
  <li><script type="math/tex">p_{hMax}</script> is the projection with the highest X component</li>
  <li><script type="math/tex">p_{hMin}</script> is the projection with the lowest X component</li>
  <li><script type="math/tex">p_{vMax}</script> is the projection with the highest Y component</li>
  <li><script type="math/tex">p_{vMin}</script> is the projection with the lowest Y component</li>
</ul>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_outermost/projections_outermost.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_outermost/projections_outermost.png" /></a></center><figcaption class="media-caption"> All outermost projections </figcaption></figure>

<h2 id="calculating-the-camera-position">Calculating the camera position</h2>

<h3 id="in-the-transformed-space">In the transformed space</h3>

<p>The X and Y components of the desired camera position in the transformed space are the midpoints of their respective outermost projections, this is, the midpoint between <script type="math/tex">p_{hMax}</script> and <script type="math/tex">p_{hMin}</script> is the camera’s X position, and the midpoint between <script type="math/tex">p_{vMax}</script> and <script type="math/tex">p_{vMin}</script> is the camera’s Y position.</p>

<p>The Z component of the camera position in the transformed space is calculated by backtracking a view frustrum from the the outermost projections to the camera Z component candidate. The furthest Z component from the projection plane will be the chosen, in order for the final camera position to contain all targets within its view.</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_distance/projections_distance_with_captions_vertical.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_distance/projections_distance_with_captions_vertical.png" /></a></center><figcaption class="media-caption"> Elements for adjacent<sub>v</sub> calculation </figcaption></figure>

<figure class="image"><center><a href="http://lopespm.github.io/files/camera_multi_target/projections_distance/projections_distance_with_captions_horizontal.png"><img src="http://lopespm.github.io/files/camera_multi_target/projections_distance/projections_distance_with_captions_horizontal.png" /></a></center><figcaption class="media-caption"> Elements for adjacent<sub>h</sub> calculation </figcaption></figure>

<p>Once again, trigonometric ratios will be used to calculate these Z position candidates.</p>

<script type="math/tex; mode=display">
adjacent_{v} = \frac{opposite_{v}}{tan(\theta)}
\\
adjacent_{h} = \frac{opposite_{h}}{tan(\gamma)}
</script>

<p>The highest value between <script type="math/tex">adjacent_{v}</script> and <script type="math/tex">adjacent_{h}</script> will be picked for the camera’s Z position component in the transformed space.</p>

<h3 id="final-camera-position-in-the-original-space">Final camera position in the original space</h3>

<p>With the camera position calculated in the transformed space, we can now multiply the desired camera rotation with this position, which will provide us with the final desired camera position. The actual camera position is then progressively interpolated towards this desired position, to smooth out the camera movement.</p>

<h1 id="where-to-get-it">Where to get it</h1>

<p>The library is available on GitHub and the Unity Asset Store. An example scene of the library’s usage is included. Feedback is most welcome and I hope this can be useful!</p>

<ul>
  <li><a href="https://github.com/lopespm/unity-camera-multi-target">GitHub Repository</a></li>
  <li><a href="https://assetstore.unity.com/packages/tools/camera/camera-multi-target-dynamic-135922">Unity Asset Store Package</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ArXiv Papers: React Native Android App]]></title>
    <link href="http://lopespm.github.io/apps/2018/03/12/arxiv-papers.html"/>
    <updated>2018-03-12T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/apps/2018/03/12/arxiv-papers</id>
    <content type="html"><![CDATA[<center>
          <picture>
             <source type="image/webp" srcset="/files/arxiv-papers/arxiv_layout_implementation.webp" />
             <source type="image/png" srcset="/files/arxiv-papers/arxiv_layout_implementation.png" />
             <img src="http://lopespm.github.io/files/arxiv-papers/arxiv_layout_implementation.png" />
           </picture>
         </center>

<p>ArXiv Papers is an <a href="https://github.com/lopespm/arxiv-papers-mobile">open source</a> mobile application to search, download and save arXiv papers. It was developed using a react native / redux framework and is currently available for smartphone and tablet devices. You can get it on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.arxiv">Google Play</a>.</p>

<!--more-->

<h2 id="motivation">Motivation</h2>

<p>Often I would find myself downloading the same arXiv paper on my phone because it would get buried in the “downloads” folder under the default arXiv id file name, so instead of sifting through a myriad of already downloaded PDFs I would just revisit the paper’s web page and redownload it.</p>

<p>Organizing each paper neatly into its proper folder with a more discernable filename is quite laborious on a mobile device and the apps I tried out either did too much or too little. I just wanted to quickly search an arXiv article and quickly refer to the ones I downloaded.</p>

<p>This seemed not only a good opportunity to leverage on <a href="https://arxiv.org/help/api/index">arXiv’s public API</a> and develop this idea into an application, but also to expand from Android native development and dive into cross platform app development, specifically Android/iOS.</p>

<h1 id="bootstraping">Bootstraping</h1>

<p>Cross platform mobile app development frameworks come far and wide, so the plan was to go through them, choose one framework, learn its foundations, and attempt to solidify my understanding of its various concepts, advantages, pains and quirks by using it in this new application.</p>

<p>I went shopping for a suitable framework and eventually settled with react native/redux. It seemed like a good fit for the project and a valuable add to my programming tool belt. React and redux had been around for a while and were quite mature and capable at that point. The react native library did not reach a major version yet, but appeared to be quite solid and had a rich ecosystem, vibrant community and various <a href="https://code.facebook.com/posts/1189117404435352/react-native-for-android-how-we-built-the-first-cross-platform-react-native-app/">successful</a> <a href="https://developers.soundcloud.com/blog/react-native-at-soundcloud">use</a> <a href="https://www.youtube.com/watch?v=8qCociUB6aQ">cases</a> in demanding production apps.</p>

<p>I first built my foundations through courses (<a href="https://egghead.io/instructors/dan-abramov">Dan Abramov’s</a> react and redux courses are wonderful), examples, lectures, tutorials, documentation and articles. I was pleasantly impressed with the <a href="https://github.com/react-community/create-react-native-app">CRNA</a> project’s workflow and how it enables you to develop an entire application without touching a single native implementation piece. However, Android and iOS are two distinct platforms, with different design and UX paradigms, meaning that even unejected applications should be specifically tailored for each platform. With this in mind, and knowing that I would need native integrations to download papers for example, I decided to narrow the scope to Android only, but a potential future iOS version should not be a huge pain.</p>

<h1 id="design">Design</h1>

<figure class="image"><center><a href="http://lopespm.github.io/files/arxiv-papers/arxiv_layout_mockup.png"><img src="http://lopespm.github.io/files/arxiv-papers/arxiv_layout_mockup.png" /></a></center><figcaption class="media-caption"> Final Android smartphone app design, made in Sketch </figcaption></figure>

<p>After a few iterations, the above design seemed like a good compromise between functionality, breadth and ease of use. Most importantly, it catered to the app’s motif: search, download and save arXiv articles. </p>

<p>Many, if not all of the apps I explored, allowed the user to browse through arXiv’s categories. For this application however, it did not seem to be an essential feature. Browsing categories serves discoverability, but I am assuming that whoever uses this app would at least have a rough idea of what she is looking for. In that case, keyword search can fill that use case. Looking for DeepMind’s <a href="https://arxiv.org/pdf/1312.5602.pdf">Playing Atari with Deep Reinforcement Learning</a>? Write “deep atari” and the paper you are looking for appears on the first page.</p>

<p>Bookmark-wise, the assumption is that if you download a paper, then most probably it is something your are interested in, thus the higher the probability for its recall. How about merging the two concepts? You are on the article’s details screen, you download it, read it if you wish, and when you arrive back at the home screen, it will be available for immediate recall.</p>

<h1 id="implementation">Implementation</h1>

<p>React by itself is a library which could be seen as the view in model-view-controller, whereas react native builds upon it by providing native component rendering, and redux is used for state management. Moreover, I chose to use <a href="https://redux-observable.js.org/">Redux Observable</a> to handle redux action side effects, <a href="https://nativebase.io/">Native Base</a> for most of the UI components, Flow for type safety, <a href="https://github.com/wix/detox">Detox</a> for end to end tests and Jest for unit and integration tests.</p>

<h2 id="action-side-effects">Action Side Effects</h2>

<p>Redux Observable does not seem to be a popular choice for action side effects, when compared with redux sagas for example, but I think it offers one of the best solutions in this domain. At its core it uses rxJS, a library for reactive programming, which is a natural fit for the asynchronous nature of redux action side effects. This approach is inherently declarative, provides powerful composition (complex async tasks godsend) and makes it easy to cancel async tasks, a common edge case which is often overlooked and can make the difference between an OK application and a very solid application. </p>

<h2 id="testing">Testing</h2>

<p>Application logic was contained inside the action’s handlers, selectors and reducers, so I chose not to test the react components because the unit and integration tests could extract an higher value when applied to the before mentioned triad. Adding to this, ESLint was used for code quality control and catching basic errors, and much of the application was typed using Flow. Redux Observable epics were not type checked, since at the moment there are <a href="https://github.com/redux-observable/redux-observable/issues/258">no meaningful type definitions available</a>.</p>

<p>Integration tests were done for the different application modules (articles, papers, etc), using actions as inputs and selectors as outputs; action side effects were tested separately in their respective unit tests. More unit tests could be made to each of the redux units and further satisfy the traditional test pyramid, but the before mentioned static analysis served as a good foundation and most of the module’s inner code paths were covered by the aforementioned tests, so I opted for a more a pragmatic approach, one which could yield a higher value with minimum developer effort.</p>

<p>To encompass the whole application, end to end tests were built with the aid of Detox. Detox’s android support is still nearing completion, but appears to be a very promising solution. One of its most distinctive features, when compared with popular black box testing frameworks like Appium, is its direct integration with the native layers via EarlGrey for iOS and Espresso for Android, translating to more stable, faster and less flaky tests. Using it was straightforward, the API is well designed and it did not stand in the way while writing the tests. Overall, a good experience for this small application.</p>

<h2 id="folder-structure">Folder Structure</h2>

<p>Regarding folder structure, I opted for a feature based tree which branched into framework concepts (reducer, action, selector, react component, redux observable epic). As it typically happens in other domains, using the framework’s concepts as the root of your folder structure works great for small applications, but as complexity grows, root division by features makes the structure much easier to extend and reason about.</p>

<div><div class="CodeRay">
  <div class="code"><pre>├── __e2e__
├── modules
│   ├── articles
│   ├── connection
│   ├── donations
│   ├── navigation
│   ├── papers
│   │   ├── __tests__
│   │   ├── epic
│   │   ├── paper.js
│   │   ├── actions.js
│   │   ├── reducer.js
│   │   └── selector.js
│   ├── permission
│   ├── shared
│   └── snackbar
├── configureStore
├── theme
├── App.js
├── epic.js
├── reducer.js
└── selector.js
</pre></div>
</div>
</div>

<p><br /></p>

<h1 id="get-it-hack-it">Get it, Hack it</h1>

<p>Feel free to hack away <a href="https://github.com/lopespm/arxiv-papers-mobile">the code</a> or download the application on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.arxiv">Google Play</a>, let me know your thoughts about it!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[React Native Offscreen Toolbar]]></title>
    <link href="http://lopespm.github.io/libraries/2018/01/25/react-native-offscreen-toolbar.html"/>
    <updated>2018-01-25T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/libraries/2018/01/25/react-native-offscreen-toolbar</id>
    <content type="html"><![CDATA[<p>In material design, there is a <a href="https://material.io/guidelines/patterns/scrolling-techniques.html#scrolling-techniques-behavior">common scrolling technique</a> in which the toolbar is smoothly tucked away while scrolling down and is made visible again when scrolling up. This behaviour is fairly straightforward to implement when developing a native android app, but for a react native app, the best solution I found was <a href="https://medium.com/appandflow/react-native-collapsible-navbar-e51a049b560a">Janic Duplessis’</a>.</p>

<p>The <a href="https://www.npmjs.com/package/react-native-offscreen-toolbar">library</a> I am releasing today is an encapsulation of this behaviour and its implementation is heavily based on Janic Duplessis’ approach.</p>

<center>
  <div style="overflow: auto; margin-bottom: 16px">
    <div style="float: left;height: 10px; width:8%;"></div>
        <video autoplay="" loop="" muted="" playsinline="" style="width: auto; border: 0px; box-shadow: rgba(0, 0, 0, 0.02) 0px 1px 4px; float: left;height: auto; width:40%;">
        <source src="http://lopespm.github.io/files/rn-offscreen-toolbar/simplelist_demo.webm" type="video/webm" />
        <source src="http://lopespm.github.io/files/rn-offscreen-toolbar/simplelist_demo.mp4" type="video/mp4" />
        <!--[if lt IE 9]><img src="http://lopespm.github.io/files/rn-offscreen-toolbar/simplelist_demo.gif"><![endif]-->
    </video> 
    <div style="float: left;height: 10px; width:4%;"></div>
    <video autoplay="" loop="" muted="" playsinline="" style="width: auto; border: 0px; box-shadow: rgba(0, 0, 0, 0.02) 0px 1px 4px; float: left;height: auto; width:40%;">
        <source src="http://lopespm.github.io/files/rn-offscreen-toolbar/search_demo.webm" type="video/webm" />
        <source src="http://lopespm.github.io/files/rn-offscreen-toolbar/search_demo.mp4" type="video/mp4" />
        <!--[if lt IE 9]><img src="http://lopespm.github.io/files/rn-offscreen-toolbar/search_demo.gif"><![endif]-->
    </video> 
  </div>
  <figcaption class="media-caption">Library usage in both the <a href="https://github.com/lopespm/react-native-offscreen-toolbar/tree/master/example">example</a> bundled with the library and in the search screen of a to be released application</figcaption>
</center>

<!--more-->

<h2 id="how-to-use-it">How to Use it</h2>

<p>Installation:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ $ npm install react-native-offscreen-toolbar --save
</pre></div>
</div>
</div>

<p><br /></p>

<p>Usage (full example <a href="https://github.com/lopespm/react-native-offscreen-toolbar/tree/master/example">here</a>):</p>

<div><div class="CodeRay">
  <div class="code"><pre>import OffscreenToolbar from 'react-native-offscreen-toolbar';
 
export default class YourComponent extends React.Component {
    render() {
        const toolbar = () =&gt; (&lt;ToolbarAndroid title={'Title'} /&gt;);
        const listItem = ({item}) =&gt; &lt;Text&gt;{item.key}&lt;/Text&gt;;
        const scrollable = () =&gt; (&lt;FlatList data={DUMMY_DATA} renderItem={listItem}/&gt;);
        return (
            &lt;View style={styles.container}&gt;
                &lt;OffscreenToolbar
                    toolbar={toolbar}
                    scrollable={scrollable} /&gt;
            &lt;/View&gt;
        );
    }
}
</pre></div>
</div>
</div>

<p><br /></p>

<p>You can pass any component you desire as a <code>toolbar</code>, but typically this will be a toolbar/navbar backed by react native or a UI library of your choice, like <a href="https://nativebase.io/">Native Base</a>.</p>

<p>The scrollable however, will need to be scrollable component such as a <code>ScrollView</code>, <code>ListView</code>, <code>FlatList</code>, etc. The <code>OffscreenToolbar</code> will then create hooks around this component in order to gauge the user’s scrolling behaviour and change toolbar’s animation accordingly.</p>

<p>You can also provide the following properties to the component:</p>

<ul>
  <li><code>scrollableOverlay</code>: for a search screen as the one presented above, this component can be used to present an overlay between the scrollable and the toolbar</li>
  <li><code>toolbarHeight</code>: adjust this property according to your toolbar height. This value is used for the toolbar’s animation</li>
  <li><code>scrollablePaddingTop</code>: since the scrollable is laying behind the toolbar, this property comes in handy to place the scrollable content below the toolbar</li>
</ul>

<h2 id="where-to-get-the-library">Where to get the library</h2>

<ul>
  <li><a href="https://github.com/lopespm/react-native-offscreen-toolbar">Source code on GitHub</a></li>
  <li><a href="https://www.npmjs.com/package/react-native-offscreen-toolbar">NPM distribution</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Deep Reinforcement Learning: Playing a Racing Game]]></title>
    <link href="http://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html"/>
    <updated>2016-10-06T00:00:00+01:00</updated>
    <id>http://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game</id>
    <content type="html"><![CDATA[<center>
 <div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
     <div class="youtube-player video-frame-two" data-id="spzYVhOgKBA"></div></div>
   <p class="media-caption media-caption-two">Agent playing Out Run, session 201609171218_175eps<br />No time limit, no traffic, 2X time lapse</p>
 </div>
  </center>

<p>Above is the built <a href="https://deepmind.com/research/dqn/">deep Q-network (DQN)</a> agent playing <a href="https://en.wikipedia.org/wiki/Out_Run">Out Run</a>, trained for a total of 1.8 million frames on a Amazon Web Services g2.2xlarge (GPU enabled) instance. The agent was built using python and tensorflow. The Out Run game emulator is a modified version of <a href="https://github.com/lopespm/cannonball">Cannonball</a>. All source code for this project is <a href="http://lopespm.github.io/machine_learning/2016/10/06/deep-reinforcement-learning-racing-game.html#source-code">available on GitHub</a>.</p>

<!--more-->

<p>The agent learnt how to play by being rewarded for high speeds and penalized for crashing or going off road. It fetched the game’s screens, car speed, number of off-road wheels and collision state from the emulator and issued actions to it such as pressing the left, right, accelerate or brake virtual button. </p>

<p>Agent trainer implements the deep Q-learning algorithm used by <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Google’s DeepMind Team to play various Atari 2600 games</a>. It uses a reward function and hyperparameters that fit best for Out Run, but could potentially be used to <a href="#plugging-other-problems-and-games">play other games or solve other problems</a>.</p>

<p>There is a wealth of <a href="#further-references">good information</a> about this reinforcement learning algorithm, but I found some topics difficult to grasp or contextualize solely from the information available online. I will attempt to add my humble contribution by tackling these and also provide details about the project’s implementation, results and how it can be used/modified/deployed.</p>

<p>Let’s start by one of its main gears: Q-learning</p>

<p />

<h1 id="concepts">Concepts</h1>

<h2 id="q-learning">Q-learning</h2>

<p>At the heart of deep Q-learning lies Q-learning, a popular and effective <a href="https://www.youtube.com/watch?time_continue=258&amp;v=bFPoHrAoPoQ">model-free</a> algorithm for learning from delayed reinforcement.  </p>

<p>Jacob Schrum has made available a terse and accessible <a href="https://www.youtube.com/playlist?list=PL4uSLeZ-ET3xLlkPVEGw9Bn4Z8Mbp-SQc">explanation</a> which takes around 45 minutes to watch and serves as a great starting point for the paragraphs below. 
Let’s take the canonical reinforcement learning example presented by Jacob (grid world):</p>

<p><img class="center" src="http://lopespm.github.io/files/dqn_outrun/gridworld.png" /></p>

<p>To implement this algorithm, we need to build the Q-function (<a href="https://www.youtube.com/watch?v=XLkW_WGJoyQ">one of the forms</a> of the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bell-Equation</a>) by using the Q-value iteration update:</p>

<p><img class="center" src="http://lopespm.github.io/files/dqn_outrun/q_value_iteration_update.svg" /></p>

<p>In the above grid, there are 9 actionable states, 2 terminal states and 4 possible actions (left, right, up, down), resulting in 36 (9 actionable states x 4 possible actions) Q-values.</p>

<p>This project aims to train an agent to play Out Run via its game screens, so for the sake of argument, let´s consider that each game screen is transformed into a 80x80 greyscale image (each pixel value ranging from 0 to 255), and that each transformed image represents a state. 6400 pixels (80x80) and 256 possible values per pixel translate to 256<sup>6400</sup> possible states. This value alone is a good indicator of how inflated the number of possible Q-values will be.</p>

<p>Multiplying <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py">9 possible actions</a> by 256<sup>6400</sup> possible states results in 256<sup>6400</sup> x 9 possible Q-values. If we use multiple and/or colored images for state representation, then this value will be even higher. Quite unwieldy if we want to store these values in a table or similar structure.</p>

<h2 id="enter-deep-neural-networks">Enter deep neural networks</h2>

<p>Deep neural networks work quite well for inferring the mapping implied by data, giving them the ability to predict an approximated output from an input that they never saw before. No longer do we need to store all state/action pair’s Q-values, we can now model these mappings in a more general, less redundant way. These networks also automatically learn a set of internal features which are useful in complex non-linear mapping domains, such as image processing, releasing us from laborious feature handcrafting tasks.</p>

<p>This is perfect. We can now use a deep neural network to approximate the Q-function: the network would accept a state/action combination as input and would output the corresponding Q-value. Training-wise, we would need the network’s Q-value output for a given state/action combo (obtained through a forward pass) and the target Q-value, which is calculated through the expression: <script type="math/tex">r_{t+1} + \gamma \underset{a}\max Q(s_{t+1}, a)</script>. With these two values, we can perform a gradient step on the squared difference between the target Q-value and the network’s output.</p>

<p>This is perfect, but there is still room for improvement. Imagine we have 5 possible actions for any given state: when calculating the target Q-value, to get the optimal future value estimate (consequent state’s maximum Q-value) we need to ask (forward pass) our neural network for a Q-value 5 times per learning step.</p>

<p>Another approach (used in <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">DeepMind’s</a> <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">network</a>) would be to feed in the game’s screens and have the network output the Q-value for each possible action. This way, a single forward pass would output all the Q-values for a given state, translating into one forward pass per optimal future value estimate.</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/q_network_formulations.png"><img src="http://lopespm.github.io/files/dqn_outrun/q_network_formulations.png" /></a></center><figcaption class="media-caption"> Image courtesy of Tambet Matiisen’s <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning">Demystifying Deep Reinforcement Learning</a> - Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind papers. </figcaption></figure>

<p>Q-learning and deep neural networks are the center pieces of a deep Q-network reinforcement learning agent and I think that by understanding them and how they fit together, it can be easier to picture how the algorithm works as a whole.</p>

<h1 id="implementation">Implementation</h1>

<center><a href="http://lopespm.github.io/files/dqn_outrun/overall_view_play.png"><img align="center" src="http://lopespm.github.io/files/dqn_outrun/overall_view_play.png" alt="" /></a></center>

<p />

<p>Above is an overall representation of how the different components relate during a play evaluation, centered around the <code>deep Q-network for playing</code><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, the main decision component.</p>

<p>Each game screen is resized to a desaturated 80x80 pixels image (opposed to 84x84 on DeepMind’s papers), and if you might be wondering why each state is a sequence of four game screens instead of one, that is because the agent’s history is used for better motion perception. Achieving this requires a sequence of preprocessed images to be stacked in channels (like you would stack RGB channels on a colored image) and fed to the network. Note that RGB channels and agent history could be used simultaneously for state representation. For example, with three channels per (RGB) image and an agent history length of four, the network would be fed twelve channels per input state.</p>

<p>The <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/q_network.py#L121">network’s architecture</a> is essentially the same used by DeepMind, except for the first convolutional neural network’s input (80x80x4 instead of 84x84x4, to account for the different input sizes) and the linear layer’s output (9 instead of 18, to account for the different number of actions available)</p>

<p>The algorithm used to train this network is well described <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">here (page 7)</a> and <a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">here</a>, but I would like to present it graphically, to hopefully provide some further intuition.</p>

<p>Below is <a href="https://github.com/lopespm/agent-trainer">agent trainer</a>´s implementation of the aforementioned algorithm. It adds some new concepts which were not approached by this article:</p>

<ul>
  <li>Experience replay mechanism sported by replay memories: randomly samples previous transitions, thereby smoothing the training distribution over many past behaviors</li>
  <li>Separate training network, cloned at fixed intervals to the target playing network, making the algorithm more stable when compared with standard online Q-learning</li>
  <li>ε-greedy policy to balance exploitation/exploration</li>
</ul>

<center><a href="http://lopespm.github.io/files/dqn_outrun/overall_view_train_merged.png"><img align="center" src="http://lopespm.github.io/files/dqn_outrun/overall_view_train_merged.png" alt="" /></a></center>

<p />

<h2 id="reward-function">Reward function</h2>

<p>The reward function’s definition is crucial for good learning performance and determines the goal in a reinforcement learning problem. April Yu et al. have an interesting paper on <a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">simulated autonomous vehicle control</a> which details a DQN agent used to drive a game that strongly resembles Out Run (<a href="http://codeincomplete.com/games/racer/v4-final/">JavaScript Racer</a>). Based on their reward function experiments, I’ve built a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L60">function</a> which rewards logarithmically based on speed and penalizes when the car is off-road, crashed or stopped. </p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/reward_function_plot.png"><img src="http://lopespm.github.io/files/dqn_outrun/reward_function_plot.png" /></a></center><figcaption class="media-caption"> Reward values for when the car is not crashed or off-road </figcaption></figure>

<h1 id="deployment">Deployment</h1>

<p>Run the trainer and emulator on your local machine by following the guide available on <a href="https://github.com/lopespm/agent-trainer/blob/master/README.md">agent-trainer’s readme</a>.</p>

<p>It is also possible to deploy the agent to an AWS EC2 instance or generic Linux remote machine by using a set of bash scripts offered by <a href="https://github.com/lopespm/agent-trainer-deployer">agent-trainer-deployer</a>.</p>

<h3 id="aws-ec2">AWS EC2</h3>

<p>Amazon allows you to bid on spare EC2 computing capacity via <a href="https://aws.amazon.com/ec2/spot/">spot instances</a>. These can cost a fraction of on-demand ones, and for this reason were chosen as the prime method for training in this project, leading to the need for mid-training instance termination resilience.</p>

<p>To accommodate this scenario, the deployment scripts and agent-trainer are designed to support train session resumes. To persist results and decrease boot up time between sessions, a long-lived EBS volume is attached to the live instance. The volume contains the training results, agent-trainer´s source code, cannonball’s source code, dockerfiles and their respective docker images. </p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/deployed-diagram.png"><img src="http://lopespm.github.io/files/dqn_outrun/deployed-diagram.png" /></a></center><figcaption class="media-caption"> Relationship between components when deploying agent-trainer to an AWS EC2 instance </figcaption></figure>

<h1 id="results">Results</h1>

<p>The hyperparameters used on all sessions mimic the ones used on DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a>, except for the number of frames skipped between actions, which are spaced apart by 450ms (equivalent to 13 frames) on agent-trainer.</p>

<p>The Out Run game, as you would play it in an arcade, clutters the road with various cars in order to make the game more challenging. In-game traffic was disabled for both training and evaluation plays, rendering a more achievable starting point for these experiments. Training with random traffic could be an interesting posterior experiment.</p>

<p>Some experiments were made by increasing the discount factor up its final value during training, as proposed on <a href="https://arxiv.org/abs/1512.02011">“How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies”</a>, but did not achieve better stability or rewards when compared to a fixed 0.99 discount factor. The aforementioned paper also proposes decaying the learning rate during training, which increased stability and performance significantly. Decaying the learning rate without minimum value clipping yielded the best results.</p>

<p>Another improvement was to train the game without a time limit, meaning that the training episode would only finish when the car reached the last stage’s end. This allowed for a broader replay memory training set, since the agent traversed a wide range of different tracks and settings.</p>

<p>Play evaluation was the same between all experiments, this is, the agent was evaluated by playing on the default 80 second, easy mode.</p>

<p>Here is a summary of the most relevant training sessions (you can find their models, metrics and visualizations on <a href="https://github.com/lopespm/agent-trainer-results">agent-trainer-results</a>):</p>

<table>
  <thead>
    <tr>
      <th>Session</th>
      <th>M</th>
      <th>Training<br />game mode</th>
      <th>Learning rate decay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>201609040550_5010eps</td>
      <td>a)</td>
      <td>timed; easy</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_2700eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609111241_7300eps</td>
      <td>b)</td>
      <td>timed; easy</td>
      <td>unclipped learning rate decay</td>
    </tr>
    <tr>
      <td>201609160922_54eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>without learning rate decay</td>
    </tr>
    <tr>
      <td>201609171218_175eps</td>
      <td>b)</td>
      <td>unlimited time</td>
      <td>unclipped learning rate decay</td>
    </tr>
  </tbody>
</table>

<p class="media-caption"><a href="https://github.com/lopespm/agent-trainer-results">Training sessions</a> summary: session names are formed by &#60;session ID&#62;_&#60;number of episodes trained&#62;<br />(M)achine used:  a) AMD Athlon(tm) II X2 250 Processor @ 3GHz; 2GB RAM DDR3-1333 SDRAM; SSD 500 GB: Samsung 850 EVO (CPU only training); b) AWS EC2 g2.2xlarge (GPU enabled instance), 200 GB General Purpose SSD (GP2)</p>

<p />

<center>
<div class="video-media-caption-wrapper-two"><div class="video-wrapper-two">
   <div class="youtube-player video-frame-two" data-id="1Gpl9Xc-E8M"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609111241_2700eps</p>
</div>
<div class="video-media-caption-wrapper-two"> <div class="video-wrapper-two">
    <div class="youtube-player video-frame-two" data-id="6F3eCoCw57E"></div></div>
 <p class="media-caption media-caption-two">Agent playing Out Run (timed easy mode, no traffic)<br />Session 201609171218_175eps<br class="video-br-end" /></p>
</div>
</center>

<p>Notice on the videos above how the timed mode trained session <code>201609111241_2700eps</code> reaches the first checkpoint about five seconds earlier than the unlimited time mode trained session <code>201609171218_175eps</code>, but proceeds to drive off-road two turns after. Its stability gets increasingly compromised as more episodes are trained, which can be observed by the rampant loss increase before 7300 episodes are reached (<code>201609111241_7300eps</code>):</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609111241_7300eps/metrics_in_train.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609111241_7300eps/metrics_in_train.png" /></a></center><figcaption class="media-caption"> Training metrics for session 201609111241_7300eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609111241_7300eps/metrics_trained_play.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609111241_7300eps/metrics_trained_play.png" /></a></center><figcaption class="media-caption"> Play evaluation metrics for session 201609111241_7300eps: using ε=0.0; evaluation made at the end of every 20 training episodes </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/metrics_in_train.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/metrics_in_train.png" /></a></center><figcaption class="media-caption"> Training metrics for session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/metrics_trained_play.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/metrics_trained_play.png" /></a></center><figcaption class="media-caption"> Play evaluation metrics 201609171218_175eps: using ε=0.0; evaluation made at the end of every training episode </figcaption></figure>

<p>Both <code>201609111241_2700eps</code> and <code>201609111241_7300eps</code> timed trained sessions mostly drive off-road and stall after the first stage, whereas the unlimited time mode trained session <code>201609171218_175eps</code> can race through all the stages crashing <em>only</em> three times (as shown on the article’s first video) and is able to match the performance of a timed trained session when evaluated on the default easy timed mode. </p>

<p>Below is the loss plot for <code>201609160922_54eps</code> and <code>201609171218_175eps</code>, both trained using the game’s unlimited time mode, difference being that <code>201609160922_54eps</code> keeps a fixed learning rate and <code>201609171218_175eps</code> decays it every 50100 steps:</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/loss_201609171218_201609160922.png"><img src="http://lopespm.github.io/files/dqn_outrun/loss_201609171218_201609160922.png" /></a></center><figcaption class="media-caption"> Loss comparison between sessions <font color="#9c27b0">■</font> 201609160922_54eps and <font color="#009688">■</font> 201609171218_175eps, as viewed on <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a> </figcaption></figure>

<p>Other representative visualizations:</p>

<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/t-SNE_timed_easy_mode.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/t-SNE_timed_easy_mode.png" /></a></center><figcaption class="media-caption"> t-SNE visualization, generated by letting the agent play one game on timed easy mode. Agent is using the network trained on session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/t-SNE_no_time_mode.png"><img src="http://lopespm.github.io/files/dqn_outrun/201609171218_175eps/t-SNE_no_time_mode.png" /></a></center><figcaption class="media-caption"> t-SNE visualization, generated by letting the agent play one game on unlimited time mode. Agent is using the network trained on session 201609171218_175eps </figcaption></figure>
<figure class="image"><center><a href="http://lopespm.github.io/files/dqn_outrun/conv_net_filters.png"><img src="http://lopespm.github.io/files/dqn_outrun/conv_net_filters.png" /></a></center><figcaption class="media-caption"> Visualization of the first convolutional network layer’s filters. These can be viewed via <a href="https://github.com/lopespm/agent-trainer-results">tensorboard</a></figcaption></figure>

<p><br /></p>

<h1 id="final-notes">Final Notes</h1>

<h2 id="plugging-other-problems-and-games">Plugging other problems and games</h2>

<p>Agent-trainer was not built from the get-go to train games or problems other than Out Run, but I think it would be interesting to perform a thought exercise on what would be necessary to do so.</p>

<p>There are three main areas in which <a href="https://github.com/lopespm/agent-trainer">agent-trainer</a> has domain knowledge about Out Run:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer/tree/master/agent/game"><code>game</code></a> package, which contains
    <ul>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/action.py"><code>Action</code></a> enumeration: describes all the possible actions in the game.</li>
      <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/game/cannonball_wrapper.py"><code>cannonball_wrapper</code></a> module: only this module has access to the cannonball emulator. It translates the aforementioned actions into game actions and is accessed by methods such as <code>start_game()</code>, <code>reset()</code> and <code>speed()</code>.</li>
    </ul>
  </li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> class: contains the reward function. Instead of using a generic reward function like DeepMind, it was chosen to have a tailor-made reward function for Out Run, which takes into account the car’s speed and its off-road and crash status.</li>
  <li><a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/visualization/metrics.py"><code>metrics</code></a> module: aware of the <code>speed</code> metric, which is Out Run specific, and <code>score</code>, which is game specific domain knowledge.</li>
</ul>

<p>Training another game would require the creation of a new wrapper with the same interface as <code>cannonball_wrapper</code>, a new <code>Action</code> enumerator specific to the game, a new <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/episode.py#L56"><code>RewardCalculator</code></a> with a different reward function and the removal/replacement of the <code>speed</code> metric.</p>

<p>Apart from the previously mentioned steps, solving generic problems would require the preprocessor to be changed/replaced if images were not to be used for state representation. An option would be to create a new preprocessor class with a <a href="https://github.com/lopespm/agent-trainer/blob/master/agent/trainer/image_preprocessor.py#L13"><code>process(input)</code></a> method, tweak the hyperparameters as required (so that the network knows which dimensions to expect on its input), and finally inject the newly created class in <a href="https://github.com/lopespm/agent-preprocessor/blob/master/agent/trainer/episode.py#L300"><code>EpisodeRunner</code></a>, replacing the old preprocessor class.</p>

<h2 id="further-references">Further references</h2>

<p>I am not a machine learning expert, but from my learner’s point of view, if you are interested in getting your feet wet, Andrew Ng’s Machine Learning Course is as a great starting point. It is freely available on the <a href="https://www.coursera.org/learn/machine-learning">Coursera online learning platform</a>. This was my first solid contact with the subject and served as a major stepping stone for related topics such as Reinforcement Learning.</p>

<p><a href="https://www.udacity.com/course/deep-learning--ud730">Udacity Google Deep Learning</a>: this free course tackles some of the popular deep learning techniques, all the while using tensorflow. I did this right after Andrew Ng’s course and found it to leave the student with less support during lessons - less hand-holding if you will - and as result I spent a good amount of time dabbling to reach a solution for the assignments. </p>

<p>As a side note, I started building this project by the end of the Deep Learning course, mostly because I wanted to apply and consolidate the concepts I learnt into something more practical and to share this knowledge further, so it could hopefully help more people who are interested in this.</p>

<p>Other useful resources:</p>

<ul>
  <li>DeepMind’s <a href="http://www.readcube.com/articles/10.1038/nature14236?shared_access_token=Lo_2hFdW4MuqEcF3CVBZm9RgN0jAjWel9jnR3ZoTv0P5kedCCNjz3FJ2FhQCgXkApOr3ZSsJAldp-tw3IWgTseRnLpAc9xQq-vTA2Z5Ji9lg16_WvCy4SaOgpK5XXA6ecqo8d8J7l4EJsdjwai53GqKt-7JuioG0r3iV67MQIro74l6IxvmcVNKBgOwiMGi8U0izJStLpmQp6Vmi_8Lw_A%3D%3D">Human-Level Control through Deep Reinforcement Learning paper</a> and its respective <a href="https://sites.google.com/a/deepmind.com/dqn/">source code</a></li>
  <li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a></li>
  <li><a href="http://cs231n.stanford.edu/reports2016/112_Report.pdf">Deep Reinforcement Learning for Simulated Autonomous Vehicle Control</a></li>
  <li><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Demystifying Deep Reinforcement Learning</a></li>
  <li><a href="https://www.udacity.com/course/reinforcement-learning--ud600">Udacity Reinforcement Learning by Georgia Tech</a></li>
  <li><a href="https://www.youtube.com/watch?v=dV80NAlEins">Deep learning lecture by Nando de Freitas</a></li>
  <li><a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/">Learning reinforcement learning (with code, exercises and solutions)</a></li>
  <li><a href="https://gym.openai.com/">OpenAI Gym</a>: quoting the project’s page: <em>”a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Go”</em></li>
  <li><a href="https://github.com/devsisters/DQN-tensorflow">Tensorflow implementation</a> of Human-Level Control through Deep Reinforcement Learning, by Devsisters corp.</li>
</ul>

<h1 id="source-code">Source code</h1>

<p>All source code is available on GitHub:</p>

<ul>
  <li><a href="https://github.com/lopespm/agent-trainer">Agent Trainer</a>: the core python+tensorflow application</li>
  <li><a href="https://github.com/lopespm/cannonball">Cannonball</a>: custom Cannonball (Out Run game emulator) fork which contains the changes needed to access the emulator externally</li>
  <li><a href="https://github.com/lopespm/agent-trainer-deployer">Agent Trainer Deployer</a>: bash scripts to deploy agent-trainer to a generic remote machine or AWS EC2 instance</li>
  <li><a href="https://github.com/lopespm/agent-trainer-docker">Agent Trainer Docker</a>: Dockerfiles used when deploying agent-trainer to a remote machine</li>
  <li><a href="https://github.com/lopespm/agent-trainer-results">Agent Trainer Results</a>: Collection of training sessions, each containing their resulting network, metrics and visualizations</li>
</ul>

<p><br /></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>“Deep Q-network for playing” in this project is equivalent to DeepMind’s “target network $\hat Q$” and “Deep Q-network for training” is equivalent to DeepMind’s “network Q”<a href="#fnref:1" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Compressed Deque in Python]]></title>
    <link href="http://lopespm.github.io/libraries/2016/08/27/compressed-deque.html"/>
    <updated>2016-08-27T00:00:00+01:00</updated>
    <id>http://lopespm.github.io/libraries/2016/08/27/compressed-deque</id>
    <content type="html"><![CDATA[<p>This post aims to give some insights about a recently <a href="https://github.com/lopespm/compressed-deque">open-sourced</a> python deque which compresses its items for a decreased volatile and persistent memory footprint.</p>

<!--more-->

<h2 id="why-how-can-it-be-useful">Why? How can it be useful?</h2>

<p>Compressed Deque came up while buiding an algorithm which relied on fast random accesses of numerous complex objects. When compressed, this blob of data could easily fit in fast volatile memory, as opposed to being fetched from a slow hard drive. Even though there was a performance penalty from compressing/decompressing items from volatile memory, this solution proved to be faster than accessing these items from indexed files on a hard disk (on a SSD the gains are more negligible though).</p>

<p>This collection might also be useful for programs running on devices with very limited memory available.</p>

<h2 id="structure">Structure</h2>

<p>Compressed Deque inherits from <a href="https://docs.python.org/2/library/collections.html#collections.deque">deque</a> and stores its items as zlib compressed pickles. The middle pickle layer only serves as a generic serialization method which can provide a serialized object string for zlib to compress. Although pickle can compress objects, its compression rate does not match zlib’s, even using <a href="https://docs.python.org/2/library/pickle.html#data-stream-format">higher protocols</a>.</p>

<p><img src="http://lopespm.github.io/files/compressed_deque/value_layers.png" alt="image" /></p>

<p><code>save_to_file()</code> and <code>load_from_file()</code> static methods are provided to persist the collection directly into disk in its compressed representation, without much overhead. </p>

<p>The persisted file contains a sequence of header/compressed_value pairs: the header is a 4 byte integer description of the compressed value’s length and the compressed value is similiar to its in-memory representation.</p>

<p><img src="http://lopespm.github.io/files/compressed_deque/persisted_values.png" alt="image" /></p>

<h2 id="how-to-use-it">How to Use it</h2>

<p>An easy way to install this package is by using <code>pip</code>:</p>

<div><div class="CodeRay">
  <div class="code"><pre>$ pip install compressed-deque
</pre></div>
</div>
</div>

<p><br /></p>

<p>You can then import the class and use it:</p>

<div><div class="CodeRay">
  <div class="code"><pre><span class="keyword">from</span> <span class="include">compdeque</span> <span class="keyword">import</span> <span class="include">CompressedDeque</span>

<span class="comment"># Instantiate the Deque</span>
collection = CompressedDeque()

<span class="comment"># Use it as a normal deque</span>
collection.append(<span class="integer">1</span>)

<span class="comment"># Persist to a file</span>
CompressedDeque.save_to_file(collection, file_path=<span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/collection.dat</span><span class="delimiter">&quot;</span></span>)

<span class="comment"># ...</span>

<span class="comment"># and load it when you need it later</span>
loaded_collection = CompressedDeque.load_from_file(<span class="string"><span class="delimiter">&quot;</span><span class="content">/path/to/collection.dat</span><span class="delimiter">&quot;</span></span>)
</pre></div>
</div>
</div>

<p><br /></p>

<h2 id="source-code">Source Code</h2>

<p>Source code is available here: <a href="https://github.com/lopespm/compressed-deque">https://github.com/lopespm/compressed-deque</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Attenuate GoPro Chest Mount Heartbeat - using GarageBand/iMovie]]></title>
    <link href="http://lopespm.github.io/audio/2015/12/08/gopro-heartbeat.html"/>
    <updated>2015-12-08T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/audio/2015/12/08/gopro-heartbeat</id>
    <content type="html"><![CDATA[<p>If you have a chest mount (chesty) for your GoPro, then you might have had the surprise of hearing your heartbeat on the recording, since it may lay fairly close to your heart, picking up its beat. This post will:</p>

<ul>
  <li>show you how to attenuate the heartbeat sound to a point where it is almost imperceptible, while maintaining the overall sound</li>
  <li>attenuate the difference between quiet and loud sounds, so the listener gets less startled by a sudden noise created when entering with the bike on rought terrain or the common bike bell sound.</li>
</ul>

<!--more-->

<h2 id="steps-using-garagebandimovie">Steps, using GarageBand/iMovie</h2>

<p>In this specific case, GarageBand 10.0.3 and iMovie 10.0.5 were used, but other versions should work just as fine.</p>

<ol>
  <li>Open iMovie
    <ol>
      <li>Import your GoPro file(s) into your iMovie project. This is the best point in time to edit them and arrange the movie to your liking</li>
      <li>Export your movie (File-&gt;Share-&gt;File…)
        <ul>
          <li>Open GarageBand</li>
        </ul>
      </li>
      <li>Drag the exported movie into your GarageBand project
        <ul>
          <li>Decompress <a href="http://lopespm.github.io/files/gopro_heartbeat/gopro_master_track.patch.zip">gopro_master_track.patch.zip</a> and copy <strong>gopro_master_track.patch</strong> to the “~/Music/Audio Music Apps/Patches/Output”</li>
          <li>Go to your Master Track (if you do not see it, click “Track-&gt;Show MasterTrack”) and click “User Patches” in your Library tab (if you do not see it, click “View-&gt;Show Library”)</li>
          <li>You should now see inside the “User Patches” the patch file we just copied, select it</li>
          <li><em>Your audio should now have an attenuated heartbeat and a more audible and balanced sound</em></li>
          <li>Export the audio (Share-&gt;Export to Disk). AIFF or MP3 will work great. I personally use the MP3 192kBit/s export, since the GoPro audio is not extra pristine to start with, so the tiny losses in compression are pretty much neglegible</li>
          <li>Go back to iMovie</li>
        </ul>
      </li>
      <li>Remove the sound on your original GoPro video(s)
        <ul>
          <li>Drag the export audio from GarageBand, and place it below the video timeline</li>
          <li>Done!</li>
        </ul>
      </li>
    </ol>
  </li>
</ol>

<h2 id="why-did-this-hopefully-worked-for-you">Why did this *hopefully* worked for you?</h2>

<p>The whole concept is very simple really, the heartbeat is simply being attenuated on specific frequencies by a Channel EQ plugin included in the above Master Track patch. This is how it looks visually:</p>

<p><img src="http://lopespm.github.io/files/gopro_heartbeat/heartbeat_channel_eq.png" alt="image" /></p>

<p>This Channel EQ alone will turn this:</p>

<audio controls=""><source src="http://lopespm.github.io/files/gopro_heartbeat/gopro_sample_with_heartbeat.mp3" /></audio>

<p>into this:</p>

<audio controls=""><source src="http://lopespm.github.io/files/gopro_heartbeat/gopro_sample_without_heartbeat.mp3" /></audio>

<p>And that is it! The rest of the plugins included on the Master Track Patch are:</p>

<ul>
  <li>An additional Channel EQ that attenuates clicking sounds I heard along my videos</li>
  <li>Two compressors and a limiter which increase the overall loudness and diminish the difference between quiet and loud sounds</li>
</ul>

<h2 id="support-files">Support Files:</h2>

<ul>
  <li><a href="http://lopespm.github.io/files/gopro_heartbeat/gopro_master_track.patch.zip">Master Track Patch</a> (which includes all the above plugins)</li>
  <li><a href="http://lopespm.github.io/files/gopro_heartbeat/gopro_eq_heartbeat_attenuate.pst">Channel EQ preset</a> to attenuate the heartbeat sound. This can be used by opening a Channel EQ plugin in any of your tracks and clicking “More-&gt;Load..” inside it</li>
  <li><a href="https://www.youtube.com/watch?v=sczmkokpEuY">Example video</a> in which these plugins were applied</li>
</ul>

<p><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Quickly Customize OSX Terminal Style and Functionality]]></title>
    <link href="http://lopespm.github.io/workflow/2015/01/27/customizing-terminal.html"/>
    <updated>2015-01-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/workflow/2015/01/27/customizing-terminal</id>
    <content type="html"><![CDATA[<center>
          <picture>
             <source type="image/webp" srcset="/files/terminal_custom/terminal_before_after.webp" />
             <source type="image/png" srcset="/files/terminal_custom/terminal_before_after.png" />
             <img src="http://lopespm.github.io/files/terminal_custom/terminal_before_after.png" />
           </picture>
         </center>

<!--more-->

<h3 id="style---how-it-looks">Style - how it looks</h3>
<p>Add the <em>real world tested</em> <a href="http://ethanschoonover.com/solarized">Solarized (dark/light)</a> Terminal profiles: <a href="https://github.com/tomislav/osx-terminal.app-colors-solarized">https://github.com/tomislav/osx-terminal.app-colors-solarized</a></p>

<ul>
  <li>After cloning the repo, just open the profile files, which will add them to your Terminal.app preferences</li>
  <li>You can set them as the default profiles on your Terminal.app preferences</li>
  <li>To enable the terminal colors, add the line <code>export CLICOLOR=1</code> to your .bash_profile</li>
</ul>

<h3 id="function---what-it-does">Function - what it does</h3>
<p>Grab the .bash_profile here: <a href="https://gist.github.com/lopespm/3877ff1ca4064a392686">https://gist.github.com/lopespm/3877ff1ca4064a392686</a>, which is heavily based on <a href="http://natelandau.com/my-mac-osx-bash_profile/">Nathaniel Landau’s profile</a></p>

<ul>
  <li>In OSX, .bash_profile is executed <a href="http://www.joshstaiger.org/archives/2005/07/bash_profile_vs.html">everytime you open a new Terminal window</a>, and it lives in your user’s home directory, i.e. <code>~/.bash_profile</code></li>
  <li>If it does not exist, just <code>touch ~/.bash_profile</code> to create it</li>
  <li>If you do not want to clutter your .bash_profile, you can create a file like <code>~/scripts/utilities.bash</code> with the portions you find useful and then add this line to your .bash_profile: <code>source ~/scripts/utilities.bash</code></li>
</ul>

<p>Hopefully now your Terminal is even more inviting :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Survival Ball on OUYA]]></title>
    <link href="http://lopespm.github.io/games/2013/09/08/survival-ball-ouya.html"/>
    <updated>2013-09-08T00:00:00+01:00</updated>
    <id>http://lopespm.github.io/games/2013/09/08/survival-ball-ouya</id>
    <content type="html"><![CDATA[<p>Survival Ball in now available on OUYA, and its publishing process was fairly straightforward. From the day of submission (2nd of September) to its approval (5th of September), it took a mere 3 days, counting that a re-submission had to be made in-between.</p>

<p>Survival Ball was already available on <a href="https://play.google.com/store/apps/details?id=com.rockbyte.survivalball">Google Play</a>, <a href="http://www.amazon.com/RockByte-Software-Survival-Ball/dp/B00ARVZ7F0/ref=sr_1_1?s=mobile-apps&amp;ie=UTF8&amp;qid=1378394674&amp;sr=1-1">Amazon </a>and <a href="http://www.kongregate.com/games/rockbyte/survival-ball">Kongregate</a>, but OUYA just felt like the perfect platform for the game. With it, you could just grab the OUYA controller and/or a spare XBox/PS3 controller, and just have a quick match. I really loved the concept, and re-iterated on the previous version of the game to give it more controller friendly menus, better graphics, refined textures, and an overall revamp.</p>

<!--more-->

<p>After finishing up the port to OUYA, on the 2nd of September, I’ve made the first submission. I’ll just point out that upon this moment, it seemed to be stuck with a red status message which read “<em>Still verifying apk…</em>”. To resolve this, I edited the submission, re-submitted the APK, and waited a few seconds. The game was then submitted for approval.</p>

<p>On the 4th of September, a re-submission was asked by the OUYA review team (which were very friendly by the way), since some elements fell of the safe-frame, which is safe-guard for gamers who have screens that perform <a href="https://devs.ouya.tv/developers/docs/content-review-guidelines">too much overscan</a>.</p>

<p>The game was re-submitted on the same day, and was approved on the 5th of September. From this point on, one can release the game whenever it seems more appropriate. The game was released on the same day.</p>

<p>I’ve came to learn during this process, after publishing the game, it first goes to the “Sandbox” section. Only after a major acceptance from the game community, can the game be promoted to the primary categories. So, if you would like to take a quick peek at the game, you can find it at the “Sandbox” section.</p>

<center><iframe width="560" height="315" src="http://lopespm.github.io//www.youtube.com/embed/ZrqakDKWwjg" frameborder="0" allowfullscreen=""></iframe></center>

<p><br /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Nordic Game Jam’s winners]]></title>
    <link href="http://lopespm.github.io/games/2013/01/27/nordic-game-jams-winners.html"/>
    <updated>2013-01-27T00:00:00+00:00</updated>
    <id>http://lopespm.github.io/games/2013/01/27/nordic-game-jams-winners</id>
    <content type="html"><![CDATA[<p>This year’s Nordic Game Jam was crowned as the world’s largest game jam with about 470 participants. The winners have been announced, and can see the full list here: <a href="http://nordicgamejam.org/2013/01/21/winners-ngj13/">http://nordicgamejam.org/2013/01/21/winners-ngj13/</a></p>

<p>My personal highlight goes to Stalagflight (<a href="https://dl.dropbox.com/u/85666/Stalagflight-web/webBuild.html">https://dl.dropbox.com/u/85666/Stalagflight-web/webBuild.html</a>), which is as simple as stupidly fun! Stalagflight can be played with the keyboard, but it is tricky. If you have a gamepad the experience is much better.</p>

<!--more-->

<p>On other lines, if you want to stretch the face of Justin Bieber and force him to make heavenly sounds, rejoice with this: <a href="http://unicorn7.org/static/be/">http://unicorn7.org/static/be/</a></p>
]]></content>
  </entry>
  
</feed>
